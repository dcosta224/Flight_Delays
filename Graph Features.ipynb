{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25799e25-afa8-4ea7-984f-4eb2227a9f0c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Graph-Based Feature Engineering for Flight Delay Prediction\n",
    "\n",
    "**Author:** Daniel Costa  \n",
    "**Project:** Flight Delay Prediction (W261 Final Project)  \n",
    "**Dataset:** US Domestic Flights (2015-2019)\n",
    "\n",
    "---\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook engineers **graph-based features** from the flight network to capture airport connectivity and route characteristics that influence flight delays. The US flight network is modeled as a directed graph where airports are nodes and flight routes are edges.\n",
    "\n",
    "### Feature Categories\n",
    "\n",
    "| Category | Features | Description |\n",
    "|----------|----------|-------------|\n",
    "| **Centrality Metrics** | PageRank, Betweenness, Closeness | Measures of airport importance in the network |\n",
    "| **Degree Metrics** | In/Out Degree (weighted & unweighted) | Airport connectivity and traffic volume |\n",
    "| **Infrastructure** | Number of Runways | Physical capacity constraints |\n",
    "| **Route Features** | Avg Daily Flights, Avg Delay, Hourly Traffic | Route-specific patterns |\n",
    "| **Historical Delays** | Avg Origin/Destination Delay | Airport-level delay tendencies |\n",
    "\n",
    "### Network Representation\n",
    "\n",
    "```\n",
    "                    ┌─────────┐\n",
    "        ┌──────────►│   ATL   │◄──────────┐\n",
    "        │           └────┬────┘           │\n",
    "        │                │                │\n",
    "   ┌────┴────┐      ┌────▼────┐      ┌────┴────┐\n",
    "   │   ORD   │◄────►│   DFW   │◄────►│   LAX   │\n",
    "   └────┬────┘      └────┬────┘      └────┬────┘\n",
    "        │                │                │\n",
    "        └───────────►┌───▼───┐◄───────────┘\n",
    "                     │  JFK  │\n",
    "                     └───────┘\n",
    "\n",
    "Nodes = Airports (with centrality, degree, runway features)\n",
    "Edges = Routes (with traffic volume, delay patterns)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Setup & Configuration](#1-setup-configuration)\n",
    "2. [Data Loading & Preprocessing](#2-data-loading-preprocessing)\n",
    "3. [Utility Functions](#3-utility-functions)\n",
    "4. [Vertex (Airport) Features](#4-vertex-airport-features)\n",
    "   - [4.1 PageRank](#41-pagerank)\n",
    "   - [4.2 Degree Metrics](#42-degree-metrics)\n",
    "   - [4.3 Airport Infrastructure](#43-airport-infrastructure)\n",
    "   - [4.4 Centrality Metrics](#44-centrality-metrics)\n",
    "   - [4.5 Weighted Betweenness Centrality](#45-weighted-betweenness-centrality)\n",
    "   - [4.6 Historical Delay Features](#46-historical-delay-features)\n",
    "5. [Edge (Route) Features](#5-edge-route-features)\n",
    "6. [Feature Pipeline](#6-feature-pipeline)\n",
    "7. [Execution](#7-execution)\n",
    "8. [Summary](#8-summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c49b28fd-260e-491f-9677-834afcc8a403",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def load_airport_coordinates():\n",
    "    \"\"\"\n",
    "    Load airport coordinates from CSV and extract latitude/longitude.\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame: Airport IATA codes with lat/long coordinates\n",
    "        dict: Mapping of IATA code to {lat, long} for fast lookups\n",
    "    \"\"\"\n",
    "    df_airports = (\n",
    "        spark.read.csv(AIRPORT_CODES_PATH, header=True, inferSchema=True)\n",
    "        .withColumn(\"long\", F.split(F.col(\"coordinates\"), \",\")[0].cast(\"float\"))\n",
    "        .withColumn(\"lat\", F.split(F.col(\"coordinates\"), \",\")[1].cast(\"float\"))\n",
    "        .select(\"iata_code\", \"long\", \"lat\")\n",
    "        .dropna()\n",
    "    )\n",
    "    \n",
    "    # Create lookup dictionary for fast access\n",
    "    airport_info = {\n",
    "        row[\"iata_code\"]: {\"long\": row[\"long\"], \"lat\": row[\"lat\"]}\n",
    "        for row in df_airports.collect()\n",
    "    }\n",
    "    \n",
    "    return df_airports, airport_info\n",
    "\n",
    "\n",
    "def transform_df(df):\n",
    "    \"\"\"\n",
    "    Apply standard transformations to flight DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        df: Raw flight DataFrame\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with parsed date and hour columns\n",
    "    \"\"\"\n",
    "    return (\n",
    "        df\n",
    "        .withColumn(\"FL_DATE\", F.to_date(\"FL_DATE\", \"yyyy-MM-dd\"))\n",
    "        .withColumn(\"hour\", (F.col(\"CRS_DEP_TIME\").cast(\"int\") / F.lit(100)).cast(\"int\"))\n",
    "    )\n",
    "\n",
    "\n",
    "def ingest_data(dataset_path):\n",
    "    \"\"\"\n",
    "    Load train, validation, and test datasets from parquet files.\n",
    "    \n",
    "    Args:\n",
    "        dataset_path: Base path to dataset directory\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (train_df, validation_df, test_df)\n",
    "    \"\"\"\n",
    "    train_df = transform_df(spark.read.parquet(f\"{dataset_path}/train.parquet\"))\n",
    "    validation_df = transform_df(spark.read.parquet(f\"{dataset_path}/validation.parquet\"))\n",
    "    test_df = transform_df(spark.read.parquet(f\"{dataset_path}/test.parquet\"))\n",
    "    \n",
    "    return train_df, validation_df, test_df\n",
    "\n",
    "\n",
    "def get_dataset_days(flight_df):\n",
    "    \"\"\"\n",
    "    Calculate the number of days spanned by the dataset.\n",
    "    \n",
    "    Args:\n",
    "        flight_df: Flight DataFrame with FL_DATE column\n",
    "        \n",
    "    Returns:\n",
    "        int: Number of days in dataset\n",
    "    \"\"\"\n",
    "    dates = flight_df.agg(\n",
    "        F.min(\"FL_DATE\").alias(\"min_date\"),\n",
    "        F.max(\"FL_DATE\").alias(\"max_date\")\n",
    "    ).collect()[0]\n",
    "    \n",
    "    return (dates[\"max_date\"] - dates[\"min_date\"]).days + 1\n",
    "\n",
    "\n",
    "# Load airport data\n",
    "df_airports, airport_info = load_airport_coordinates()\n",
    "print(f\"Loaded {len(airport_info)} airports with coordinates\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ddcfe09b-2d6e-4c39-a4c7-6eae8f811c77",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 3. Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0afa0cdb-848b-4bb8-80f5-9915f2874c48",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def haversine_np(lat1, lon1, lat2, lon2, radius=EARTH_RADIUS_M):\n",
    "    \"\"\"\n",
    "    Calculate great-circle distance between two points using the Haversine formula.\n",
    "    \n",
    "    Vectorized implementation that works with NumPy arrays for efficiency.\n",
    "    \n",
    "    Args:\n",
    "        lat1, lon1: Latitude and longitude of first point(s) in degrees\n",
    "        lat2, lon2: Latitude and longitude of second point(s) in degrees\n",
    "        radius: Earth's radius (default: 6,371,008.8 meters)\n",
    "        \n",
    "    Returns:\n",
    "        Distance in kilometers\n",
    "    \"\"\"\n",
    "    lat1 = np.asarray(lat1, dtype=np.float64)\n",
    "    lon1 = np.asarray(lon1, dtype=np.float64)\n",
    "    lat2 = np.asarray(lat2, dtype=np.float64)\n",
    "    lon2 = np.asarray(lon2, dtype=np.float64)\n",
    "\n",
    "    # Convert degrees to radians\n",
    "    φ1, λ1 = np.radians(lat1), np.radians(lon1)\n",
    "    φ2, λ2 = np.radians(lat2), np.radians(lon2)\n",
    "\n",
    "    dφ = φ2 - φ1\n",
    "    dλ = λ2 - λ1\n",
    "\n",
    "    # Haversine formula\n",
    "    s = np.sin(dφ * 0.5)\n",
    "    t = np.sin(dλ * 0.5)\n",
    "    a = s * s + np.cos(φ1) * np.cos(φ2) * t * t\n",
    "\n",
    "    # Return distance in kilometers\n",
    "    return (2.0 * radius) * np.arcsin(np.minimum(1.0, np.sqrt(a))) / 1000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5dac0dec-d6c9-4f1d-98dd-82fca6a37c8a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 4. Vertex (Airport) Features\n",
    "\n",
    "Airport-level features capture the structural importance and operational characteristics of each airport in the flight network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 PageRank\n",
    "\n",
    "PageRank measures the importance of an airport based on the number and quality of incoming flight connections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13586059-3f5c-4681-b5b2-197cc63373fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def page_rank(flight_df, d=0.85, max_iter=20):\n",
    "    \"\"\"\n",
    "    Compute PageRank for airports based on flight connections.\n",
    "    \n",
    "    PageRank measures the importance of an airport based on the number and\n",
    "    quality of incoming flight connections. Airports with more flights from\n",
    "    other important airports will have higher PageRank scores.\n",
    "    \n",
    "    Args:\n",
    "        flight_df: Flight DataFrame with ORIGIN, DEST columns\n",
    "        d: Damping factor (probability of following a link vs random jump)\n",
    "        max_iter: Number of iterations for convergence\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with ORIGIN and page_rank columns\n",
    "    \"\"\"\n",
    "    # Build weighted edge list (weight = number of unique flights)\n",
    "    edges = (\n",
    "        flight_df\n",
    "        .groupBy(\"ORIGIN\", \"DEST\")\n",
    "        .agg(F.countDistinct(\"flight_uid\").alias(\"weight\"))\n",
    "        .withColumnRenamed(\"ORIGIN\", \"src\")\n",
    "        .withColumnRenamed(\"DEST\", \"dst\")\n",
    "    )\n",
    "\n",
    "    # Normalize edge weights by source out-degree\n",
    "    out_sums = edges.groupBy(\"src\").agg(F.sum(\"weight\").alias(\"total_w\"))\n",
    "    edges_norm = (\n",
    "        edges.join(out_sums, on=\"src\")\n",
    "        .withColumn(\"norm_w\", F.col(\"weight\") / F.col(\"total_w\"))\n",
    "    )\n",
    "\n",
    "    # Initialize all vertices with PageRank = 1.0\n",
    "    vertices = edges.select(\"src\").union(edges.select(\"dst\")).distinct()\n",
    "    ranks = vertices.withColumn(\"page_rank\", F.lit(1.0))\n",
    "\n",
    "    # Iterative PageRank computation\n",
    "    for _ in range(max_iter):\n",
    "        contribs = (\n",
    "            edges_norm.join(ranks, edges_norm[\"src\"] == ranks[\"src\"])\n",
    "            .select(\n",
    "                edges_norm.dst.alias(\"id\"),\n",
    "                (F.col(\"page_rank\") * F.col(\"norm_w\")).alias(\"contrib\")\n",
    "            )\n",
    "        )\n",
    "        new_ranks = contribs.groupBy(\"id\").agg(F.sum(\"contrib\").alias(\"sum_contrib\"))\n",
    "        ranks = new_ranks.select(\n",
    "            F.col(\"id\").alias(\"src\"),\n",
    "            ((1 - d) + d * F.col(\"sum_contrib\")).alias(\"page_rank\")\n",
    "        )\n",
    "\n",
    "    return ranks.withColumnRenamed(\"src\", \"ORIGIN\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Degree Metrics\n",
    "\n",
    "Degree metrics measure airport connectivity - how many unique connections and total flight volume each airport handles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9991c0ee-bed4-47a3-8d31-0374b272e82f",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1765005720929}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def out_degree(flight_df):\n",
    "    \"\"\"\n",
    "    Calculate unweighted out-degree (number of unique destinations from each airport).\n",
    "    \"\"\"\n",
    "    return flight_df.groupBy(\"ORIGIN\").agg(F.countDistinct(\"DEST\").alias(\"out_degree\"))\n",
    "\n",
    "\n",
    "def in_degree(flight_df):\n",
    "    \"\"\"\n",
    "    Calculate unweighted in-degree (number of unique origins flying to each airport).\n",
    "    \"\"\"\n",
    "    return (\n",
    "        flight_df.groupBy(\"DEST\")\n",
    "        .agg(F.countDistinct(\"ORIGIN\").alias(\"in_degree\"))\n",
    "        .withColumnRenamed(\"DEST\", \"ORIGIN\")\n",
    "    )\n",
    "\n",
    "\n",
    "def weighted_in_out_degree(flight_df, dataset_n_days):\n",
    "    \"\"\"\n",
    "    Calculate weighted in/out degree based on average daily flight volume.\n",
    "    \n",
    "    Unlike unweighted degree which counts unique connections, weighted degree\n",
    "    captures actual traffic volume - an airport with 100 daily flights to 5\n",
    "    destinations has higher weighted out-degree than one with 10 flights to 20.\n",
    "    \n",
    "    Args:\n",
    "        flight_df: Flight DataFrame\n",
    "        dataset_n_days: Number of days in dataset for normalization\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with weighted_in_degree and weighted_out_degree columns\n",
    "    \"\"\"\n",
    "    weighted_in = (\n",
    "        flight_df.groupBy(\"DEST\")\n",
    "        .agg((F.count(\"*\") / F.lit(dataset_n_days)).alias(\"weighted_in_degree\"))\n",
    "        .withColumnRenamed(\"DEST\", \"ORIGIN\")\n",
    "    )\n",
    "\n",
    "    weighted_out = (\n",
    "        flight_df.groupBy(\"ORIGIN\")\n",
    "        .agg((F.count(\"*\") / F.lit(dataset_n_days)).alias(\"weighted_out_degree\"))\n",
    "    )\n",
    "\n",
    "    return weighted_in.join(weighted_out, on=\"ORIGIN\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Airport Infrastructure\n",
    "\n",
    "Physical infrastructure constraints like runway count affect airport capacity and delay potential."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6447dbbb-f460-4570-a13c-2f388e70680e",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1765005916291}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_runways(flights_df):\n",
    "    \"\"\"\n",
    "    Load airport runway counts from embedded data.\n",
    "    \n",
    "    Number of runways affects airport capacity and potential for delays.\n",
    "    Major hubs like ORD (8 runways) can handle more simultaneous operations\n",
    "    than smaller airports.\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with ORIGIN and N_RUNWAYS columns\n",
    "    \"\"\"\n",
    "    airport_runways = pd.read_csv(StringIO(RUNWAY_DATA))\n",
    "    return spark.createDataFrame(airport_runways)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec982a27-f2c9-4c18-8cb0-5ac66d56f00d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 4.4 Centrality Metrics\n",
    "\n",
    "Centrality metrics measure the importance of airports in the network structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72bc866c-1a6b-4522-bc51-b6c2b481a40e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Note: airport_info lookup dictionary is created during data loading (Section 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02a5eaf4-5519-45d2-8c91-2c9fa02784b2",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1765006077187}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_unweighted_centralities(flight_df):\n",
    "    \"\"\"\n",
    "    Calculate betweenness and closeness centrality using NetworkX.\n",
    "    \n",
    "    - Betweenness Centrality: Measures how often an airport lies on the shortest\n",
    "      path between other airports. High betweenness = important hub for connections.\n",
    "    \n",
    "    - Closeness Centrality: Measures how close an airport is to all other airports.\n",
    "      Uses geographic distance as edge weights.\n",
    "    \n",
    "    Args:\n",
    "        flight_df: Flight DataFrame with ORIGIN, DEST columns\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with betweenness_unweighted and closeness columns\n",
    "    \"\"\"\n",
    "    # Build directed graph with geographic distances as weights\n",
    "    edges = flight_df.select(\"ORIGIN\", \"DEST\").distinct().collect()\n",
    "    \n",
    "    G = nx.DiGraph()\n",
    "    for row in edges:\n",
    "        lat1, lon1 = airport_info[row[\"ORIGIN\"]][\"lat\"], airport_info[row[\"ORIGIN\"]][\"long\"]\n",
    "        lat2, lon2 = airport_info[row[\"DEST\"]][\"lat\"], airport_info[row[\"DEST\"]][\"long\"]\n",
    "        distance = haversine_np(lat1, lon1, lat2, lon2)\n",
    "        G.add_edge(row[\"ORIGIN\"], row[\"DEST\"], weight=distance)\n",
    "\n",
    "    # Compute centrality metrics\n",
    "    bc = nx.betweenness_centrality(G, normalized=True, weight=None)\n",
    "    cc = nx.closeness_centrality(G, distance=\"weight\")\n",
    "\n",
    "    # Convert to Spark DataFrames\n",
    "    bc_df = spark.createDataFrame([\n",
    "        Row(ORIGIN=node, betweenness_unweighted=float(score)) \n",
    "        for node, score in bc.items()\n",
    "    ])\n",
    "    cc_df = spark.createDataFrame([\n",
    "        Row(ORIGIN=node, closeness=float(score)) \n",
    "        for node, score in cc.items()\n",
    "    ])\n",
    "\n",
    "    return bc_df.join(cc_df, on=\"ORIGIN\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "276448a6-3dec-4a0a-afcb-ac64290a140c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 4.5 Weighted Betweenness Centrality\n",
    "\n",
    "Uses flight frequency as edge weights (more flights = lower cost path)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea1c701d-84b7-4884-948f-868bcbb83d8e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_weighted_centrality(flight_df):\n",
    "    \"\"\"\n",
    "    Calculate weighted betweenness centrality using flight frequency.\n",
    "    \n",
    "    Routes with more flights are treated as \"shorter\" paths (lower cost),\n",
    "    so airports that connect high-traffic routes will have higher centrality.\n",
    "    \n",
    "    Args:\n",
    "        flight_df: Flight DataFrame with ORIGIN, DEST columns\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with ORIGIN and betweenness columns\n",
    "    \"\"\"\n",
    "    rows = flight_df.groupBy(\"ORIGIN\", \"DEST\").agg(F.count(\"*\").alias(\"num_flights\")).collect()\n",
    "\n",
    "    G = nx.DiGraph()\n",
    "    for r in rows:\n",
    "        # Convert flight frequency to cost (more flights = lower cost)\n",
    "        cost = 1.0 / r[\"num_flights\"]\n",
    "        G.add_edge(r[\"ORIGIN\"], r[\"DEST\"], weight=cost)\n",
    "\n",
    "    bc = nx.betweenness_centrality(G, weight=\"weight\", normalized=True)\n",
    "\n",
    "    return spark.createDataFrame([\n",
    "        Row(ORIGIN=node, betweenness=float(score))\n",
    "        for node, score in bc.items()\n",
    "    ])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 Historical Delay Features\n",
    "\n",
    "Airport-level delay statistics capture historical patterns that may predict future delays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "014c6903-34ce-4cbb-bda0-ec71a60191b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def average_delay_at_origin(flight_df):\n",
    "    \"\"\"\n",
    "    Calculate average departure delay at each origin airport.\n",
    "    \n",
    "    Captures airport-level delay tendencies that may affect future flights.\n",
    "    \"\"\"\n",
    "    return flight_df.groupBy(\"ORIGIN\").agg(F.avg(\"DEP_DELAY\").alias(\"avg_origin_dep_delay\"))\n",
    "\n",
    "\n",
    "def average_delay_at_dest(flight_df):\n",
    "    \"\"\"\n",
    "    Calculate average arrival delay at each destination airport.\n",
    "    \n",
    "    Airports with historically high arrival delays may indicate congestion issues.\n",
    "    \"\"\"\n",
    "    return flight_df.groupBy(\"DEST\").agg(F.avg(\"ARR_DELAY\").alias(\"avg_dest_arr_delay\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Edge (Route) Features\n",
    "\n",
    "Route-level features capture characteristics of specific origin-destination pairs, including traffic volume, historical delays, and hourly patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a379370f-a602-4caa-ad39-90811e4be62f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def average_daily_flights_on_route(flight_df, dataset_n_days):\n",
    "    \"\"\"\n",
    "    Calculate average daily flight count for each route (ORIGIN → DEST).\n",
    "    \n",
    "    High-traffic routes may have different delay patterns than low-traffic routes.\n",
    "    \"\"\"\n",
    "    return flight_df.groupBy(\"ORIGIN\", \"DEST\").agg(\n",
    "        (F.count(\"*\") / F.lit(dataset_n_days)).alias(\"avg_daily_route_flights\")\n",
    "    )\n",
    "\n",
    "\n",
    "def average_delay_on_route(flight_df):\n",
    "    \"\"\"\n",
    "    Calculate average departure delay for each route.\n",
    "    \n",
    "    Some routes may have consistently higher delays due to scheduling,\n",
    "    weather patterns, or operational factors.\n",
    "    \"\"\"\n",
    "    return flight_df.groupBy(\"ORIGIN\", \"DEST\").agg(\n",
    "        F.avg(\"DEP_DELAY\").alias(\"avg_route_delay\")\n",
    "    )\n",
    "\n",
    "\n",
    "def average_flights_at_hour(flight_df, dataset_n_days):\n",
    "    \"\"\"\n",
    "    Calculate average flight count at each hour for each route.\n",
    "    \n",
    "    Captures hourly traffic patterns - rush hour flights may experience\n",
    "    more delays than off-peak flights.\n",
    "    \"\"\"\n",
    "    return flight_df.groupBy(\"ORIGIN\", \"DEST\", \"hour\").agg(\n",
    "        (F.count(\"*\") / F.lit(dataset_n_days)).alias(\"avg_hourly_flights\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Feature Pipeline\n",
    "\n",
    "The feature pipeline orchestrates the computation and joining of all graph features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0e75667-2f45-433a-8af4-471af75d691b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Feature Function Registry\n",
    "# -----------------------------------------------------------------------------\n",
    "# Organize feature functions by their join key for systematic computation\n",
    "\n",
    "# Origin airport features (join on ORIGIN)\n",
    "VERTEX_FUNCTIONS_ORIGIN = [\n",
    "    page_rank,\n",
    "    out_degree,\n",
    "    in_degree,\n",
    "    weighted_in_out_degree,\n",
    "    get_runways,\n",
    "    get_unweighted_centralities,\n",
    "    get_weighted_centrality,\n",
    "    average_delay_at_origin\n",
    "]\n",
    "\n",
    "# Destination airport features (join on DEST)\n",
    "VERTEX_FUNCTIONS_DEST = [\n",
    "    average_delay_at_dest\n",
    "]\n",
    "\n",
    "# Route features (join on ORIGIN, DEST)\n",
    "EDGE_FUNCTIONS = [\n",
    "    average_daily_flights_on_route,\n",
    "    average_delay_on_route\n",
    "]\n",
    "\n",
    "# Hourly route features (join on ORIGIN, DEST, hour)\n",
    "EDGE_HOURLY_FUNCTIONS = [\n",
    "    average_flights_at_hour\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cfb83c2b-3319-4e22-b2df-024e7804c26a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def form_feature_df(functions, train_df, join_cols, dataset_n_days):\n",
    "    \"\"\"\n",
    "    Compute and combine features from a list of feature functions.\n",
    "    \n",
    "    Args:\n",
    "        functions: List of feature functions to apply\n",
    "        train_df: Training DataFrame\n",
    "        join_cols: Columns to join on\n",
    "        dataset_n_days: Number of days in dataset for normalization\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with all computed features joined together\n",
    "    \"\"\"\n",
    "    assert len(functions) > 0, \"At least one feature function required\"\n",
    "    \n",
    "    # Compute first feature (handle functions with/without dataset_n_days param)\n",
    "    try:\n",
    "        df = functions[0](train_df, dataset_n_days)\n",
    "    except TypeError:\n",
    "        df = functions[0](train_df)\n",
    "    \n",
    "    # Join remaining features\n",
    "    for func in functions[1:]:\n",
    "        try:\n",
    "            df = df.join(func(train_df, dataset_n_days), on=join_cols)\n",
    "        except TypeError:\n",
    "            df = df.join(func(train_df), on=join_cols)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def join_features(input_df, origin_df, dest_df, edge_df, edge_hour_df):\n",
    "    \"\"\"\n",
    "    Join all computed features to the input flight DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        input_df: Base flight DataFrame\n",
    "        origin_df: Origin airport features\n",
    "        dest_df: Destination airport features  \n",
    "        edge_df: Route features\n",
    "        edge_hour_df: Hourly route features\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with all features joined\n",
    "    \"\"\"\n",
    "    return (\n",
    "        input_df\n",
    "        .join(origin_df, on=\"ORIGIN\")\n",
    "        .join(dest_df, on=\"DEST\")\n",
    "        .join(edge_df, on=[\"ORIGIN\", \"DEST\"])\n",
    "        .join(edge_hour_df, on=[\"ORIGIN\", \"DEST\", \"hour\"])\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "43cc3e30-edd0-4e67-bee3-40ed5a58fd0f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def add_features(dataset_dir):\n",
    "    \"\"\"\n",
    "    Main pipeline function to compute and save graph features for a dataset.\n",
    "    \n",
    "    Steps:\n",
    "    1. Load train/validation/test splits\n",
    "    2. Compute all vertex and edge features from training data\n",
    "    3. Join features to all splits (train uses same features for val/test)\n",
    "    4. Save enriched datasets to parquet\n",
    "    \n",
    "    Args:\n",
    "        dataset_dir: Name of dataset directory (e.g., '5_year_custom_joined')\n",
    "    \"\"\"\n",
    "    print(f\"Starting run for {dataset_dir}\")\n",
    "    dataset_path = f\"{CHECKPOINT_PATH}/{dataset_dir}\"\n",
    "    graph_dir = \"graph_feature_splits\"\n",
    "    dbutils.fs.mkdirs(f\"{dataset_path}/{graph_dir}\")\n",
    "\n",
    "    # Step 1: Load data\n",
    "    print(\"...Ingesting Data\")\n",
    "    train_df, validation_df, test_df = ingest_data(\n",
    "        f\"{dataset_path}/feature_eng_ph3/training_splits\"\n",
    "    )\n",
    "\n",
    "    # Step 2: Compute features (using training data only to prevent leakage)\n",
    "    print(\"...Computing Features\")\n",
    "    dataset_n_days = get_dataset_days(train_df)\n",
    "\n",
    "    origin_df = form_feature_df(\n",
    "        VERTEX_FUNCTIONS_ORIGIN, train_df, [\"ORIGIN\"], dataset_n_days\n",
    "    )\n",
    "    dest_df = form_feature_df(\n",
    "        VERTEX_FUNCTIONS_DEST, train_df, [\"DEST\"], dataset_n_days\n",
    "    )\n",
    "    edge_df = form_feature_df(\n",
    "        EDGE_FUNCTIONS, train_df, [\"ORIGIN\", \"DEST\"], dataset_n_days\n",
    "    )\n",
    "    edge_hour_df = form_feature_df(\n",
    "        EDGE_HOURLY_FUNCTIONS, train_df, [\"ORIGIN\", \"DEST\", \"hour\"], dataset_n_days\n",
    "    )\n",
    "\n",
    "    # Step 3: Join features to all splits\n",
    "    print(\"...Joining Features\")\n",
    "    train_df_joined = join_features(train_df, origin_df, dest_df, edge_df, edge_hour_df)\n",
    "    validation_df_joined = join_features(validation_df, origin_df, dest_df, edge_df, edge_hour_df)\n",
    "    test_df_joined = join_features(test_df, origin_df, dest_df, edge_df, edge_hour_df)\n",
    "\n",
    "    # Step 4: Save to parquet\n",
    "    print(\"...Writing Features\")\n",
    "    train_df_joined.write.parquet(f\"{dataset_path}/{graph_dir}/train\", mode=\"overwrite\")\n",
    "    validation_df_joined.write.parquet(f\"{dataset_path}/{graph_dir}/validation\", mode=\"overwrite\")\n",
    "    test_df_joined.write.parquet(f\"{dataset_path}/{graph_dir}/test\", mode=\"overwrite\")\n",
    "    \n",
    "    print(f\"✓ Completed {dataset_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Execution\n",
    "\n",
    "Run the feature pipeline on all dataset sizes (3-month, 1-year, 5-year)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7873bd3-61f3-4e8a-95d4-55b3a8937df9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Process all datasets\n",
    "for dataset_dir in DATASET_DIRS:\n",
    "    start_time = time.time()\n",
    "    add_features(dataset_dir)\n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"  → Completed in {elapsed/60.0:.2f} minutes\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc39749c-f1f6-4d5b-89c6-44cf1a6ef779",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Verify output directories\n",
    "display(dbutils.fs.ls(CHECKPOINT_PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "684d8a44-7a4a-4597-a7b8-97a77effccd5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 8. Summary\n",
    "\n",
    "This notebook computes graph-based features that capture the network structure of the US flight system. These features provide the model with information about airport importance, connectivity, and historical patterns that raw flight data alone cannot capture.\n",
    "\n",
    "### Features Generated\n",
    "\n",
    "| Feature | Type | Description |\n",
    "|---------|------|-------------|\n",
    "| `page_rank` | Vertex | Airport importance based on incoming connections |\n",
    "| `out_degree` | Vertex | Number of unique destinations |\n",
    "| `in_degree` | Vertex | Number of unique origins |\n",
    "| `weighted_in_degree` | Vertex | Average daily incoming flights |\n",
    "| `weighted_out_degree` | Vertex | Average daily outgoing flights |\n",
    "| `N_RUNWAYS` | Vertex | Physical runway count |\n",
    "| `betweenness_unweighted` | Vertex | Centrality based on shortest paths |\n",
    "| `betweenness` | Vertex | Centrality weighted by flight frequency |\n",
    "| `closeness` | Vertex | Geographic closeness centrality |\n",
    "| `avg_origin_dep_delay` | Vertex | Historical departure delay at origin |\n",
    "| `avg_dest_arr_delay` | Vertex | Historical arrival delay at destination |\n",
    "| `avg_daily_route_flights` | Edge | Average daily flights on route |\n",
    "| `avg_route_delay` | Edge | Historical delay for route |\n",
    "| `avg_hourly_flights` | Edge | Average flights at specific hour |\n",
    "\n",
    "### Key Design Decisions\n",
    "\n",
    "1. **Training Data Only**: All features are computed from training data to prevent data leakage\n",
    "2. **Normalization**: Daily averages account for different dataset sizes\n",
    "3. **Multiple Centrality Metrics**: Both weighted and unweighted versions capture different aspects of importance\n",
    "4. **Hierarchical Features**: Vertex (airport) and edge (route) features at different granularities\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Graph Features",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python (SAM_3)",
   "language": "python",
   "name": "sam_3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

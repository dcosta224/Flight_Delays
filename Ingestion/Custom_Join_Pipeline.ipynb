{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Flight-Weather Join Pipeline with Kriging Interpolation\n",
    "\n",
    "**Author:** Daniel Costa  \n",
    "**Project:** Flight Delay Prediction (W261 Final Project)  \n",
    "**Dataset:** US Domestic Flights (2015-2019)\n",
    "\n",
    "---\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook implements a **custom spatial-temporal join** between flight data and weather station data using **Ordinary Kriging** for spatial interpolation. Unlike simple nearest-neighbor joins, this approach uses geostatistical methods to produce optimal weighted averages of weather measurements from multiple stations.\n",
    "\n",
    "### Why Kriging?\n",
    "\n",
    "Standard approaches join each flight to the nearest weather station. This is problematic because:\n",
    "1. Weather stations may be far from airports\n",
    "2. Single-station measurements are noisy\n",
    "3. Nearest station may have missing data\n",
    "\n",
    "**Kriging provides optimal interpolation** by:\n",
    "- Weighting multiple nearby stations based on spatial correlation\n",
    "- Accounting for measurement uncertainty\n",
    "- Producing statistically optimal estimates with known variance\n",
    "\n",
    "### Pipeline Architecture\n",
    "\n",
    "```\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502                    KRIGING-BASED WEATHER INTERPOLATION                       \u2502\n",
    "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
    "\u2502                                                                              \u2502\n",
    "\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n",
    "\u2502  \u2502 Flight Data  \u2502     \u2502 Weather Data \u2502     \u2502 Station Coordinates          \u2502 \u2502\n",
    "\u2502  \u2502 (with times) \u2502     \u2502 (hourly obs) \u2502     \u2502 (lat/long per station)       \u2502 \u2502\n",
    "\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n",
    "\u2502         \u2502                    \u2502                            \u2502                  \u2502\n",
    "\u2502         \u25bc                    \u25bc                            \u25bc                  \u2502\n",
    "\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\u2502\n",
    "\u2502  \u2502                    SPATIAL WEIGHT COMPUTATION                            \u2502\u2502\n",
    "\u2502  \u2502  1. Find all stations within spatial_radius (50 km) of each airport     \u2502\u2502\n",
    "\u2502  \u2502  2. Compute Mat\u00e9rn covariance between station pairs                     \u2502\u2502\n",
    "\u2502  \u2502  3. Solve Ordinary Kriging system for optimal weights                   \u2502\u2502\n",
    "\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\u2502\n",
    "\u2502                                    \u2502                                         \u2502\n",
    "\u2502                                    \u25bc                                         \u2502\n",
    "\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\u2502\n",
    "\u2502  \u2502                    TEMPORAL AGGREGATION                                  \u2502\u2502\n",
    "\u2502  \u2502  For each flight, for each station:                                     \u2502\u2502\n",
    "\u2502  \u2502  - Get weather readings from [dep_time - time_radius, dep_time - 2hr]   \u2502\u2502\n",
    "\u2502  \u2502  - Weight by exponential decay: exp(-\u0394t / 5 hours)                      \u2502\u2502\n",
    "\u2502  \u2502  - Aggregate: \u03a3(time_weight \u00d7 measurement) / \u03a3(time_weight)             \u2502\u2502\n",
    "\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\u2502\n",
    "\u2502                                    \u2502                                         \u2502\n",
    "\u2502                                    \u25bc                                         \u2502\n",
    "\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\u2502\n",
    "\u2502  \u2502                    SPATIAL AGGREGATION                                   \u2502\u2502\n",
    "\u2502  \u2502  For each flight:                                                        \u2502\u2502\n",
    "\u2502  \u2502  - Combine station values using Kriging weights                         \u2502\u2502\n",
    "\u2502  \u2502  - Final estimate: \u03a3(spatial_weight \u00d7 station_value) / \u03a3(spatial_weight)\u2502\u2502\n",
    "\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\u2502\n",
    "\u2502                                    \u2502                                         \u2502\n",
    "\u2502                                    \u25bc                                         \u2502\n",
    "\u2502                         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                                 \u2502\n",
    "\u2502                         \u2502  Joined Dataset  \u2502                                 \u2502\n",
    "\u2502                         \u2502  (flight + weather)\u2502                               \u2502\n",
    "\u2502                         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                 \u2502\n",
    "\u2502                                                                              \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "```\n",
    "\n",
    "### Key Parameters\n",
    "\n",
    "| Parameter | Value | Description |\n",
    "|-----------|-------|-------------|\n",
    "| `spatial_radius` | 50 km | Maximum distance to include weather stations |\n",
    "| `time_radius` | 12 hours | Lookback window for weather observations |\n",
    "| `time_dropoff` | 5 hours | Exponential decay constant for temporal weighting |\n",
    "| `\u03bd (nu)` | 1.5 | Mat\u00e9rn smoothness parameter |\n",
    "| `a` | 10 km | Mat\u00e9rn range parameter |\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Setup & Configuration](#1-setup--configuration)\n",
    "2. [Data Ingestion](#2-data-ingestion)\n",
    "3. [Kriging Weight Computation](#3-kriging-weight-computation)\n",
    "   - [3.1 Spatial Covariance (Mat\u00e9rn)](#31-spatial-covariance-mat\u00e9rn)\n",
    "   - [3.2 Ordinary Kriging Solver](#32-ordinary-kriging-solver)\n",
    "4. [Custom Join Pipeline](#4-custom-join-pipeline)\n",
    "5. [Execution](#5-execution)\n",
    "6. [Summary](#6-summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Imports\n",
    "# -----------------------------------------------------------------------------\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import (\n",
    "    col, udf, desc, regexp_extract, expr,\n",
    "    min as spark_min, max as spark_max, when\n",
    ")\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField, StringType, FloatType, DoubleType, TimestampType\n",
    ")\n",
    "\n",
    "from datetime import datetime\n",
    "from timezonefinder import TimezoneFinder\n",
    "import pytz\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.linalg import cho_factor, cho_solve\n",
    "from scipy.special import kv, gamma\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Configuration\n",
    "# -----------------------------------------------------------------------------\n",
    "# Dataset paths\n",
    "FLIGHT_DATASETS = {\n",
    "    '3m': 'dbfs:/mnt/mids-w261/datasets_final_project_2022/parquet_airlines_data_3m/',\n",
    "    '1y': 'dbfs:/mnt/mids-w261/datasets_final_project_2022/parquet_airlines_data_1y/',\n",
    "    '5y': 'dbfs:/mnt/mids-w261/datasets_final_project_2022/parquet_airlines_data/'\n",
    "}\n",
    "\n",
    "WEATHER_DATASETS = {\n",
    "    '3m': 'dbfs:/mnt/mids-w261/datasets_final_project_2022/parquet_weather_data_3m/',\n",
    "    '1y': 'dbfs:/mnt/mids-w261/datasets_final_project_2022/parquet_weather_data_1y/',\n",
    "    '5y': 'dbfs:/mnt/mids-w261/datasets_final_project_2022/parquet_weather_data/'\n",
    "}\n",
    "\n",
    "AIRPORT_CODES_PATH = \"dbfs:/mnt/mids-w261/airport-codes_csv.csv\"\n",
    "OUTPUT_BASE_PATH = \"dbfs:/student-groups/Group_2_2\"\n",
    "\n",
    "# Kriging parameters\n",
    "SPATIAL_RADIUS = 50      # km - max distance to include weather stations\n",
    "TIME_RADIUS = 12         # hours - lookback window for weather\n",
    "TIME_DROPOFF = 5         # hours - exponential decay constant\n",
    "NU_S = 1.5               # Mat\u00e9rn smoothness (1.5 = moderate smoothness)\n",
    "A_S = 10.0               # Mat\u00e9rn range parameter (km)\n",
    "SIGMA2_S = 1.0           # Mat\u00e9rn variance (assumes standardized data)\n",
    "EPSILON = 1e-10          # Numerical stability constant\n",
    "\n",
    "# Reference time for timezone calculations\n",
    "REF_TIME = \"2000-01-05 00:00:00\"\n",
    "\n",
    "# Earth's radius for distance calculations\n",
    "EARTH_RADIUS_M = 6_371_008.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Timezone Conversion Helper\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def _get_timezone_finder():\n",
    "    \"\"\"Lazy singleton for TimezoneFinder (one per executor).\"\"\"\n",
    "    if not hasattr(_get_timezone_finder, \"instance\"):\n",
    "        _get_timezone_finder.instance = TimezoneFinder()\n",
    "    return _get_timezone_finder.instance\n",
    "\n",
    "\n",
    "def get_gmt_offset(local_time_str: str, lat: float, long: float) -> float:\n",
    "    \"\"\"\n",
    "    Get GMT offset in hours for a location at a given time.\n",
    "    \n",
    "    Args:\n",
    "        local_time_str: Local time as string \"YYYY-MM-DD HH:MM:SS\"\n",
    "        lat: Latitude in degrees\n",
    "        long: Longitude in degrees\n",
    "        \n",
    "    Returns:\n",
    "        Offset from GMT in hours (e.g., -8.0 for PST)\n",
    "    \"\"\"\n",
    "    tf = _get_timezone_finder()\n",
    "    tz_name = tf.timezone_at(lat=lat, lng=long)\n",
    "    if tz_name is None:\n",
    "        return None\n",
    "    \n",
    "    tz = pytz.timezone(tz_name)\n",
    "    local_time = datetime.strptime(local_time_str, \"%Y-%m-%d %H:%M:%S\")\n",
    "    local_dt = tz.localize(local_time)\n",
    "    \n",
    "    return float(local_dt.utcoffset().total_seconds() / 3600.0)\n",
    "\n",
    "\n",
    "# Register as Spark UDF\n",
    "get_gmt_offset_udf = udf(get_gmt_offset, FloatType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Data Ingestion Functions\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def ingest_flight_data(parquet_path: str):\n",
    "    \"\"\"\n",
    "    Load and preprocess flight data.\n",
    "    \n",
    "    Creates unique flight identifier and parses departure timestamps.\n",
    "    \n",
    "    Args:\n",
    "        parquet_path: Path to flight parquet files\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (df_flight_times, df_flights)\n",
    "    \"\"\"\n",
    "    print(f\"Loading flight data from {parquet_path}...\")\n",
    "    \n",
    "    df_flights = spark.read.parquet(parquet_path).dropDuplicates()\n",
    "    \n",
    "    # Create unique flight identifier\n",
    "    df_flights = df_flights.withColumn(\n",
    "        \"flight_uid\",\n",
    "        F.concat(\n",
    "            F.col(\"ORIGIN\"), F.lit(\"-\"),\n",
    "            F.col(\"FL_DATE\"), F.lit(\"-\"),\n",
    "            F.coalesce(F.col(\"OP_CARRIER_AIRLINE_ID\").cast(\"string\"), \n",
    "                       F.floor(F.rand() * 100000 + 1).cast(\"string\")), F.lit(\"-\"),\n",
    "            F.coalesce(F.col(\"OP_CARRIER_FL_NUM\").cast(\"string\"),\n",
    "                       F.floor(F.rand() * 100000 + 1).cast(\"string\")), F.lit(\"-\"),\n",
    "            F.coalesce(F.col(\"TAIL_NUM\"),\n",
    "                       F.floor(F.rand() * 100000 + 1).cast(\"string\")), F.lit(\"-\"),\n",
    "            F.col(\"CRS_DEP_TIME\")\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Extract time components for timestamp construction\n",
    "    df_flight_times = (\n",
    "        df_flights\n",
    "        .select(\"flight_uid\", \"YEAR\", \"MONTH\", \"DAY_OF_MONTH\", \"ORIGIN\", \"DEST\",\n",
    "                \"CRS_DEP_TIME\", \"DEP_TIME\", \"DEP_DELAY\", \"CRS_ARR_TIME\", \"ARR_TIME\", \"ARR_DELAY\")\n",
    "        .withColumn(\"dep_hour\", F.lpad((F.floor(F.col(\"CRS_DEP_TIME\") / 100)).cast(\"string\"), 2, \"0\"))\n",
    "        .withColumn(\"month_str\", F.lpad(F.col(\"MONTH\").cast(\"string\"), 2, \"0\"))\n",
    "        .withColumn(\"day_str\", F.lpad(F.col(\"DAY_OF_MONTH\").cast(\"string\"), 2, \"0\"))\n",
    "        .withColumn(\n",
    "            \"dep_timestamp\",\n",
    "            F.to_timestamp(\n",
    "                F.concat(\n",
    "                    F.col(\"YEAR\"), F.lit(\"-\"), F.col(\"month_str\"), F.lit(\"-\"), F.col(\"day_str\"),\n",
    "                    F.lit(\"T\"), F.col(\"dep_hour\"), F.lit(\":\"),\n",
    "                    F.lpad((F.col(\"CRS_DEP_TIME\") % 100).cast(\"string\"), 2, \"0\"), F.lit(\":00\")\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        .dropna()\n",
    "    )\n",
    "    \n",
    "    print(f\"  Loaded {df_flights.count():,} flights\")\n",
    "    return df_flight_times, df_flights\n",
    "\n",
    "\n",
    "def ingest_airport_data(df_flight_times):\n",
    "    \"\"\"\n",
    "    Load airport coordinates and compute timezone offsets.\n",
    "    \n",
    "    Args:\n",
    "        df_flight_times: Flight DataFrame to get valid airport codes\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with airport codes, coordinates, and GMT offsets\n",
    "    \"\"\"\n",
    "    print(\"Loading airport data...\")\n",
    "    \n",
    "    df_airports = (\n",
    "        spark.read.csv(AIRPORT_CODES_PATH, header=True, inferSchema=True)\n",
    "        .withColumn(\"long\", F.split(F.col(\"coordinates\"), \",\")[0].cast(\"float\"))\n",
    "        .withColumn(\"lat\", F.split(F.col(\"coordinates\"), \",\")[1].cast(\"float\"))\n",
    "        .select(\"iata_code\", \"long\", \"lat\")\n",
    "        .dropna()\n",
    "    )\n",
    "    \n",
    "    # Add timezone offset\n",
    "    df_airports = df_airports.withColumn(\n",
    "        \"gmt_offset_hours\",\n",
    "        get_gmt_offset_udf(F.lit(REF_TIME), F.col(\"lat\"), F.col(\"long\"))\n",
    "    )\n",
    "    \n",
    "    # Filter to airports in our flight data\n",
    "    valid_codes = set(row[\"ORIGIN\"] for row in df_flight_times.select(\"ORIGIN\").distinct().collect())\n",
    "    df_airports_us = df_airports.filter(df_airports.iata_code.isin(valid_codes))\n",
    "    \n",
    "    print(f\"  Found {df_airports_us.count()} airports\")\n",
    "    return df_airports_us\n",
    "\n",
    "\n",
    "def ingest_weather_data(parquet_path: str):\n",
    "    \"\"\"\n",
    "    Load and preprocess weather data.\n",
    "    \n",
    "    Cleans numeric columns, extracts cloud coverage, and computes wind components.\n",
    "    \n",
    "    Args:\n",
    "        parquet_path: Path to weather parquet files\n",
    "        \n",
    "    Returns:\n",
    "        Weather DataFrame with cleaned features\n",
    "    \"\"\"\n",
    "    print(f\"Loading weather data from {parquet_path}...\")\n",
    "    \n",
    "    df_weather = spark.read.parquet(parquet_path)\n",
    "    \n",
    "    # Clean numeric columns (remove special characters)\n",
    "    exclude_cols = [\"NAME\", \"REPORT_TYPE\", \"HourlySkyConditions\", \"STATION\", \"DATE\"]\n",
    "    cols_to_clean = [c for c in df_weather.columns if c not in exclude_cols]\n",
    "    \n",
    "    for c in cols_to_clean:\n",
    "        df_weather = df_weather.withColumn(\n",
    "            c,\n",
    "            F.when(F.col(c).contains(\"*\"), None)\n",
    "            .otherwise(F.regexp_replace(F.col(c), r\"[^0-9.\\-]\", \"\"))\n",
    "            .cast(\"double\")\n",
    "        )\n",
    "    \n",
    "    # Extract cloud coverage (oktas) and elevation from sky conditions\n",
    "    df_weather = (\n",
    "        df_weather\n",
    "        .withColumn(\"oktas\", regexp_extract(col(\"HourlySkyConditions\"), r\".*([A-Z]{3}:)\\s*([0-9]*)\", 2).cast(\"int\"))\n",
    "        .withColumn(\"elev_hundreds_ft\", regexp_extract(col(\"HourlySkyConditions\"), r\".*([A-Z]{3}:)\\s*([0-9]*)\\s*([0-9]*)\", 3).cast(\"int\"))\n",
    "    )\n",
    "    \n",
    "    # Decompose wind into north/east components\n",
    "    df_weather = (\n",
    "        df_weather\n",
    "        .withColumn(\"from_east\", -1 * col(\"HourlyWindSpeed\") * F.sin(col(\"HourlyWindDirection\") * F.pi() / 180))\n",
    "        .withColumn(\"from_north\", -1 * col(\"HourlyWindSpeed\") * F.cos(col(\"HourlyWindDirection\") * F.pi() / 180))\n",
    "        .withColumn(\"DATE\", col(\"DATE\").cast(\"timestamp\"))\n",
    "    )\n",
    "    \n",
    "    print(f\"  Loaded {df_weather.count():,} weather observations\")\n",
    "    return df_weather\n",
    "\n",
    "\n",
    "def ingest_station_data(df_weather):\n",
    "    \"\"\"\n",
    "    Extract unique weather station locations.\n",
    "    \n",
    "    Args:\n",
    "        df_weather: Weather DataFrame with station info\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with station codes, coordinates, and GMT offsets\n",
    "    \"\"\"\n",
    "    print(\"Extracting station locations...\")\n",
    "    \n",
    "    df_stations = (\n",
    "        df_weather\n",
    "        .groupBy(\"STATION\")\n",
    "        .agg(\n",
    "            F.first(F.col(\"LONGITUDE\")).alias(\"long\"),\n",
    "            F.first(F.col(\"LATITUDE\")).alias(\"lat\")\n",
    "        )\n",
    "        .dropna()\n",
    "        .withColumn(\n",
    "            \"gmt_offset_hours\",\n",
    "            get_gmt_offset_udf(F.lit(REF_TIME), F.col(\"lat\"), F.col(\"long\"))\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    print(f\"  Found {df_stations.count()} weather stations\")\n",
    "    return df_stations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Kriging Weight Computation\n",
    "\n",
    "### 3.1 Spatial Covariance (Mat\u00e9rn)\n",
    "\n",
    "The Mat\u00e9rn covariance function models spatial correlation between weather stations. It's parameterized by:\n",
    "- **\u03bd (nu)**: Smoothness parameter (1.5 gives moderate smoothness)\n",
    "- **a**: Range parameter (correlation decays to ~0.14 at distance a)\n",
    "- **\u03c3\u00b2**: Variance parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Distance and Covariance Functions\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def haversine_distance(lat1, lon1, lat2, lon2):\n",
    "    \"\"\"\n",
    "    Compute great-circle distance between points using Haversine formula.\n",
    "    \n",
    "    Args:\n",
    "        lat1, lon1: First point coordinates (degrees)\n",
    "        lat2, lon2: Second point coordinates (degrees)\n",
    "        \n",
    "    Returns:\n",
    "        Distance in kilometers\n",
    "    \"\"\"\n",
    "    lat1, lon1 = np.radians(np.asarray(lat1)), np.radians(np.asarray(lon1))\n",
    "    lat2, lon2 = np.radians(np.asarray(lat2)), np.radians(np.asarray(lon2))\n",
    "    \n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    \n",
    "    a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2\n",
    "    return (2.0 * EARTH_RADIUS_M * np.arcsin(np.minimum(1.0, np.sqrt(a)))) / 1000\n",
    "\n",
    "\n",
    "def matern_covariance(h: float) -> float:\n",
    "    \"\"\"\n",
    "    Compute Mat\u00e9rn covariance for distance h.\n",
    "    \n",
    "    C(h) = \u03c3\u00b2 * (2^(1-\u03bd)/\u0393(\u03bd)) * (h/a)^\u03bd * K_\u03bd(h/a)\n",
    "    \n",
    "    Args:\n",
    "        h: Distance in kilometers\n",
    "        \n",
    "    Returns:\n",
    "        Covariance value\n",
    "    \"\"\"\n",
    "    h = max(h, EPSILON)\n",
    "    val = (2**(1-NU_S)) / gamma(NU_S) * (h/A_S)**NU_S * kv(NU_S, h/A_S)\n",
    "    return float(SIGMA2_S * val) if not math.isnan(val) else float(SIGMA2_S)\n",
    "\n",
    "\n",
    "# Covariance at distance 0 (equals variance)\n",
    "K_0 = matern_covariance(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Ordinary Kriging Solver\n",
    "\n",
    "Ordinary Kriging finds optimal weights that:\n",
    "1. Are unbiased (weights sum to 1)\n",
    "2. Minimize estimation variance\n",
    "\n",
    "The system solves: **K\u00b7w = k** subject to **\u03a3w = 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Ordinary Kriging Solver\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def find_nearest_stations(airport_points, station_points, top_n=100):\n",
    "    \"\"\"\n",
    "    Find nearest weather stations for each airport.\n",
    "    \n",
    "    Args:\n",
    "        airport_points: Array of [long, lat, code] for airports\n",
    "        station_points: Array of [long, lat, code] for stations\n",
    "        top_n: Maximum stations to return per airport\n",
    "        \n",
    "    Returns:\n",
    "        List of (airport_code, [(distance, station_code), ...])\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    airport_codes = airport_points[:, -1]\n",
    "    airport_coords = airport_points[:, :-1].astype(float)\n",
    "    station_codes = station_points[:, -1]\n",
    "    station_coords = station_points[:, :-1].astype(float)\n",
    "    \n",
    "    for i, (lon, lat) in enumerate(airport_coords):\n",
    "        distances = haversine_distance(lat, lon, station_coords[:, 1], station_coords[:, 0])\n",
    "        idx = np.argsort(distances)[:top_n]\n",
    "        results.append((airport_codes[i], list(zip(distances[idx], station_codes[idx]))))\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def build_neighbor_dataframe(df_stations, df_airports):\n",
    "    \"\"\"\n",
    "    Build DataFrame of airport-station distances.\n",
    "    \n",
    "    Args:\n",
    "        df_stations: Station DataFrame with coordinates\n",
    "        df_airports: Airport DataFrame with coordinates\n",
    "        \n",
    "    Returns:\n",
    "        Spark DataFrame with (airport_code, station_code, distance)\n",
    "    \"\"\"\n",
    "    print(\"Building airport-station neighbor relationships...\")\n",
    "    \n",
    "    station_points = np.array(df_stations.select(\"long\", \"lat\", \"STATION\").collect())\n",
    "    airport_points = np.array(df_airports.select(\"long\", \"lat\", \"iata_code\").collect())\n",
    "    \n",
    "    nearest = find_nearest_stations(airport_points, station_points)\n",
    "    \n",
    "    data = []\n",
    "    for airport_code, distances in nearest:\n",
    "        for dist, station_code in distances:\n",
    "            data.append([airport_code, station_code, float(dist)])\n",
    "    \n",
    "    schema = StructType([\n",
    "        StructField(\"airport_code\", StringType(), True),\n",
    "        StructField(\"station_code\", StringType(), True),\n",
    "        StructField(\"distance\", FloatType(), True)\n",
    "    ])\n",
    "    \n",
    "    return spark.createDataFrame(pd.DataFrame(data, columns=[\"airport_code\", \"station_code\", \"distance\"]), schema)\n",
    "\n",
    "\n",
    "def solve_kriging_weights(cov_df, nugget_rel=1e-10):\n",
    "    \"\"\"\n",
    "    Solve Ordinary Kriging system for optimal station weights.\n",
    "    \n",
    "    Args:\n",
    "        cov_df: DataFrame with station covariances for one airport\n",
    "        nugget_rel: Nugget effect for numerical stability\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (weights, lagrange_multiplier, variance, station_codes)\n",
    "    \"\"\"\n",
    "    station_coords = cov_df[[\"lat_station\", \"long_station\"]].values\n",
    "    station_codes = cov_df[\"station_code\"].values\n",
    "    n = len(station_coords)\n",
    "    \n",
    "    # Build station-station covariance matrix K_xx\n",
    "    K_xx = np.empty((n, n), dtype=float)\n",
    "    for i in range(n):\n",
    "        K_xx[i, i] = K_0\n",
    "        for j in range(i+1, n):\n",
    "            d = haversine_distance(station_coords[i, 0], station_coords[i, 1],\n",
    "                                   station_coords[j, 0], station_coords[j, 1])\n",
    "            cij = matern_covariance(d)\n",
    "            K_xx[i, j] = K_xx[j, i] = cij\n",
    "    \n",
    "    # Airport-station covariances (already computed)\n",
    "    K_x = cov_df[\"C_space\"].to_numpy(dtype=float)\n",
    "    \n",
    "    # Add nugget for numerical stability\n",
    "    np.fill_diagonal(K_xx, np.diag(K_xx) + nugget_rel * K_0)\n",
    "    \n",
    "    # Solve using Cholesky decomposition\n",
    "    ones = np.ones(n, dtype=float)\n",
    "    c_fac = cho_factor(K_xx, check_finite=False)\n",
    "    alpha = cho_solve(c_fac, ones, check_finite=False)\n",
    "    beta = cho_solve(c_fac, K_x, check_finite=False)\n",
    "    \n",
    "    # Compute weights with Lagrange multiplier for unbiasedness\n",
    "    denom = ones @ alpha\n",
    "    lam = (1.0 - ones @ beta) / denom\n",
    "    w = beta + lam * alpha\n",
    "    \n",
    "    # Kriging variance\n",
    "    var = max(0.0, K_0 - w @ K_x - lam)\n",
    "    \n",
    "    return w, lam, float(var), station_codes\n",
    "\n",
    "\n",
    "def compute_spatial_weights(neighbor_df, df_stations, df_airports, spatial_radius, time_radius):\n",
    "    \"\"\"\n",
    "    Compute Kriging weights for all airport-station pairs.\n",
    "    \n",
    "    Args:\n",
    "        neighbor_df: DataFrame of airport-station distances\n",
    "        df_stations: Station DataFrame\n",
    "        df_airports: Airport DataFrame\n",
    "        spatial_radius: Maximum distance to include\n",
    "        time_radius: Time radius for weight filtering\n",
    "        \n",
    "    Returns:\n",
    "        Spark DataFrame with station weights per airport\n",
    "    \"\"\"\n",
    "    print(f\"Computing Kriging weights (spatial_radius={spatial_radius}km)...\")\n",
    "    \n",
    "    # Filter to stations within radius and compute covariances\n",
    "    spatial_cov = neighbor_df.toPandas()\n",
    "    spatial_cov = spatial_cov[spatial_cov[\"distance\"] < spatial_radius].copy()\n",
    "    spatial_cov[\"C_space\"] = spatial_cov[\"distance\"].apply(matern_covariance)\n",
    "    \n",
    "    # Join with coordinates\n",
    "    stations_pd = df_stations.toPandas()\n",
    "    airports_pd = df_airports.toPandas()\n",
    "    \n",
    "    spatial_cov = (\n",
    "        spatial_cov\n",
    "        .merge(stations_pd, left_on=\"station_code\", right_on=\"STATION\")\n",
    "        .rename(columns={\"lat\": \"lat_station\", \"long\": \"long_station\"})\n",
    "        .drop(columns=[\"STATION\"])\n",
    "        .merge(airports_pd, left_on=\"airport_code\", right_on=\"iata_code\")\n",
    "        .rename(columns={\"lat\": \"lat_airport\", \"long\": \"long_airport\"})\n",
    "        .drop(columns=[\"iata_code\"])\n",
    "    )\n",
    "    \n",
    "    # Solve Kriging for each airport\n",
    "    results = []\n",
    "    for airport_code, group in spatial_cov.groupby(\"airport_code\"):\n",
    "        w, lam, var, station_codes = solve_kriging_weights(group)\n",
    "        for weight, station in zip(w, station_codes):\n",
    "            results.append([airport_code, station, weight, lam, var])\n",
    "    \n",
    "    weight_df = pd.DataFrame(results, columns=[\"airport_code\", \"station_code\", \"weight\", \"lambda\", \"variance\"])\n",
    "    \n",
    "    # Filter small weights (won't contribute meaningfully)\n",
    "    min_weight = 0.001 / np.exp(-time_radius / TIME_DROPOFF)\n",
    "    weight_df = weight_df[abs(weight_df[\"weight\"]) > min_weight]\n",
    "    \n",
    "    # Merge back with spatial info\n",
    "    weight_df = spatial_cov.merge(weight_df, on=[\"airport_code\", \"station_code\"], how=\"inner\")\n",
    "    \n",
    "    print(f\"  Computed weights for {weight_df['airport_code'].nunique()} airports\")\n",
    "    return spark.createDataFrame(weight_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Custom Join Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Custom Join Pipeline\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def add_gmt_timestamp(df, new_col, timestamp_col):\n",
    "    \"\"\"Convert local timestamp to GMT.\"\"\"\n",
    "    return df.withColumn(\n",
    "        new_col,\n",
    "        (col(timestamp_col).cast(\"long\") - (expr(\"gmt_offset_hours\") * 3600)).cast(\"timestamp\")\n",
    "    )\n",
    "\n",
    "\n",
    "def custom_weather_join(df_flight_times, df_neighbor_weights, df_weather, df_flights, time_radius):\n",
    "    \"\"\"\n",
    "    Perform spatial-temporal join of flights with weather using Kriging weights.\n",
    "    \n",
    "    The join:\n",
    "    1. For each flight, finds all weather readings within time_radius hours\n",
    "    2. Weights readings by temporal proximity (exponential decay)\n",
    "    3. Aggregates across time for each station\n",
    "    4. Combines stations using pre-computed Kriging weights\n",
    "    \n",
    "    Args:\n",
    "        df_flight_times: Flight times DataFrame\n",
    "        df_neighbor_weights: Kriging weights DataFrame\n",
    "        df_weather: Weather observations DataFrame\n",
    "        df_flights: Original flights DataFrame\n",
    "        time_radius: Hours of weather lookback\n",
    "        \n",
    "    Returns:\n",
    "        Joined DataFrame with interpolated weather features\n",
    "    \"\"\"\n",
    "    print(\"Performing custom spatial-temporal join...\")\n",
    "    \n",
    "    # Register temp views for SQL\n",
    "    df_flight_times.createOrReplaceTempView(\"flights\")\n",
    "    df_neighbor_weights.createOrReplaceTempView(\"weights\")\n",
    "    df_weather.createOrReplaceTempView(\"weather\")\n",
    "    \n",
    "    # SQL query for weighted aggregation\n",
    "    result_df = spark.sql(f\"\"\"\n",
    "    WITH flight_readings AS (\n",
    "        -- Get all weather readings for each flight within time window\n",
    "        SELECT \n",
    "            f.flight_uid,\n",
    "            w.*,\n",
    "            EXP((CAST(f.dep_timestamp_gmt AS double) - CAST(w.DATE_gmt AS double)) / (-3600.0 * {TIME_DROPOFF})) AS t_weight,\n",
    "            wt.weight AS s_weight\n",
    "        FROM flights f\n",
    "        JOIN weights wt ON f.ORIGIN = wt.airport_code\n",
    "        JOIN weather w ON wt.station_code = w.STATION\n",
    "            AND w.DATE_gmt <= f.dep_timestamp_gmt - INTERVAL 2 HOURS\n",
    "            AND w.DATE_gmt >= f.dep_timestamp_gmt - INTERVAL {time_radius} HOURS\n",
    "    ),\n",
    "    station_agg AS (\n",
    "        -- Aggregate over time for each station\n",
    "        SELECT\n",
    "            flight_uid, STATION, FIRST(s_weight) AS s_weight,\n",
    "            SUM(t_weight * HourlyDryBulbTemperature) / NULLIF(SUM(CASE WHEN HourlyDryBulbTemperature IS NULL THEN 0 ELSE t_weight END), 0) AS temp,\n",
    "            SUM(t_weight * HourlyDewPointTemperature) / NULLIF(SUM(CASE WHEN HourlyDewPointTemperature IS NULL THEN 0 ELSE t_weight END), 0) AS dew_temp,\n",
    "            SUM(t_weight * HourlyRelativeHumidity) / NULLIF(SUM(CASE WHEN HourlyRelativeHumidity IS NULL THEN 0 ELSE t_weight END), 0) AS humidity,\n",
    "            SUM(t_weight * HourlyAltimeterSetting) / NULLIF(SUM(CASE WHEN HourlyAltimeterSetting IS NULL THEN 0 ELSE t_weight END), 0) AS altimeter,\n",
    "            SUM(t_weight * HourlyVisibility) / NULLIF(SUM(CASE WHEN HourlyVisibility IS NULL THEN 0 ELSE t_weight END), 0) AS visibility,\n",
    "            SUM(t_weight * HourlyStationPressure) / NULLIF(SUM(CASE WHEN HourlyStationPressure IS NULL THEN 0 ELSE t_weight END), 0) AS pressure,\n",
    "            SUM(t_weight * HourlyWetBulbTemperature) / NULLIF(SUM(CASE WHEN HourlyWetBulbTemperature IS NULL THEN 0 ELSE t_weight END), 0) AS wet_temp,\n",
    "            SUM(t_weight * HourlyPrecipitation) / NULLIF(SUM(CASE WHEN HourlyPrecipitation IS NULL THEN 0 ELSE t_weight END), 0) AS precip,\n",
    "            SUM(t_weight * oktas) / NULLIF(SUM(CASE WHEN oktas IS NULL THEN 0 ELSE t_weight END), 0) AS cloud_cover,\n",
    "            SUM(t_weight * elev_hundreds_ft) / NULLIF(SUM(CASE WHEN elev_hundreds_ft IS NULL THEN 0 ELSE t_weight END), 0) AS cloud_elev,\n",
    "            SUM(t_weight * from_north) / NULLIF(SUM(CASE WHEN from_north IS NULL THEN 0 ELSE t_weight END), 0) AS wind_north,\n",
    "            SUM(t_weight * from_east) / NULLIF(SUM(CASE WHEN from_east IS NULL THEN 0 ELSE t_weight END), 0) AS wind_east\n",
    "        FROM flight_readings\n",
    "        GROUP BY flight_uid, STATION\n",
    "    ),\n",
    "    spatial_agg AS (\n",
    "        -- Aggregate across stations using Kriging weights\n",
    "        SELECT\n",
    "            flight_uid,\n",
    "            SUM(s_weight * temp) / NULLIF(SUM(CASE WHEN temp IS NULL THEN 0 ELSE s_weight END), 0) AS HourlyDryBulbTemperature,\n",
    "            SUM(s_weight * dew_temp) / NULLIF(SUM(CASE WHEN dew_temp IS NULL THEN 0 ELSE s_weight END), 0) AS HourlyDewPointTemperature,\n",
    "            SUM(s_weight * humidity) / NULLIF(SUM(CASE WHEN humidity IS NULL THEN 0 ELSE s_weight END), 0) AS HourlyRelativeHumidity,\n",
    "            SUM(s_weight * altimeter) / NULLIF(SUM(CASE WHEN altimeter IS NULL THEN 0 ELSE s_weight END), 0) AS HourlyAltimeterSetting,\n",
    "            SUM(s_weight * visibility) / NULLIF(SUM(CASE WHEN visibility IS NULL THEN 0 ELSE s_weight END), 0) AS HourlyVisibility,\n",
    "            SUM(s_weight * pressure) / NULLIF(SUM(CASE WHEN pressure IS NULL THEN 0 ELSE s_weight END), 0) AS HourlyStationPressure,\n",
    "            SUM(s_weight * wet_temp) / NULLIF(SUM(CASE WHEN wet_temp IS NULL THEN 0 ELSE s_weight END), 0) AS HourlyWetBulbTemperature,\n",
    "            SUM(s_weight * precip) / NULLIF(SUM(CASE WHEN precip IS NULL THEN 0 ELSE s_weight END), 0) AS HourlyPrecipitation,\n",
    "            SUM(s_weight * cloud_cover) / NULLIF(SUM(CASE WHEN cloud_cover IS NULL THEN 0 ELSE s_weight END), 0) AS HourlyCloudCoverage,\n",
    "            SUM(s_weight * cloud_elev) / NULLIF(SUM(CASE WHEN cloud_elev IS NULL THEN 0 ELSE s_weight END), 0) AS HourlyCloudElevation,\n",
    "            SQRT(POWER(SUM(s_weight * wind_north) / NULLIF(SUM(CASE WHEN wind_north IS NULL THEN 0 ELSE s_weight END), 0), 2) +\n",
    "                 POWER(SUM(s_weight * wind_east) / NULLIF(SUM(CASE WHEN wind_east IS NULL THEN 0 ELSE s_weight END), 0), 2)) AS HourlyWindSpeed\n",
    "        FROM station_agg\n",
    "        GROUP BY flight_uid\n",
    "    )\n",
    "    SELECT * FROM spatial_agg\n",
    "    \"\"\")\n",
    "    \n",
    "    # Join with original flight data\n",
    "    final_df = df_flights.join(result_df, on=\"flight_uid\", how=\"left\")\n",
    "    \n",
    "    print(\"  Join complete\")\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_custom_join_pipeline(dataset_id: str, spatial_radius: int = 50, time_radius: int = 12):\n",
    "    \"\"\"\n",
    "    Execute the full custom join pipeline for a dataset.\n",
    "    \n",
    "    Args:\n",
    "        dataset_id: Dataset identifier ('3m', '1y', '5y')\n",
    "        spatial_radius: Maximum station distance in km\n",
    "        time_radius: Weather lookback hours\n",
    "        \n",
    "    Returns:\n",
    "        Joined DataFrame\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Running pipeline: {dataset_id}, spatial={spatial_radius}km, time={time_radius}hrs\")\n",
    "    print('='*60)\n",
    "    \n",
    "    # 1. Ingest data\n",
    "    df_flight_times, df_flights = ingest_flight_data(FLIGHT_DATASETS[dataset_id])\n",
    "    df_airports = ingest_airport_data(df_flight_times)\n",
    "    df_weather = ingest_weather_data(WEATHER_DATASETS[dataset_id])\n",
    "    df_stations = ingest_station_data(df_weather)\n",
    "    \n",
    "    # 2. Add airport coordinates to flights\n",
    "    df_flight_times = df_flight_times.join(\n",
    "        df_airports, \n",
    "        df_flight_times.ORIGIN == df_airports.iata_code, \n",
    "        how=\"left\"\n",
    "    )\n",
    "    \n",
    "    # 3. Compute Kriging weights\n",
    "    neighbor_df = build_neighbor_dataframe(df_stations, df_airports)\n",
    "    df_weights = compute_spatial_weights(neighbor_df, df_stations, df_airports, spatial_radius, time_radius)\n",
    "    \n",
    "    # 4. Prepare data for join (convert to GMT)\n",
    "    df_flight_times = add_gmt_timestamp(df_flight_times, \"dep_timestamp_gmt\", \"dep_timestamp\")\n",
    "    \n",
    "    station_codes = [r[\"station_code\"] for r in df_weights.select(\"station_code\").distinct().collect()]\n",
    "    df_weather_filtered = (\n",
    "        df_weather\n",
    "        .filter(df_weather[\"STATION\"].isin(station_codes))\n",
    "        .join(df_stations, on=\"STATION\", how=\"left\")\n",
    "    )\n",
    "    df_weather_filtered = add_gmt_timestamp(df_weather_filtered, \"DATE_gmt\", \"DATE\")\n",
    "    \n",
    "    # 5. Perform join\n",
    "    final_df = custom_weather_join(df_flight_times, df_weights, df_weather_filtered, df_flights, time_radius)\n",
    "    \n",
    "    print(f\"\\n\u2713 Pipeline complete: {final_df.count():,} rows\")\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the pipeline for 5-year dataset\n",
    "spark.sparkContext.setCheckpointDir(\"/tmp/spark-checkpoints\")\n",
    "\n",
    "final_df = run_custom_join_pipeline(\n",
    "    dataset_id=\"5y\",\n",
    "    spatial_radius=SPATIAL_RADIUS,\n",
    "    time_radius=TIME_RADIUS\n",
    ")\n",
    "\n",
    "# Save results\n",
    "output_path = f\"{OUTPUT_BASE_PATH}/5_year_custom_joined\"\n",
    "final_df.write.mode(\"overwrite\").parquet(output_path)\n",
    "print(f\"\\nSaved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Summary\n",
    "\n",
    "This notebook implements a sophisticated **Kriging-based spatial interpolation** pipeline for joining flight data with weather observations.\n",
    "\n",
    "### Key Features\n",
    "\n",
    "| Feature | Description |\n",
    "|---------|-------------|\n",
    "| **Mat\u00e9rn Covariance** | Models spatial correlation with tunable smoothness |\n",
    "| **Ordinary Kriging** | Provides optimal unbiased estimates with minimum variance |\n",
    "| **Temporal Weighting** | Exponential decay prioritizes recent observations |\n",
    "| **Multi-Station Fusion** | Combines data from multiple stations for robustness |\n",
    "\n",
    "### Weather Features Produced\n",
    "\n",
    "| Feature | Description |\n",
    "|---------|-------------|\n",
    "| `HourlyDryBulbTemperature` | Air temperature (\u00b0F) |\n",
    "| `HourlyDewPointTemperature` | Dew point (\u00b0F) |\n",
    "| `HourlyRelativeHumidity` | Relative humidity (%) |\n",
    "| `HourlyVisibility` | Visibility (miles) |\n",
    "| `HourlyStationPressure` | Barometric pressure (inHg) |\n",
    "| `HourlyPrecipitation` | Precipitation (inches) |\n",
    "| `HourlyCloudCoverage` | Cloud cover (oktas, 0-8) |\n",
    "| `HourlyCloudElevation` | Cloud base height (hundreds of feet) |\n",
    "| `HourlyWindSpeed` | Wind speed (mph) |\n",
    "\n",
    "### Advantages Over Simple Nearest-Neighbor Join\n",
    "\n",
    "1. **Reduced Noise**: Averaging multiple stations reduces measurement error\n",
    "2. **Missing Data Handling**: If one station has gaps, others fill in\n",
    "3. **Optimal Weighting**: Kriging weights minimize estimation variance\n",
    "4. **Spatial Consistency**: Nearby airports get similar weather estimates"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "(Clone) custom_join_pipeline",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
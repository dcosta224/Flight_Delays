{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "33522dac-766e-4b34-a31c-32101aef2e13",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Multi-tower neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ce9ab048-8dcf-444c-af06-6d4505303562",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d920560c-696d-4dff-b153-a60ed48f98d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.21.3\nCreated new experiment with ID: 1920497510759002\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql import Window\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "from pyspark.ml.regression import LinearRegression, RandomForestRegressor\n",
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "from pyspark.ml.evaluation import RegressionEvaluator, BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from xgboost.spark import SparkXGBRegressor\n",
    "\n",
    "from mlflow.models import infer_signature\n",
    "\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import mlflow\n",
    "print(mlflow.__version__)\n",
    "\n",
    "import os\n",
    "\n",
    "spark.conf.set(\"spark.databricks.mlflow.trackMLlib.enabled\", \"true\")\n",
    "\n",
    "RANDOM_SEED = 0\n",
    "# Define experiment name with proper Databricks path\n",
    "EXPERIMENT_NAME = \"/Shared/team_2_2/mlflow-nn-tower-hyperparameters\"\n",
    "# Create the experiment if it doesn't exist\n",
    "try:\n",
    "    experiment = mlflow.get_experiment_by_name(EXPERIMENT_NAME)\n",
    "    if experiment is None:\n",
    "        experiment_id = mlflow.create_experiment(EXPERIMENT_NAME)\n",
    "        print(f\"Created new experiment with ID: {experiment_id}\")\n",
    "    else:\n",
    "        print(f\"Using existing experiment: {experiment.name}\")\n",
    "    mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "except Exception as e:\n",
    "    print(f\"Error with experiment setup: {e}\")\n",
    "    # Fallback to default experiment in workspace\n",
    "    mlflow.set_experiment(f\"/Users/{dbutils.notebook.entry_point.getDbutils().notebook().getContext().userName().get()}/default\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "200b4f72-de11-4b6b-8980-cedcbcd80ace",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fedb30a4-2f2d-4020-95bf-1d814c25d370",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def checkpoint_dataset(dataset, file_path):\n",
    "    # Create base folder\n",
    "    section = \"2\"\n",
    "    number = \"2\"\n",
    "    base_folder = f\"dbfs:/student-groups/Group_{section}_{number}\"\n",
    "    dbutils.fs.mkdirs(base_folder)\n",
    "    # Create subfolders if file_path contains directories\n",
    "    full_path = f\"{base_folder}/{file_path}.parquet\"\n",
    "    subfolder = \"/\".join(full_path.split(\"/\")[:-1])\n",
    "    dbutils.fs.mkdirs(subfolder)\n",
    "    # Save dataset as a parquet file\n",
    "    dataset.write.mode(\"overwrite\").parquet(full_path)\n",
    "    print(f\"Checkpointed {file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "60cc9a59-141d-4ac4-8867-abe88459507b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Datasets (Custom Join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d11348a9-4157-45eb-8e8e-a748e292c6d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th><th>modificationTime</th></tr></thead><tbody><tr><td>dbfs:/student-groups/Group_2_2/3_month_custom_joined/fe_graph_and_holiday/cv_splits/</td><td>cv_splits/</td><td>0</td><td>1765328209514</td></tr><tr><td>dbfs:/student-groups/Group_2_2/3_month_custom_joined/fe_graph_and_holiday/stacked_input_optimized/</td><td>stacked_input_optimized/</td><td>0</td><td>1765328209514</td></tr><tr><td>dbfs:/student-groups/Group_2_2/3_month_custom_joined/fe_graph_and_holiday/training_splits/</td><td>training_splits/</td><td>0</td><td>1765328209514</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "dbfs:/student-groups/Group_2_2/3_month_custom_joined/fe_graph_and_holiday/cv_splits/",
         "cv_splits/",
         0,
         1765328209514
        ],
        [
         "dbfs:/student-groups/Group_2_2/3_month_custom_joined/fe_graph_and_holiday/stacked_input_optimized/",
         "stacked_input_optimized/",
         0,
         1765328209514
        ],
        [
         "dbfs:/student-groups/Group_2_2/3_month_custom_joined/fe_graph_and_holiday/training_splits/",
         "training_splits/",
         0,
         1765328209514
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "path",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "size",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "modificationTime",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(dbutils.fs.ls(\"dbfs:/student-groups/Group_2_2/3_month_custom_joined/fe_graph_and_holiday\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69ccf366-f086-406a-8afa-182831eb7c87",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_df = spark.read.parquet(\n",
    "    \"dbfs:/student-groups/Group_2_2/1_year_custom_joined/fe_graph_and_holiday/training_splits/train.parquet/\"\n",
    ")\n",
    "val_df = spark.read.parquet(\n",
    "    \"dbfs:/student-groups/Group_2_2/1_year_custom_joined/fe_graph_and_holiday/training_splits/validation.parquet/\"\n",
    ")\n",
    "test_df = spark.read.parquet(\n",
    "    \"dbfs:/student-groups/Group_2_2/1_year_custom_joined/fe_graph_and_holiday/training_splits/test.parquet/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9b269132-a156-47bc-af76-95ba33f9a892",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a01576fb-da14-4197-9176-d28ee41b7c5c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Time-derived cyclic features\n",
    "# -----------------------------\n",
    "def add_time_features(df):\n",
    "    # Hour of departure as float\n",
    "    df = df.withColumn(\"dep_hour\", F.col(\"CRS_DEP_MINUTES\") / 60.0)\n",
    "\n",
    "    # Day of year\n",
    "    df = df.withColumn(\"day_of_year\", F.dayofyear(\"utc_timestamp\").cast(\"double\"))\n",
    "\n",
    "    # Cyclic transforms (double precision)\n",
    "    df = df.withColumn(\"dep_hour_sin\", F.sin(2 * F.lit(np.pi) * F.col(\"dep_hour\") / 24))\n",
    "    df = df.withColumn(\"dep_hour_cos\", F.cos(2 * F.lit(np.pi) * F.col(\"dep_hour\") / 24))\n",
    "\n",
    "    df = df.withColumn(\"dow_sin\", F.sin(2 * F.lit(np.pi) * F.col(\"DAY_OF_WEEK\") / 7))\n",
    "    df = df.withColumn(\"dow_cos\", F.cos(2 * F.lit(np.pi) * F.col(\"DAY_OF_WEEK\") / 7))\n",
    "\n",
    "    df = df.withColumn(\"doy_sin\", F.sin(2 * F.lit(np.pi) * F.col(\"day_of_year\") / 365))\n",
    "    df = df.withColumn(\"doy_cos\", F.cos(2 * F.lit(np.pi) * F.col(\"day_of_year\") / 365))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23237986-6977-4cd5-9dce-f4146dd24b82",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Weather delta features (3-hour changes)\n",
    "# -----------------------------\n",
    "def add_weather_deltas(df):\n",
    "    w = Window.partitionBy(\"ORIGIN_AIRPORT_SEQ_ID\").orderBy(\"utc_timestamp\")\n",
    "    \n",
    "    for col in [\n",
    "        \"HourlyVisibility\", \"HourlyStationPressure\",\n",
    "        \"HourlyDryBulbTemperature\", \"HourlyWindSpeed\",\n",
    "        \"HourlyPrecipitation\"\n",
    "    ]:\n",
    "        lag_col = F.lag(col, 3).over(w)\n",
    "        delta_col = F.col(col) - lag_col\n",
    "        # Use lag value if missing instead of 0 to avoid small bias\n",
    "        df = df.withColumn(\n",
    "            f\"{col}_3h_change\",\n",
    "            F.when(lag_col.isNull(), F.lit(None)).otherwise(delta_col)\n",
    "        )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "888ab275-70cc-4254-bdf7-d8b13159ce4d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Origin congestion features\n",
    "# -----------------------------\n",
    "def add_congestion_features(df):\n",
    "    # Rolling window: 1 hour before current event\n",
    "    df = df.withColumn(\"utc_ts_sec\", F.col(\"utc_timestamp\").cast(\"long\"))\n",
    "\n",
    "    w = Window.partitionBy(\"ORIGIN_AIRPORT_SEQ_ID\").orderBy(\"utc_ts_sec\").rangeBetween(-3600, 0)\n",
    "\n",
    "    df = df.withColumn(\n",
    "        \"ground_flights_last_hour\",\n",
    "        F.count(\"utc_ts_sec\").over(w) - 1\n",
    "    )\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47f438f3-d136-4930-a04d-903c393d5d62",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Destination congestion features\n",
    "# -----------------------------\n",
    "def add_dest_congestion_features(df):\n",
    "    # Convert timestamp to seconds\n",
    "    df = df.withColumn(\"utc_ts_sec\", F.col(\"utc_timestamp\").cast(\"long\"))\n",
    "    \n",
    "    # Rolling window: 1 hour (3600 seconds) before current row\n",
    "    w = Window.partitionBy(\"DEST_AIRPORT_SEQ_ID\") \\\n",
    "              .orderBy(\"utc_ts_sec\") \\\n",
    "              .rangeBetween(-3600, 0)\n",
    "    \n",
    "    df = df.withColumn(\n",
    "        \"arrivals_last_hour\",\n",
    "        F.count(\"utc_ts_sec\").over(w) - 1  # exclude current row\n",
    "    )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b82b8b0a-7033-4560-b919-b5db69c8d055",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "### Apply additional feature engineering\n",
    "train_df_fe = (train_df\n",
    "               .transform(add_time_features)\n",
    "               .transform(add_weather_deltas)\n",
    "               .transform(add_congestion_features)\n",
    "               .transform(add_dest_congestion_features))\n",
    "\n",
    "val_df_fe   = (val_df\n",
    "               .transform(add_time_features)\n",
    "               .transform(add_weather_deltas)\n",
    "               .transform(add_congestion_features)\n",
    "               .transform(add_dest_congestion_features))\n",
    "\n",
    "test_df_fe  = (test_df\n",
    "               .transform(add_time_features)\n",
    "               .transform(add_weather_deltas)\n",
    "               .transform(add_congestion_features)\n",
    "               .transform(add_dest_congestion_features))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5cbefcee-b735-48ca-95b2-6e163e535fce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpointed 1_year_custom_joined/fe_graph_and_holiday_nnfeat/training_splits/train\nCheckpointed 1_year_custom_joined/fe_graph_and_holiday_nnfeat/training_splits/val\nCheckpointed 1_year_custom_joined/fe_graph_and_holiday_nnfeat/training_splits/test\n"
     ]
    }
   ],
   "source": [
    "## Checkpoint updated data\n",
    "\n",
    "checkpoint_dataset(train_df_fe, \"1_year_custom_joined/fe_graph_and_holiday_nnfeat/training_splits/train\")\n",
    "checkpoint_dataset(val_df_fe, \"1_year_custom_joined/fe_graph_and_holiday_nnfeat/training_splits/val\")\n",
    "checkpoint_dataset(test_df_fe, \"1_year_custom_joined/fe_graph_and_holiday_nnfeat/training_splits/test\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e4f3792d-59b4-4cd5-a22b-061e8f4158ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Start here when running experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba7f898b-923b-4e98-b3a4-6c5b7fd232e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Test if it can be read\n",
    "train_df_fe = spark.read.parquet(\n",
    "    \"dbfs:/student-groups/Group_2_2/1_year_custom_joined/fe_graph_and_holiday_nnfeat/training_splits/train.parquet/\"\n",
    ")\n",
    "val_df_fe = spark.read.parquet(\n",
    "    \"dbfs:/student-groups/Group_2_2/1_year_custom_joined/fe_graph_and_holiday_nnfeat/training_splits/val.parquet/\"\n",
    ")\n",
    "test_df = spark.read.parquet(\n",
    "    \"dbfs:/student-groups/Group_2_2/1_year_custom_joined/fe_graph_and_holiday_nnfeat/training_splits/test.parquet/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8979212d-e8dc-4fda-817c-46260ec1f81f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Vectorization for Torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b37de593-0380-4d7f-8bd5-ec6ef6f1db48",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "### Column organization / feature selection\n",
    "\n",
    "# -----------------------------\n",
    "# Categorical features (for embeddings)\n",
    "# -----------------------------\n",
    "categorical_cols = [\n",
    "    \"OP_UNIQUE_CARRIER\",       # Airline carrier code\n",
    "    \"ORIGIN_AIRPORT_SEQ_ID\",   # Origin airport\n",
    "    \"DEST_AIRPORT_SEQ_ID\",     # Destination airport\n",
    "    \"route\",                   # Route string\n",
    "    \"AIRPORT_HUB_CLASS\",       # Hub classification\n",
    "    \"AIRLINE_CATEGORY\",        # Airline type\n",
    "]\n",
    "\n",
    "# -----------------------------\n",
    "# Numerical features (to normalize)\n",
    "# -----------------------------\n",
    "numerical_cols = [\n",
    "    ### baseline features\n",
    "    \"DISTANCE\",\n",
    "    \"CRS_ELAPSED_TIME\",\n",
    "    \"prev_flight_delay_in_minutes\",   # Phase 2 Feature Eng\n",
    "    \"origin_delays_4h\",               # Phase 2 Feature Eng\n",
    "    \"delay_origin_7d\",                # Phase 2 Feature Eng\n",
    "    \"delay_origin_carrier_7d\",        # Phase 2 Feature Eng\n",
    "    \"delay_route_7d\",                 # Phase 2 Feature Eng\n",
    "    \"flight_count_24h\",               # Phase 2 Feature Eng\n",
    "    \"AVG_TAXI_OUT_ORIGIN\",            # Phase 2 Feature Eng\n",
    "    \"AVG_ARR_DELAY_ORIGIN\",           # Phase 2 Feature Eng\n",
    "\n",
    "    ### graph features\n",
    "    # \"page_rank\",\n",
    "    \"in_degree\",\n",
    "    \"out_degree\",\n",
    "    \"weighted_in_degree\",\n",
    "    \"weighted_out_degree\",\n",
    "    \"betweenness\",\n",
    "    \"closeness\",\n",
    "    \"N_RUNWAYS\",\n",
    "\n",
    "    ### weather (raw only)\n",
    "    \"HourlyVisibility\",\n",
    "    \"HourlyStationPressure\",\n",
    "    \"HourlyWindSpeed\",\n",
    "    \"HourlyDryBulbTemperature\",\n",
    "    \"HourlyDewPointTemperature\",\n",
    "    \"HourlyRelativeHumidity\",\n",
    "    \"HourlyAltimeterSetting\",\n",
    "    \"HourlyVisibility\",\n",
    "    \"HourlyStationPressure\",\n",
    "    \"HourlyWetBulbTemperature\",\n",
    "    \"HourlyPrecipitation\",\n",
    "    \"HourlyCloudCoverage\",\n",
    "    \"HourlyCloudElevation\",\n",
    "\n",
    "    ### congestion\n",
    "    \"ground_flights_last_hour\",     # New feature engineered\n",
    "    \"arrivals_last_hour\",           # New feature engineered\n",
    "\n",
    "    ### time features               # New feature engineered start\n",
    "    # \"dep_hour_sin\",\n",
    "    # \"dep_hour_cos\",\n",
    "    \"dow_sin\",\n",
    "    \"dow_cos\",\n",
    "    \"doy_sin\",\n",
    "    \"doy_cos\",                      # New feature engineered end\n",
    "]\n",
    "\n",
    "# -----------------------------\n",
    "# Time feature (optional numerical input)\n",
    "# -----------------------------\n",
    "time_col = \"CRS_DEP_MINUTES\"  # Exact departure minute, optional as input\n",
    "\n",
    "# -----------------------------\n",
    "# Target\n",
    "# -----------------------------\n",
    "target_col = \"DEP_DELAY_NEW\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d11bc6d-a016-4a03-8654-285e859b2f50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Convert PySpark, Pandas to Torch-ready dicts\n",
    "train_pd = train_df_fe.select(categorical_cols + numerical_cols + [time_col, target_col]).toPandas()\n",
    "val_pd   = val_df_fe.select(categorical_cols + numerical_cols + [time_col, target_col]).toPandas()\n",
    "test_pd  = test_df_fe.select(categorical_cols + numerical_cols + [time_col, target_col]).toPandas()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "43519b71-56cc-47f7-817b-0ec1cefece77",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Encode categoricals; build encoder dictionary for train data only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6aaf9a0-ea34-4070-a2ce-9f3761241b1b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Build category maps from training data\n",
    "# -----------------------------\n",
    "def build_category_maps(train_df, categorical_cols):\n",
    "    \"\"\"\n",
    "    Creates a mapping of unique category values to integer IDs for each categorical column.\n",
    "    UNK (unknown) is mapped to 0.\n",
    "    \n",
    "    Returns:\n",
    "        dict: {column_name: {category_value: id, ...}, ...}\n",
    "    \"\"\"\n",
    "    cat_maps = {}\n",
    "\n",
    "    for c in categorical_cols:\n",
    "        # Get unique categories in training set as strings\n",
    "        uniques = train_df[c].astype(str).unique().tolist()\n",
    "\n",
    "        # Map categories to integers, reserving 0 for UNK\n",
    "        cat_maps[c] = {\"UNK\": 0, **{v: i + 1 for i, v in enumerate(sorted(uniques))}}\n",
    "\n",
    "    return cat_maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9b517d9-e564-4c73-9766-e770beb92100",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Apply category maps to any dataset\n",
    "# -----------------------------\n",
    "def apply_category_maps(df, cat_maps, categorical_cols):\n",
    "    \"\"\"\n",
    "    Encodes categorical columns using pre-built category maps.\n",
    "    Unseen values are mapped to 0 (UNK).\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: copy of input df with encoded categorical columns\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    for c in categorical_cols:\n",
    "        mapping = cat_maps[c]\n",
    "        # Convert to string, map to integer IDs, unseen -> 0\n",
    "        df[c] = df[c].astype(str).apply(lambda x: mapping.get(x, 0))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd170515-06f2-4a2c-b78e-8c108558b9a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Apply to train/val/test\n",
    "# -----------------------------\n",
    "# Build mapping dicts from training data only\n",
    "cat_maps = build_category_maps(train_pd, categorical_cols)\n",
    "\n",
    "# Apply safely to all datasets\n",
    "train_pd = apply_category_maps(train_pd, cat_maps, categorical_cols)\n",
    "val_pd   = apply_category_maps(val_pd, cat_maps, categorical_cols)\n",
    "test_pd  = apply_category_maps(test_pd, cat_maps, categorical_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0680a083-0e97-4fb4-9743-d72641610b40",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OP_UNIQUE_CARRIER: 18 categories -> embedding dim 2\nORIGIN_AIRPORT_SEQ_ID: 373 categories -> embedding dim 5\nDEST_AIRPORT_SEQ_ID: 388 categories -> embedding dim 5\nroute: 6252 categories -> embedding dim 13\nAIRPORT_HUB_CLASS: 7 categories -> embedding dim 1\nAIRLINE_CATEGORY: 4 categories -> embedding dim 1\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# Compute embedding sizes\n",
    "# -----------------------------\n",
    "# Number of unique categories for each column (for embedding layers)\n",
    "cat_dims = [len(cat_maps[c]) for c in categorical_cols]\n",
    "\n",
    "# Embedding dimensions (rule of thumb: min(64, int(cardinality ** 0.3)))\n",
    "emb_dims = [min(64, int(n**0.3)) for n in cat_dims]\n",
    "\n",
    "# Sanity check\n",
    "for c, dim, emb in zip(categorical_cols, cat_dims, emb_dims):\n",
    "    print(f\"{c}: {dim} categories -> embedding dim {emb}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6eea0405-cb07-4b90-9f7a-2b8dda4cd0a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Normalize numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "550a1ca0-757a-4d02-ad82-3e8630330b67",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "train_pd[numerical_cols] = scaler.fit_transform(train_pd[numerical_cols])\n",
    "val_pd[numerical_cols]   = scaler.transform(val_pd[numerical_cols])\n",
    "test_pd[numerical_cols]  = scaler.transform(test_pd[numerical_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e2ba31f3-4799-4ede-9bc5-07cc522dcc55",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Load Torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33b9b3d0-7033-4223-beea-9b86a39f3b53",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e5f3921-7aa5-4743-94e4-bd224dac29c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "\n",
    "class FlightDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        # Categorical features (long for embeddings)\n",
    "        self.cat = torch.tensor(df[categorical_cols].values, dtype=torch.long)\n",
    "        \n",
    "        # Numerical features (float)\n",
    "        self.num = torch.tensor(df[numerical_cols].values, dtype=torch.float32)\n",
    "        \n",
    "        # Time feature (optional, float)\n",
    "        self.time = torch.tensor(df[time_col].values, dtype=torch.float32).unsqueeze(1)\n",
    "        \n",
    "        # Target\n",
    "        self.y = torch.tensor(df[target_col].values, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.cat[idx], self.num[idx], self.time[idx], self.y[idx]\n",
    "\n",
    "# Create datasets\n",
    "train_ds = FlightDataset(train_pd)\n",
    "val_ds   = FlightDataset(val_pd)\n",
    "test_ds  = FlightDataset(test_pd)\n",
    "\n",
    "# Create data loaders\n",
    "train_dl = DataLoader(train_ds, batch_size=2048, shuffle=True, pin_memory=True)\n",
    "val_dl   = DataLoader(val_ds, batch_size=2048, pin_memory=True)\n",
    "test_dl  = DataLoader(test_ds, batch_size=2048, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4554b9cd-652e-4cb9-95ee-da064b2e30ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Model Definition (ResFiLM-MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc4f85fa-958c-40af-8c58-08ff25274141",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Residual Block\n",
    "# -----------------------------\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.ln = nn.LayerNorm(dim)\n",
    "        self.fc1 = nn.Linear(dim, dim)\n",
    "        self.fc2 = nn.Linear(dim, dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = F.gelu(self.fc1(self.ln(x)))\n",
    "        h = self.dropout(h)\n",
    "        h = self.fc2(h)\n",
    "        return x + h\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Time2Vec\n",
    "# -----------------------------\n",
    "class Time2Vec(nn.Module):\n",
    "    def __init__(self, k):\n",
    "        super().__init__()\n",
    "        self.wb = nn.Linear(1, 1)\n",
    "        self.ws = nn.Linear(1, k)\n",
    "\n",
    "    def forward(self, t):\n",
    "        b = self.wb(t)\n",
    "        s = torch.sin(self.ws(t))\n",
    "        return torch.cat([b, s], dim=-1)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# ResFiLM MLP (F2-optimized)\n",
    "# -----------------------------\n",
    "class ResFiLMMLPft(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        cat_dims,\n",
    "        emb_dims,\n",
    "        num_numerical,\n",
    "        time_dim=8,\n",
    "        emb_dropout=0.05,\n",
    "        num_dropout=0.1,\n",
    "        film_dropout=0.1,\n",
    "        final_dropout=0.2\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # --- Embedding tower ---\n",
    "        self.embeddings = nn.ModuleList([\n",
    "            nn.Embedding(cat_dim, emb_dim)\n",
    "            for cat_dim, emb_dim in zip(cat_dims, emb_dims)\n",
    "        ])\n",
    "        self.emb_total = sum(emb_dims)\n",
    "        self.emb_dropout = nn.Dropout(emb_dropout)\n",
    "\n",
    "        # --- Numeric tower ---\n",
    "        self.fc_num = nn.Linear(num_numerical, 256)\n",
    "        self.res_blocks = nn.ModuleList([\n",
    "            ResBlock(256, dropout=num_dropout)\n",
    "            for _ in range(4)\n",
    "        ])\n",
    "\n",
    "        # --- FiLM for embeddings ---\n",
    "        self.film = nn.Linear(256, 2 * self.emb_total)\n",
    "        self.film_dropout = nn.Dropout(film_dropout)\n",
    "\n",
    "        # --- Time2Vec ---\n",
    "        self.t2v = Time2Vec(time_dim)\n",
    "\n",
    "        # --- Optional: classification-specific FiLM ---\n",
    "        self.clf_film = nn.Linear(256, 2 * self.emb_total)\n",
    "        self.clf_film_dropout = nn.Dropout(film_dropout)\n",
    "\n",
    "        # --- Final fusion dimension ---\n",
    "        fused_dim = 256 + self.emb_total + (time_dim + 1) + 1\n",
    "\n",
    "        # -------------------------------\n",
    "        # Multi-task heads\n",
    "        # -------------------------------\n",
    "\n",
    "        # Regression head (delay minutes)\n",
    "        self.reg_head = nn.Sequential(\n",
    "            nn.Linear(fused_dim, 256),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(final_dropout),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(final_dropout),\n",
    "            nn.Linear(128, 1)  # raw regression output\n",
    "        )\n",
    "\n",
    "        # Classification head (delay yes/no) â€“ deeper\n",
    "        self.clf_head = nn.Sequential(\n",
    "            nn.Linear(fused_dim, 256),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(final_dropout),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(final_dropout),\n",
    "            nn.Linear(128, 1)  # raw logit, no Sigmoid\n",
    "        )\n",
    "\n",
    "    def forward(self, x_cat, x_num, x_time):\n",
    "\n",
    "        # --- Embeddings ---\n",
    "        emb = [emb_layer(x_cat[:, i]) for i, emb_layer in enumerate(self.embeddings)]\n",
    "        emb = torch.cat(emb, dim=-1)\n",
    "        emb = self.emb_dropout(emb)\n",
    "\n",
    "        # --- Numeric tower ---\n",
    "        h = F.gelu(self.fc_num(x_num))\n",
    "        for block in self.res_blocks:\n",
    "            h = block(h)\n",
    "\n",
    "        # --- FiLM modulation ---\n",
    "        gamma, beta = torch.chunk(self.film(h), 2, dim=-1)\n",
    "        gamma = self.film_dropout(gamma)\n",
    "        beta = self.film_dropout(beta)\n",
    "        emb_mod = gamma * emb + beta\n",
    "\n",
    "        # --- Time2Vec ---\n",
    "        t_feat = self.t2v(x_time)\n",
    "\n",
    "        # --- Fuse towers ---\n",
    "        z = torch.cat([emb_mod, h, t_feat, x_time], dim=-1)\n",
    "\n",
    "        # --- Output tasks ---\n",
    "        reg_out = self.reg_head(z)       # regression output\n",
    "        clf_out = self.clf_head(z)       # classification logit (no sigmoid)\n",
    "\n",
    "        return reg_out, clf_out\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Example F2-focused loss function\n",
    "# -----------------------------\n",
    "def f2_loss(logits, targets, pos_weight=4.0):\n",
    "    # logits: raw outputs from clf_head\n",
    "    # targets: binary labels (0/1)\n",
    "    weight = torch.tensor([pos_weight], device=logits.device)\n",
    "    return nn.BCEWithLogitsLoss(pos_weight=weight)(logits, targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c0e3a2d0-9f40-4219-8bac-27a7a22c2869",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Prepare fold logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90074526-84f2-4b5c-a092-9340cb831ddc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import fbeta_score, roc_auc_score, mean_squared_error, mean_absolute_error\n",
    "import copy\n",
    "import numpy as np\n",
    "import torch\n",
    "import mlflow\n",
    "\n",
    "def get_device():\n",
    "    return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def train_epoch(model, dataloader, optimizer, criterion_reg, criterion_clf, device, alpha=0.5):\n",
    "    \"\"\"\n",
    "    Train one epoch. \n",
    "    alpha: Weighting between Regression (MAE) and Classification (F2/BCE).\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for cat, num, time, y in dataloader:\n",
    "        cat, num, time, y = cat.to(device), num.to(device), time.to(device), y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        reg_out, clf_out = model(cat, num, time)\n",
    "        \n",
    "        # Create binary target: (Delay > 0)\n",
    "        y_bin = (y > 0).float()\n",
    "        \n",
    "        loss_reg = criterion_reg(reg_out, y)\n",
    "        loss_clf = criterion_clf(clf_out, y_bin)\n",
    "        \n",
    "        # Combine losses\n",
    "        loss = (alpha * loss_reg) + ((1 - alpha) * loss_clf)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def evaluate(model, dataloader, device):\n",
    "    \"\"\"\n",
    "    Evaluates on MAE, RMSE, AUC, and F2.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    preds_reg, preds_clf = [], []\n",
    "    targets_reg, targets_clf = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for cat, num, time, y in dataloader:\n",
    "            cat, num, time, y = cat.to(device), num.to(device), time.to(device), y.to(device)\n",
    "            reg_out, clf_out = model(cat, num, time)\n",
    "            \n",
    "            preds_reg.append(reg_out.cpu())\n",
    "            preds_clf.append(torch.sigmoid(clf_out).cpu()) \n",
    "            targets_reg.append(y.cpu())\n",
    "            targets_clf.append((y > 0).float().cpu())\n",
    "            \n",
    "    # Concatenate\n",
    "    y_pred_reg = torch.cat(preds_reg).numpy()\n",
    "    y_pred_clf = torch.cat(preds_clf).numpy()\n",
    "    y_true_reg = torch.cat(targets_reg).numpy()\n",
    "    y_true_clf = torch.cat(targets_clf).numpy()\n",
    "    \n",
    "    # --- Metrics ---\n",
    "    # 1. Regression Metrics\n",
    "    mae = mean_absolute_error(y_true_reg, y_pred_reg)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true_reg, y_pred_reg))\n",
    "    \n",
    "    # 2. Classification Metrics\n",
    "    try:\n",
    "        auc = roc_auc_score(y_true_clf, y_pred_clf)\n",
    "    except:\n",
    "        auc = 0.5 # Handle edge case if only 1 class in batch\n",
    "        \n",
    "    # F2 Score (Threshold 0.5, strictly penalizes False Negatives)\n",
    "    y_pred_bin = (y_pred_clf > 0.5).astype(int)\n",
    "    f2 = fbeta_score(y_true_clf, y_pred_bin, beta=2)\n",
    "    \n",
    "    return {\"mae\": mae, \"rmse\": rmse, \"auc\": auc, \"f2_score\": f2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6240eaba-64f5-4ea6-bad2-3c62d3acfc8f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def prepare_fold_data(fold_df, categorical_cols, numerical_cols, time_col, target_col):\n",
    "    \"\"\"\n",
    "    Takes the raw Spark DF for a specific fold, splits into train/val,\n",
    "    applies FE, fits scalers/encoders on Train, transforms Val.\n",
    "    \"\"\"\n",
    "    # 1. Split by split_type\n",
    "    # CHANGE: Explicitly filter for 'validation' to exclude 'gap' data\n",
    "    train_df_spark = fold_df.filter(F.col(\"split_type\") == \"train\")\n",
    "    val_df_spark = fold_df.filter(F.col(\"split_type\") == \"validation\")\n",
    "\n",
    "    # 2. Apply Feature Engineering (Redundant if already in parquet, but ensures consistency)\n",
    "    train_fe = (train_df_spark\n",
    "                .transform(add_time_features)\n",
    "                .transform(add_weather_deltas)\n",
    "                .transform(add_congestion_features)\n",
    "                .transform(add_dest_congestion_features))\n",
    "    \n",
    "    val_fe = (val_df_spark\n",
    "              .transform(add_time_features)\n",
    "              .transform(add_weather_deltas)\n",
    "              .transform(add_congestion_features)\n",
    "              .transform(add_dest_congestion_features))\n",
    "\n",
    "    # 3. Convert to Pandas for Torch\n",
    "    train_pd = train_fe.select(categorical_cols + numerical_cols + [time_col, target_col]).toPandas()\n",
    "    val_pd = val_fe.select(categorical_cols + numerical_cols + [time_col, target_col]).toPandas()\n",
    "\n",
    "    # 4. Fit Encoders (Train Only)\n",
    "    cat_maps = build_category_maps(train_pd, categorical_cols)\n",
    "    train_pd = apply_category_maps(train_pd, cat_maps, categorical_cols)\n",
    "    val_pd = apply_category_maps(val_pd, cat_maps, categorical_cols)\n",
    "\n",
    "    # 5. Fit Scaler (Train Only)\n",
    "    scaler = StandardScaler()\n",
    "    train_pd[numerical_cols] = scaler.fit_transform(train_pd[numerical_cols])\n",
    "    val_pd[numerical_cols] = scaler.transform(val_pd[numerical_cols])\n",
    "\n",
    "    # 6. Calc Dimensions for Model\n",
    "    cat_dims = [len(cat_maps[c]) for c in categorical_cols]\n",
    "    emb_dims = [min(64, int(n**0.3)) for n in cat_dims]\n",
    "\n",
    "    return train_pd, val_pd, cat_dims, emb_dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ccd0a4b4-9bec-4d64-a24e-5ddd70168d1a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import pyspark.sql.functions as sf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a2e74416-e103-422f-9e96-2dd8632640e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as sf  # Safe alias for Spark\n",
    "import torch.nn.functional as F     # Safe alias for Torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark.sql import Window\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import fbeta_score, roc_auc_score, mean_squared_error, mean_absolute_error\n",
    "import copy\n",
    "import mlflow\n",
    "\n",
    "# ==========================================\n",
    "# 1. FIX: Deduplicate Numerical Columns\n",
    "# ==========================================\n",
    "numerical_cols = list(dict.fromkeys(numerical_cols))\n",
    "print(f\"Numerical columns count after deduplication: {len(numerical_cols)}\")\n",
    "\n",
    "# ==========================================\n",
    "# 2. Redefine FE Functions (Namespace Safe)\n",
    "# ==========================================\n",
    "def add_time_features(df):\n",
    "    df = df.withColumn(\"dep_hour\", sf.col(\"CRS_DEP_MINUTES\") / 60.0)\n",
    "    df = df.withColumn(\"day_of_year\", sf.dayofyear(\"utc_timestamp\").cast(\"double\"))\n",
    "    df = df.withColumn(\"dep_hour_sin\", sf.sin(2 * sf.lit(np.pi) * sf.col(\"dep_hour\") / 24))\n",
    "    df = df.withColumn(\"dep_hour_cos\", sf.cos(2 * sf.lit(np.pi) * sf.col(\"dep_hour\") / 24))\n",
    "    df = df.withColumn(\"dow_sin\", sf.sin(2 * sf.lit(np.pi) * sf.col(\"DAY_OF_WEEK\") / 7))\n",
    "    df = df.withColumn(\"dow_cos\", sf.cos(2 * sf.lit(np.pi) * sf.col(\"DAY_OF_WEEK\") / 7))\n",
    "    df = df.withColumn(\"doy_sin\", sf.sin(2 * sf.lit(np.pi) * sf.col(\"day_of_year\") / 365))\n",
    "    df = df.withColumn(\"doy_cos\", sf.cos(2 * sf.lit(np.pi) * sf.col(\"day_of_year\") / 365))\n",
    "    return df\n",
    "\n",
    "def add_weather_deltas(df):\n",
    "    w = Window.partitionBy(\"ORIGIN_AIRPORT_SEQ_ID\").orderBy(\"utc_timestamp\")\n",
    "    for col_name in [\"HourlyVisibility\", \"HourlyStationPressure\", \"HourlyDryBulbTemperature\", \"HourlyWindSpeed\", \"HourlyPrecipitation\"]:\n",
    "        lag_col = sf.lag(col_name, 3).over(w)\n",
    "        delta_col = sf.col(col_name) - lag_col\n",
    "        df = df.withColumn(f\"{col_name}_3h_change\", sf.when(lag_col.isNull(), sf.lit(None)).otherwise(delta_col))\n",
    "    return df\n",
    "\n",
    "def add_congestion_features(df):\n",
    "    df = df.withColumn(\"utc_ts_sec\", sf.col(\"utc_timestamp\").cast(\"long\"))\n",
    "    w = Window.partitionBy(\"ORIGIN_AIRPORT_SEQ_ID\").orderBy(\"utc_ts_sec\").rangeBetween(-3600, 0)\n",
    "    df = df.withColumn(\"ground_flights_last_hour\", sf.count(\"utc_ts_sec\").over(w) - 1)\n",
    "    return df\n",
    "\n",
    "def add_dest_congestion_features(df):\n",
    "    df = df.withColumn(\"utc_ts_sec\", sf.col(\"utc_timestamp\").cast(\"long\"))\n",
    "    w = Window.partitionBy(\"DEST_AIRPORT_SEQ_ID\").orderBy(\"utc_ts_sec\").rangeBetween(-3600, 0)\n",
    "    df = df.withColumn(\"arrivals_last_hour\", sf.count(\"utc_ts_sec\").over(w) - 1)\n",
    "    return df\n",
    "\n",
    "# ==========================================\n",
    "# 3. Fold Prep & Prediction Helper\n",
    "# ==========================================\n",
    "def prepare_fold_data(fold_df, categorical_cols, numerical_cols, time_col, target_col):\n",
    "    train_df_spark = fold_df.filter(sf.col(\"split_type\") == \"train\")\n",
    "    val_df_spark   = fold_df.filter(sf.col(\"split_type\") == \"validation\")\n",
    "\n",
    "    train_fe = (train_df_spark.transform(add_time_features).transform(add_weather_deltas)\n",
    "                .transform(add_congestion_features).transform(add_dest_congestion_features))\n",
    "    val_fe   = (val_df_spark.transform(add_time_features).transform(add_weather_deltas)\n",
    "                .transform(add_congestion_features).transform(add_dest_congestion_features))\n",
    "\n",
    "    train_pd = train_fe.select(categorical_cols + numerical_cols + [time_col, target_col]).toPandas()\n",
    "    val_pd   = val_fe.select(categorical_cols + numerical_cols + [time_col, target_col]).toPandas()\n",
    "\n",
    "    cat_maps = build_category_maps(train_pd, categorical_cols)\n",
    "    train_pd = apply_category_maps(train_pd, cat_maps, categorical_cols)\n",
    "    val_pd   = apply_category_maps(val_pd, cat_maps, categorical_cols)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    train_pd[numerical_cols] = scaler.fit_transform(train_pd[numerical_cols])\n",
    "    val_pd[numerical_cols]   = scaler.transform(val_pd[numerical_cols])\n",
    "\n",
    "    cat_dims = [len(cat_maps[c]) for c in categorical_cols]\n",
    "    emb_dims = [min(64, int(n**0.3)) for n in cat_dims]\n",
    "\n",
    "    return train_pd, val_pd, cat_dims, emb_dims\n",
    "\n",
    "def save_predictions_to_parquet(model, dataloader, device, save_path, fold_id):\n",
    "    \"\"\"\n",
    "    Generates predictions using the model and saves them to a Parquet file.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    preds_reg, preds_clf = [], []\n",
    "    targets_reg, targets_clf = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for cat, num, time, y in dataloader:\n",
    "            cat, num, time, y = cat.to(device), num.to(device), time.to(device), y.to(device)\n",
    "            reg_out, clf_out = model(cat, num, time)\n",
    "            \n",
    "            preds_reg.append(reg_out.cpu().numpy())\n",
    "            preds_clf.append(torch.sigmoid(clf_out).cpu().numpy())\n",
    "            targets_reg.append(y.cpu().numpy())\n",
    "            targets_clf.append((y >= 15.0).float().cpu().numpy()) # Consistent with >15m logic\n",
    "    \n",
    "    # Flatten arrays\n",
    "    flat_preds_reg = np.concatenate(preds_reg).flatten()\n",
    "    flat_preds_clf = np.concatenate(preds_clf).flatten()\n",
    "    flat_targets_reg = np.concatenate(targets_reg).flatten()\n",
    "    flat_targets_clf = np.concatenate(targets_clf).flatten()\n",
    "    \n",
    "    # Create Pandas DataFrame\n",
    "    pdf = pd.DataFrame({\n",
    "        \"fold_id\": fold_id,\n",
    "        \"target_delay_minutes\": flat_targets_reg,\n",
    "        \"pred_delay_minutes\": flat_preds_reg,\n",
    "        \"target_is_delayed\": flat_targets_clf,\n",
    "        \"pred_prob_delayed\": flat_preds_clf\n",
    "    })\n",
    "    \n",
    "    # Save via Spark\n",
    "    print(f\"  >> Saving {len(pdf)} predictions to {save_path}...\")\n",
    "    spark.createDataFrame(pdf).write.mode(\"overwrite\").parquet(save_path)\n",
    "\n",
    "# ==========================================\n",
    "# 4. Main Execution Loop\n",
    "# ==========================================\n",
    "def get_device(): return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def train_epoch(model, dataloader, optimizer, criterion_reg, criterion_clf, device, alpha=0.5):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for cat, num, time, y in dataloader:\n",
    "        cat, num, time, y = cat.to(device), num.to(device), time.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        reg_out, clf_out = model(cat, num, time)\n",
    "        y_bin = (y >= 15.0).float()\n",
    "        loss = (alpha * criterion_reg(reg_out, y)) + ((1 - alpha) * criterion_clf(clf_out, y_bin))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def evaluate(model, dataloader, device):\n",
    "    model.eval()\n",
    "    preds_reg, preds_clf, targets_reg, targets_clf = [], [], [], []\n",
    "    with torch.no_grad():\n",
    "        for cat, num, time, y in dataloader:\n",
    "            cat, num, time, y = cat.to(device), num.to(device), time.to(device), y.to(device)\n",
    "            reg_out, clf_out = model(cat, num, time)\n",
    "            preds_reg.append(reg_out.cpu())\n",
    "            preds_clf.append(torch.sigmoid(clf_out).cpu())\n",
    "            targets_reg.append(y.cpu())\n",
    "            targets_clf.append((y >= 15.0).float().cpu())\n",
    "            \n",
    "    y_pred_reg, y_pred_clf = torch.cat(preds_reg).numpy(), torch.cat(preds_clf).numpy()\n",
    "    y_true_reg, y_true_clf = torch.cat(targets_reg).numpy(), torch.cat(targets_clf).numpy()\n",
    "    \n",
    "    mae = mean_absolute_error(y_true_reg, y_pred_reg)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true_reg, y_pred_reg))\n",
    "    try: auc = roc_auc_score(y_true_clf, y_pred_clf)\n",
    "    except: auc = 0.5\n",
    "    f2 = fbeta_score(y_true_clf, (y_pred_clf > 0.5).astype(int), beta=2)\n",
    "    return {\"mae\": mae, \"rmse\": rmse, \"auc\": auc, \"f2_score\": f2}\n",
    "\n",
    "# --- Config ---\n",
    "CV_DATA_PATH = \"dbfs:/student-groups/Group_2_2/1_year_custom_joined/fe_graph_and_holiday_nnfeat/cv_splits\"\n",
    "PREDS_SAVE_PATH = \"dbfs:/student-groups/Group_2_2/1_year_custom_joined/nn_predictions\" # <--- NEW PATH\n",
    "NUM_EPOCHS, BATCH_SIZE, LR, PATIENCE = 20, 2048, 1e-3, 5\n",
    "DEVICE = get_device()\n",
    "\n",
    "cv_full_df = spark.read.parquet(CV_DATA_PATH)\n",
    "folds = sorted([row['fold_id'] for row in cv_full_df.select(\"fold_id\").distinct().collect()])\n",
    "print(f\"Starting Cross-Validation on Folds: {folds}\")\n",
    "\n",
    "mlflow.end_run()\n",
    "with mlflow.start_run(run_name=\"CV_Orchestrator_MAE_F2\") as parent_run:\n",
    "    cv_summary = {\"best_val_f2\": [], \"best_val_mae\": []}\n",
    "    \n",
    "    for fold in folds:\n",
    "        print(f\"\\n--- Starting Fold {fold} ---\")\n",
    "        with mlflow.start_run(run_name=f\"Fold_{fold}\", nested=True):\n",
    "            fold_data = cv_full_df.filter(sf.col(\"fold_id\") == fold)\n",
    "            \n",
    "            # --- PREPARE DATA ---\n",
    "            train_pd, val_pd, cat_dims, emb_dims = prepare_fold_data(\n",
    "                fold_data, categorical_cols, numerical_cols, time_col, target_col\n",
    "            )\n",
    "            \n",
    "            train_dl = DataLoader(FlightDataset(train_pd), batch_size=BATCH_SIZE, shuffle=True)\n",
    "            val_dl = DataLoader(FlightDataset(val_pd), batch_size=BATCH_SIZE)\n",
    "            \n",
    "            # --- MODEL ---\n",
    "            model = ResFiLMMLP(cat_dims, emb_dims, len(numerical_cols), time_dim=8).to(DEVICE)\n",
    "            optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n",
    "            crit_reg = nn.L1Loss()\n",
    "            crit_clf = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([4.0]).to(DEVICE))\n",
    "\n",
    "            # --- TRAINING ---\n",
    "            best_f2 = -1.0; best_mae = float('inf')\n",
    "            best_model_f2_state = None; best_model_mae_state = None\n",
    "            early_stop_counter = 0\n",
    "            \n",
    "            for epoch in range(NUM_EPOCHS):\n",
    "                train_loss = train_epoch(model, train_dl, optimizer, crit_reg, crit_clf, DEVICE)\n",
    "                train_metrics = evaluate(model, train_dl, DEVICE)\n",
    "                val_metrics = evaluate(model, val_dl, DEVICE)\n",
    "                \n",
    "                print(f\"Fold {fold} | Ep {epoch} | Train F2: {train_metrics['f2_score']:.3f} | Val F2: {val_metrics['f2_score']:.3f}\")\n",
    "                \n",
    "                mlflow.log_metrics({\n",
    "                    \"train_loss\": train_loss,\n",
    "                    \"val_f2\": val_metrics['f2_score'], \"val_mae\": val_metrics['mae'],\n",
    "                    \"val_rmse\": val_metrics['rmse']\n",
    "                }, step=epoch)\n",
    "                \n",
    "                # Track Best F2\n",
    "                if val_metrics['f2_score'] > best_f2:\n",
    "                    best_f2 = val_metrics['f2_score']\n",
    "                    best_model_f2_state = copy.deepcopy(model.state_dict())\n",
    "                    early_stop_counter = 0\n",
    "                    mlflow.log_metrics({\"best_val_f2_so_far\": best_f2}, step=epoch)\n",
    "                else:\n",
    "                    early_stop_counter += 1\n",
    "\n",
    "                # Track Best MAE\n",
    "                if val_metrics['mae'] < best_mae:\n",
    "                    best_mae = val_metrics['mae']\n",
    "                    best_model_mae_state = copy.deepcopy(model.state_dict())\n",
    "\n",
    "                if early_stop_counter >= PATIENCE:\n",
    "                    print(\"  Early stopping triggered.\")\n",
    "                    break\n",
    "\n",
    "            # --- SAVE PREDICTIONS & MODELS ---\n",
    "            \n",
    "            # 1. Load Best F2 Model & Save Predictions\n",
    "            if best_model_f2_state:\n",
    "                model.load_state_dict(best_model_f2_state)\n",
    "                \n",
    "                # Log Model\n",
    "                example_input = next(iter(val_dl))\n",
    "                ex_in = (example_input[0].to(DEVICE), example_input[1].to(DEVICE), example_input[2].to(DEVICE))\n",
    "                mlflow.pytorch.log_model(model, f\"model_fold_{fold}_best_f2\", input_example=ex_in)\n",
    "                \n",
    "                # Save Predictions (New Step)\n",
    "                save_path = f\"{PREDS_SAVE_PATH}/fold_{fold}\"\n",
    "                save_predictions_to_parquet(model, val_dl, DEVICE, save_path, fold)\n",
    "                \n",
    "            # 2. Log Best MAE Model (Backup)\n",
    "            if best_model_mae_state:\n",
    "                model.load_state_dict(best_model_mae_state)\n",
    "                mlflow.pytorch.log_model(model, f\"model_fold_{fold}_best_mae\")\n",
    "            \n",
    "            cv_summary[\"best_val_f2\"].append(best_f2)\n",
    "            cv_summary[\"best_val_mae\"].append(best_mae)\n",
    "\n",
    "    print(f\"\\n=== CV Complete ===\")\n",
    "    print(f\"Avg Best F2: {np.mean(cv_summary['best_val_f2']):.4f}\")\n",
    "    mlflow.log_metric(\"cv_avg_best_f2\", np.mean(cv_summary['best_val_f2']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "562709cb-38e3-45e0-8351-2cf97950073c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numerical columns count after deduplication: 34\nStarting Cross-Validation on Folds: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n\n--- Starting Fold 1 ---\nFold 1 | Ep 0 | Train F2: 0.712 | Val F2: 0.722 | Train MAE: 11.385 | Val MAE: 12.040\nFold 1 | Ep 1 | Train F2: 0.723 | Val F2: 0.699 | Train MAE: 11.307 | Val MAE: 12.026\nFold 1 | Ep 2 | Train F2: 0.734 | Val F2: 0.745 | Train MAE: 11.254 | Val MAE: 11.970\nFold 1 | Ep 3 | Train F2: 0.724 | Val F2: 0.711 | Train MAE: 11.227 | Val MAE: 11.955\nFold 1 | Ep 4 | Train F2: 0.728 | Val F2: 0.724 | Train MAE: 11.134 | Val MAE: 11.910\nFold 1 | Ep 5 | Train F2: 0.729 | Val F2: 0.734 | Train MAE: 11.085 | Val MAE: 11.937\nFold 1 | Ep 6 | Train F2: 0.730 | Val F2: 0.728 | Train MAE: 11.083 | Val MAE: 11.910\nFold 1 | Ep 7 | Train F2: 0.730 | Val F2: 0.742 | Train MAE: 11.046 | Val MAE: 11.928\n  Early stopping triggered.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[31m2025/12/10 02:48:20 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001B[0m\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27faf6f1498e469b9cb8a5379a5210cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  >> Fold 1 Best Val F2: 0.7450\n\n--- Starting Fold 2 ---\nFold 2 | Ep 0 | Train F2: 0.716 | Val F2: 0.726 | Train MAE: 11.799 | Val MAE: 13.205\nFold 2 | Ep 1 | Train F2: 0.731 | Val F2: 0.745 | Train MAE: 11.625 | Val MAE: 12.925\nFold 2 | Ep 2 | Train F2: 0.729 | Val F2: 0.743 | Train MAE: 11.684 | Val MAE: 12.980\nFold 2 | Ep 3 | Train F2: 0.729 | Val F2: 0.735 | Train MAE: 11.557 | Val MAE: 13.000\nFold 2 | Ep 4 | Train F2: 0.735 | Val F2: 0.747 | Train MAE: 11.462 | Val MAE: 12.906\nFold 2 | Ep 5 | Train F2: 0.730 | Val F2: 0.743 | Train MAE: 11.391 | Val MAE: 12.941\nFold 2 | Ep 6 | Train F2: 0.739 | Val F2: 0.753 | Train MAE: 11.355 | Val MAE: 12.897\nFold 2 | Ep 7 | Train F2: 0.735 | Val F2: 0.747 | Train MAE: 11.444 | Val MAE: 13.037\nFold 2 | Ep 8 | Train F2: 0.739 | Val F2: 0.751 | Train MAE: 11.285 | Val MAE: 12.942\nFold 2 | Ep 9 | Train F2: 0.741 | Val F2: 0.755 | Train MAE: 11.346 | Val MAE: 13.006\nFold 2 | Ep 10 | Train F2: 0.733 | Val F2: 0.745 | Train MAE: 11.244 | Val MAE: 12.938\nFold 2 | Ep 11 | Train F2: 0.737 | Val F2: 0.752 | Train MAE: 11.228 | Val MAE: 12.944\nFold 2 | Ep 12 | Train F2: 0.740 | Val F2: 0.752 | Train MAE: 11.207 | Val MAE: 12.947\nFold 2 | Ep 13 | Train F2: 0.740 | Val F2: 0.753 | Train MAE: 11.140 | Val MAE: 12.971\nFold 2 | Ep 14 | Train F2: 0.739 | Val F2: 0.751 | Train MAE: 11.100 | Val MAE: 12.991\n  Early stopping triggered.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[31m2025/12/10 03:26:08 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001B[0m\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "023f8f4903394d5d82d7a5651a7a6ddb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  >> Fold 2 Best Val F2: 0.7547\n\n--- Starting Fold 3 ---\nFold 3 | Ep 0 | Train F2: 0.737 | Val F2: 0.777 | Train MAE: 11.715 | Val MAE: 14.357\nFold 3 | Ep 1 | Train F2: 0.735 | Val F2: 0.777 | Train MAE: 11.685 | Val MAE: 14.309\nFold 3 | Ep 2 | Train F2: 0.731 | Val F2: 0.755 | Train MAE: 11.608 | Val MAE: 14.448\nFold 3 | Ep 3 | Train F2: 0.736 | Val F2: 0.752 | Train MAE: 11.519 | Val MAE: 14.477\nFold 3 | Ep 4 | Train F2: 0.738 | Val F2: 0.769 | Train MAE: 11.443 | Val MAE: 14.282\nFold 3 | Ep 5 | Train F2: 0.738 | Val F2: 0.755 | Train MAE: 11.405 | Val MAE: 14.182\n  Early stopping triggered.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[31m2025/12/10 03:42:03 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001B[0m\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0df8279aaefa49539055ba1d464ee584",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  >> Fold 3 Best Val F2: 0.7774\n\n--- Starting Fold 4 ---\nFold 4 | Ep 0 | Train F2: 0.718 | Val F2: 0.771 | Train MAE: 12.035 | Val MAE: 15.477\nFold 4 | Ep 1 | Train F2: 0.712 | Val F2: 0.769 | Train MAE: 11.832 | Val MAE: 15.049\nFold 4 | Ep 2 | Train F2: 0.712 | Val F2: 0.757 | Train MAE: 11.984 | Val MAE: 15.446\nFold 4 | Ep 3 | Train F2: 0.735 | Val F2: 0.777 | Train MAE: 12.124 | Val MAE: 15.572\nFold 4 | Ep 4 | Train F2: 0.735 | Val F2: 0.779 | Train MAE: 11.562 | Val MAE: 14.980\nFold 4 | Ep 5 | Train F2: 0.725 | Val F2: 0.775 | Train MAE: 11.616 | Val MAE: 14.973\nFold 4 | Ep 6 | Train F2: 0.740 | Val F2: 0.777 | Train MAE: 11.932 | Val MAE: 15.307\nFold 4 | Ep 7 | Train F2: 0.737 | Val F2: 0.775 | Train MAE: 11.701 | Val MAE: 15.158\nFold 4 | Ep 8 | Train F2: 0.740 | Val F2: 0.778 | Train MAE: 11.423 | Val MAE: 14.905\nFold 4 | Ep 9 | Train F2: 0.737 | Val F2: 0.779 | Train MAE: 11.458 | Val MAE: 14.947\n  Early stopping triggered.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[31m2025/12/10 04:07:31 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001B[0m\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a058bb3161504619885460f99b4d4fc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  >> Fold 4 Best Val F2: 0.7791\n\n--- Starting Fold 5 ---\nFold 5 | Ep 0 | Train F2: 0.740 | Val F2: 0.756 | Train MAE: 12.019 | Val MAE: 14.078\nFold 5 | Ep 1 | Train F2: 0.735 | Val F2: 0.757 | Train MAE: 11.947 | Val MAE: 14.134\nFold 5 | Ep 2 | Train F2: 0.728 | Val F2: 0.753 | Train MAE: 11.910 | Val MAE: 14.053\nFold 5 | Ep 3 | Train F2: 0.740 | Val F2: 0.758 | Train MAE: 11.856 | Val MAE: 13.997\nFold 5 | Ep 4 | Train F2: 0.745 | Val F2: 0.758 | Train MAE: 11.789 | Val MAE: 13.971\nFold 5 | Ep 5 | Train F2: 0.743 | Val F2: 0.758 | Train MAE: 11.729 | Val MAE: 13.967\nFold 5 | Ep 6 | Train F2: 0.742 | Val F2: 0.758 | Train MAE: 11.745 | Val MAE: 14.103\nFold 5 | Ep 7 | Train F2: 0.739 | Val F2: 0.754 | Train MAE: 11.662 | Val MAE: 13.889\nFold 5 | Ep 8 | Train F2: 0.738 | Val F2: 0.758 | Train MAE: 11.654 | Val MAE: 13.892\nFold 5 | Ep 9 | Train F2: 0.748 | Val F2: 0.758 | Train MAE: 11.604 | Val MAE: 13.942\nFold 5 | Ep 10 | Train F2: 0.741 | Val F2: 0.757 | Train MAE: 11.607 | Val MAE: 13.885\nFold 5 | Ep 11 | Train F2: 0.747 | Val F2: 0.759 | Train MAE: 11.545 | Val MAE: 14.068\nFold 5 | Ep 12 | Train F2: 0.748 | Val F2: 0.758 | Train MAE: 11.488 | Val MAE: 13.885\nFold 5 | Ep 13 | Train F2: 0.750 | Val F2: 0.758 | Train MAE: 11.483 | Val MAE: 13.840\nFold 5 | Ep 14 | Train F2: 0.740 | Val F2: 0.757 | Train MAE: 11.494 | Val MAE: 13.911\nFold 5 | Ep 15 | Train F2: 0.751 | Val F2: 0.758 | Train MAE: 11.411 | Val MAE: 14.069\nFold 5 | Ep 16 | Train F2: 0.743 | Val F2: 0.757 | Train MAE: 11.431 | Val MAE: 13.974\n  Early stopping triggered.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[31m2025/12/10 04:48:49 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001B[0m\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86ebf65b60d0414db60134a8972ef437",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  >> Fold 5 Best Val F2: 0.7586\n\n--- Starting Fold 6 ---\nFold 6 | Ep 0 | Train F2: 0.732 | Val F2: 0.733 | Train MAE: 12.137 | Val MAE: 13.982\nFold 6 | Ep 1 | Train F2: 0.735 | Val F2: 0.756 | Train MAE: 12.083 | Val MAE: 13.915\nFold 6 | Ep 2 | Train F2: 0.729 | Val F2: 0.745 | Train MAE: 12.117 | Val MAE: 14.104\nFold 6 | Ep 3 | Train F2: 0.742 | Val F2: 0.755 | Train MAE: 11.984 | Val MAE: 13.886\nFold 6 | Ep 4 | Train F2: 0.746 | Val F2: 0.758 | Train MAE: 11.903 | Val MAE: 13.992\nFold 6 | Ep 5 | Train F2: 0.740 | Val F2: 0.749 | Train MAE: 11.885 | Val MAE: 14.067\nFold 6 | Ep 6 | Train F2: 0.747 | Val F2: 0.759 | Train MAE: 11.852 | Val MAE: 13.967\nFold 6 | Ep 7 | Train F2: 0.748 | Val F2: 0.760 | Train MAE: 11.783 | Val MAE: 13.927\nFold 6 | Ep 8 | Train F2: 0.747 | Val F2: 0.758 | Train MAE: 11.827 | Val MAE: 14.057\nFold 6 | Ep 9 | Train F2: 0.748 | Val F2: 0.757 | Train MAE: 11.734 | Val MAE: 13.979\nFold 6 | Ep 10 | Train F2: 0.742 | Val F2: 0.753 | Train MAE: 11.674 | Val MAE: 13.904\nFold 6 | Ep 11 | Train F2: 0.746 | Val F2: 0.755 | Train MAE: 11.691 | Val MAE: 14.041\nFold 6 | Ep 12 | Train F2: 0.749 | Val F2: 0.759 | Train MAE: 11.629 | Val MAE: 13.936\n  Early stopping triggered.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[31m2025/12/10 05:20:51 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001B[0m\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c63684735e5449ab6dc3065afb6eab9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  >> Fold 6 Best Val F2: 0.7599\n\n--- Starting Fold 7 ---\nFold 7 | Ep 0 | Train F2: 0.737 | Val F2: 0.736 | Train MAE: 12.902 | Val MAE: 13.798\nFold 7 | Ep 1 | Train F2: 0.741 | Val F2: 0.729 | Train MAE: 12.819 | Val MAE: 13.736\nFold 7 | Ep 2 | Train F2: 0.744 | Val F2: 0.737 | Train MAE: 12.756 | Val MAE: 13.714\nFold 7 | Ep 3 | Train F2: 0.754 | Val F2: 0.751 | Train MAE: 12.705 | Val MAE: 13.682\nFold 7 | Ep 4 | Train F2: 0.752 | Val F2: 0.750 | Train MAE: 12.629 | Val MAE: 13.626\nFold 7 | Ep 5 | Train F2: 0.738 | Val F2: 0.733 | Train MAE: 12.648 | Val MAE: 13.669\nFold 7 | Ep 6 | Train F2: 0.748 | Val F2: 0.740 | Train MAE: 12.591 | Val MAE: 13.654\nFold 7 | Ep 7 | Train F2: 0.756 | Val F2: 0.751 | Train MAE: 12.517 | Val MAE: 13.616\nFold 7 | Ep 8 | Train F2: 0.751 | Val F2: 0.744 | Train MAE: 12.544 | Val MAE: 13.644\nFold 7 | Ep 9 | Train F2: 0.755 | Val F2: 0.750 | Train MAE: 12.465 | Val MAE: 13.597\nFold 7 | Ep 10 | Train F2: 0.745 | Val F2: 0.731 | Train MAE: 12.578 | Val MAE: 13.792\nFold 7 | Ep 11 | Train F2: 0.757 | Val F2: 0.754 | Train MAE: 12.425 | Val MAE: 13.681\nFold 7 | Ep 12 | Train F2: 0.754 | Val F2: 0.745 | Train MAE: 12.382 | Val MAE: 13.680\nFold 7 | Ep 13 | Train F2: 0.756 | Val F2: 0.754 | Train MAE: 12.339 | Val MAE: 13.566\nFold 7 | Ep 14 | Train F2: 0.750 | Val F2: 0.732 | Train MAE: 12.269 | Val MAE: 13.579\nFold 7 | Ep 15 | Train F2: 0.752 | Val F2: 0.740 | Train MAE: 12.299 | Val MAE: 13.640\nFold 7 | Ep 16 | Train F2: 0.753 | Val F2: 0.741 | Train MAE: 12.222 | Val MAE: 13.640\n  Early stopping triggered.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[31m2025/12/10 06:05:10 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001B[0m\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f124d28d77bf4e02b68a20f263459af4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  >> Fold 7 Best Val F2: 0.7537\n\n--- Starting Fold 8 ---\nFold 8 | Ep 0 | Train F2: 0.752 | Val F2: 0.696 | Train MAE: 13.112 | Val MAE: 11.134\nFold 8 | Ep 1 | Train F2: 0.750 | Val F2: 0.690 | Train MAE: 12.960 | Val MAE: 11.082\nFold 8 | Ep 2 | Train F2: 0.758 | Val F2: 0.703 | Train MAE: 12.902 | Val MAE: 11.054\nFold 8 | Ep 3 | Train F2: 0.752 | Val F2: 0.692 | Train MAE: 12.868 | Val MAE: 11.031\nFold 8 | Ep 4 | Train F2: 0.755 | Val F2: 0.700 | Train MAE: 12.797 | Val MAE: 11.099\nFold 8 | Ep 5 | Train F2: 0.756 | Val F2: 0.701 | Train MAE: 12.767 | Val MAE: 11.073\nFold 8 | Ep 6 | Train F2: 0.757 | Val F2: 0.701 | Train MAE: 12.706 | Val MAE: 11.038\nFold 8 | Ep 7 | Train F2: 0.757 | Val F2: 0.703 | Train MAE: 12.724 | Val MAE: 11.208\nFold 8 | Ep 8 | Train F2: 0.757 | Val F2: 0.700 | Train MAE: 12.642 | Val MAE: 11.123\nFold 8 | Ep 9 | Train F2: 0.756 | Val F2: 0.700 | Train MAE: 12.590 | Val MAE: 11.059\nFold 8 | Ep 10 | Train F2: 0.755 | Val F2: 0.695 | Train MAE: 12.589 | Val MAE: 11.116\nFold 8 | Ep 11 | Train F2: 0.759 | Val F2: 0.704 | Train MAE: 12.657 | Val MAE: 11.266\nFold 8 | Ep 12 | Train F2: 0.758 | Val F2: 0.704 | Train MAE: 12.522 | Val MAE: 11.124\nFold 8 | Ep 13 | Train F2: 0.752 | Val F2: 0.694 | Train MAE: 12.530 | Val MAE: 11.202\nFold 8 | Ep 14 | Train F2: 0.760 | Val F2: 0.704 | Train MAE: 12.499 | Val MAE: 11.209\nFold 8 | Ep 15 | Train F2: 0.759 | Val F2: 0.703 | Train MAE: 12.462 | Val MAE: 11.190\nFold 8 | Ep 16 | Train F2: 0.759 | Val F2: 0.702 | Train MAE: 12.429 | Val MAE: 11.191\n  Early stopping triggered.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[31m2025/12/10 06:48:46 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001B[0m\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7e053ed533d4e2fb3f5537cecc4dfd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  >> Fold 8 Best Val F2: 0.7039\n\n--- Starting Fold 9 ---\nFold 9 | Ep 0 | Train F2: 0.754 | Val F2: 0.644 | Train MAE: 13.414 | Val MAE: 8.722\nFold 9 | Ep 1 | Train F2: 0.758 | Val F2: 0.662 | Train MAE: 13.489 | Val MAE: 8.848\nFold 9 | Ep 2 | Train F2: 0.755 | Val F2: 0.637 | Train MAE: 13.218 | Val MAE: 8.670\nFold 9 | Ep 3 | Train F2: 0.757 | Val F2: 0.639 | Train MAE: 13.179 | Val MAE: 8.755\nFold 9 | Ep 4 | Train F2: 0.758 | Val F2: 0.638 | Train MAE: 13.159 | Val MAE: 8.802\nFold 9 | Ep 5 | Train F2: 0.761 | Val F2: 0.662 | Train MAE: 13.348 | Val MAE: 8.708\nFold 9 | Ep 6 | Train F2: 0.752 | Val F2: 0.620 | Train MAE: 13.086 | Val MAE: 8.719\nFold 9 | Ep 7 | Train F2: 0.758 | Val F2: 0.630 | Train MAE: 12.949 | Val MAE: 8.716\nFold 9 | Ep 8 | Train F2: 0.757 | Val F2: 0.619 | Train MAE: 12.936 | Val MAE: 8.636\nFold 9 | Ep 9 | Train F2: 0.760 | Val F2: 0.640 | Train MAE: 12.887 | Val MAE: 8.711\nFold 9 | Ep 10 | Train F2: 0.762 | Val F2: 0.662 | Train MAE: 12.855 | Val MAE: 8.646\n  Early stopping triggered.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[31m2025/12/10 07:18:53 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001B[0m\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f45491ae3a242d887e613226a471cb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  >> Fold 9 Best Val F2: 0.6624\n\n--- Starting Fold 10 ---\nFold 10 | Ep 0 | Train F2: 0.743 | Val F2: 0.652 | Train MAE: 13.126 | Val MAE: 8.689\nFold 10 | Ep 1 | Train F2: 0.751 | Val F2: 0.657 | Train MAE: 13.017 | Val MAE: 8.782\nFold 10 | Ep 2 | Train F2: 0.748 | Val F2: 0.617 | Train MAE: 12.916 | Val MAE: 8.591\nFold 10 | Ep 3 | Train F2: 0.748 | Val F2: 0.624 | Train MAE: 12.857 | Val MAE: 8.661\nFold 10 | Ep 4 | Train F2: 0.750 | Val F2: 0.635 | Train MAE: 12.849 | Val MAE: 8.708\nFold 10 | Ep 5 | Train F2: 0.752 | Val F2: 0.655 | Train MAE: 12.761 | Val MAE: 8.569\nFold 10 | Ep 6 | Train F2: 0.745 | Val F2: 0.641 | Train MAE: 12.818 | Val MAE: 8.712\n  Early stopping triggered.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[31m2025/12/10 07:36:49 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001B[0m\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85da39e357c94ec78c0fe3de20073b70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  >> Fold 10 Best Val F2: 0.6572\n\n=== CV Complete ===\nAvg Best F2: 0.7352\n"
     ]
    }
   ],
   "source": [
    "# import pyspark.sql.functions as sf  # Safe alias for Spark\n",
    "# import torch.nn.functional as F     # Safe alias for Torch\n",
    "# import numpy as np\n",
    "# from pyspark.sql import Window\n",
    "# import torch\n",
    "# from torch.utils.data import DataLoader, Dataset\n",
    "# import torch.nn as nn\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.metrics import fbeta_score, roc_auc_score, mean_squared_error, mean_absolute_error\n",
    "# import copy\n",
    "# import mlflow\n",
    "\n",
    "# # ==========================================\n",
    "# # 1. FIX: Deduplicate Numerical Columns\n",
    "# # ==========================================\n",
    "# numerical_cols = list(dict.fromkeys(numerical_cols))\n",
    "# print(f\"Numerical columns count after deduplication: {len(numerical_cols)}\")\n",
    "\n",
    "# # ==========================================\n",
    "# # 2. Redefine FE Functions (Namespace Safe)\n",
    "# # ==========================================\n",
    "# def add_time_features(df):\n",
    "#     df = df.withColumn(\"dep_hour\", sf.col(\"CRS_DEP_MINUTES\") / 60.0)\n",
    "#     df = df.withColumn(\"day_of_year\", sf.dayofyear(\"utc_timestamp\").cast(\"double\"))\n",
    "#     df = df.withColumn(\"dep_hour_sin\", sf.sin(2 * sf.lit(np.pi) * sf.col(\"dep_hour\") / 24))\n",
    "#     df = df.withColumn(\"dep_hour_cos\", sf.cos(2 * sf.lit(np.pi) * sf.col(\"dep_hour\") / 24))\n",
    "#     df = df.withColumn(\"dow_sin\", sf.sin(2 * sf.lit(np.pi) * sf.col(\"DAY_OF_WEEK\") / 7))\n",
    "#     df = df.withColumn(\"dow_cos\", sf.cos(2 * sf.lit(np.pi) * sf.col(\"DAY_OF_WEEK\") / 7))\n",
    "#     df = df.withColumn(\"doy_sin\", sf.sin(2 * sf.lit(np.pi) * sf.col(\"day_of_year\") / 365))\n",
    "#     df = df.withColumn(\"doy_cos\", sf.cos(2 * sf.lit(np.pi) * sf.col(\"day_of_year\") / 365))\n",
    "#     return df\n",
    "\n",
    "# def add_weather_deltas(df):\n",
    "#     w = Window.partitionBy(\"ORIGIN_AIRPORT_SEQ_ID\").orderBy(\"utc_timestamp\")\n",
    "#     for col_name in [\"HourlyVisibility\", \"HourlyStationPressure\", \"HourlyDryBulbTemperature\", \"HourlyWindSpeed\", \"HourlyPrecipitation\"]:\n",
    "#         lag_col = sf.lag(col_name, 3).over(w)\n",
    "#         delta_col = sf.col(col_name) - lag_col\n",
    "#         df = df.withColumn(f\"{col_name}_3h_change\", sf.when(lag_col.isNull(), sf.lit(None)).otherwise(delta_col))\n",
    "#     return df\n",
    "\n",
    "# def add_congestion_features(df):\n",
    "#     df = df.withColumn(\"utc_ts_sec\", sf.col(\"utc_timestamp\").cast(\"long\"))\n",
    "#     w = Window.partitionBy(\"ORIGIN_AIRPORT_SEQ_ID\").orderBy(\"utc_ts_sec\").rangeBetween(-3600, 0)\n",
    "#     df = df.withColumn(\"ground_flights_last_hour\", sf.count(\"utc_ts_sec\").over(w) - 1)\n",
    "#     return df\n",
    "\n",
    "# def add_dest_congestion_features(df):\n",
    "#     df = df.withColumn(\"utc_ts_sec\", sf.col(\"utc_timestamp\").cast(\"long\"))\n",
    "#     w = Window.partitionBy(\"DEST_AIRPORT_SEQ_ID\").orderBy(\"utc_ts_sec\").rangeBetween(-3600, 0)\n",
    "#     df = df.withColumn(\"arrivals_last_hour\", sf.count(\"utc_ts_sec\").over(w) - 1)\n",
    "#     return df\n",
    "\n",
    "# # ==========================================\n",
    "# # 3. Fold Prep (Strict Filtering)\n",
    "# # ==========================================\n",
    "# def prepare_fold_data(fold_df, categorical_cols, numerical_cols, time_col, target_col):\n",
    "#     train_df_spark = fold_df.filter(sf.col(\"split_type\") == \"train\")\n",
    "#     val_df_spark   = fold_df.filter(sf.col(\"split_type\") == \"validation\")\n",
    "\n",
    "#     train_fe = (train_df_spark.transform(add_time_features).transform(add_weather_deltas)\n",
    "#                 .transform(add_congestion_features).transform(add_dest_congestion_features))\n",
    "#     val_fe   = (val_df_spark.transform(add_time_features).transform(add_weather_deltas)\n",
    "#                 .transform(add_congestion_features).transform(add_dest_congestion_features))\n",
    "\n",
    "#     train_pd = train_fe.select(categorical_cols + numerical_cols + [time_col, target_col]).toPandas()\n",
    "#     val_pd   = val_fe.select(categorical_cols + numerical_cols + [time_col, target_col]).toPandas()\n",
    "\n",
    "#     cat_maps = build_category_maps(train_pd, categorical_cols)\n",
    "#     train_pd = apply_category_maps(train_pd, cat_maps, categorical_cols)\n",
    "#     val_pd   = apply_category_maps(val_pd, cat_maps, categorical_cols)\n",
    "\n",
    "#     scaler = StandardScaler()\n",
    "#     train_pd[numerical_cols] = scaler.fit_transform(train_pd[numerical_cols])\n",
    "#     val_pd[numerical_cols]   = scaler.transform(val_pd[numerical_cols])\n",
    "\n",
    "#     cat_dims = [len(cat_maps[c]) for c in categorical_cols]\n",
    "#     emb_dims = [min(64, int(n**0.3)) for n in cat_dims]\n",
    "\n",
    "#     return train_pd, val_pd, cat_dims, emb_dims\n",
    "\n",
    "# # ==========================================\n",
    "# # 4. Main Execution Loop\n",
    "# # ==========================================\n",
    "# def get_device(): return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# def train_epoch(model, dataloader, optimizer, criterion_reg, criterion_clf, device, alpha=0.5):\n",
    "#     model.train()\n",
    "#     total_loss = 0\n",
    "#     for cat, num, time, y in dataloader:\n",
    "#         cat, num, time, y = cat.to(device), num.to(device), time.to(device), y.to(device)\n",
    "#         optimizer.zero_grad()\n",
    "#         reg_out, clf_out = model(cat, num, time)\n",
    "#         y_bin = (y > 0).float()\n",
    "#         loss = (alpha * criterion_reg(reg_out, y)) + ((1 - alpha) * criterion_clf(clf_out, y_bin))\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         total_loss += loss.item()\n",
    "#     return total_loss / len(dataloader)\n",
    "\n",
    "# def evaluate(model, dataloader, device):\n",
    "#     model.eval()\n",
    "#     preds_reg, preds_clf, targets_reg, targets_clf = [], [], [], []\n",
    "#     with torch.no_grad():\n",
    "#         for cat, num, time, y in dataloader:\n",
    "#             cat, num, time, y = cat.to(device), num.to(device), time.to(device), y.to(device)\n",
    "#             reg_out, clf_out = model(cat, num, time)\n",
    "#             preds_reg.append(reg_out.cpu()); preds_clf.append(torch.sigmoid(clf_out).cpu())\n",
    "#             targets_reg.append(y.cpu()); targets_clf.append((y > 0).float().cpu())\n",
    "            \n",
    "#     y_pred_reg, y_pred_clf = torch.cat(preds_reg).numpy(), torch.cat(preds_clf).numpy()\n",
    "#     y_true_reg, y_true_clf = torch.cat(targets_reg).numpy(), torch.cat(targets_clf).numpy()\n",
    "    \n",
    "#     mae = mean_absolute_error(y_true_reg, y_pred_reg)\n",
    "#     rmse = np.sqrt(mean_squared_error(y_true_reg, y_pred_reg))\n",
    "#     try: auc = roc_auc_score(y_true_clf, y_pred_clf)\n",
    "#     except: auc = 0.5\n",
    "#     f2 = fbeta_score(y_true_clf, (y_pred_clf > 0.5).astype(int), beta=2)\n",
    "#     return {\"mae\": mae, \"rmse\": rmse, \"auc\": auc, \"f2_score\": f2}\n",
    "\n",
    "# # --- Config ---\n",
    "# CV_DATA_PATH = \"dbfs:/student-groups/Group_2_2/1_year_custom_joined/fe_graph_and_holiday_nnfeat/cv_splits\"\n",
    "# NUM_EPOCHS, BATCH_SIZE, LR, PATIENCE = 20, 2048, 1e-3, 5\n",
    "# DEVICE = get_device()\n",
    "\n",
    "# cv_full_df = spark.read.parquet(CV_DATA_PATH)\n",
    "# folds = sorted([row['fold_id'] for row in cv_full_df.select(\"fold_id\").distinct().collect()])\n",
    "# print(f\"Starting Cross-Validation on Folds: {folds}\")\n",
    "\n",
    "# mlflow.end_run()\n",
    "# with mlflow.start_run(run_name=\"CV_Orchestrator_MAE_F2\") as parent_run:\n",
    "#     cv_summary = {\"best_val_f2\": [], \"final_val_mae\": []}\n",
    "    \n",
    "#     for fold in folds:\n",
    "#         print(f\"\\n--- Starting Fold {fold} ---\")\n",
    "#         with mlflow.start_run(run_name=f\"Fold_{fold}\", nested=True):\n",
    "#             fold_data = cv_full_df.filter(sf.col(\"fold_id\") == fold)\n",
    "            \n",
    "#             # --- PREPARE DATA ---\n",
    "#             train_pd, val_pd, cat_dims, emb_dims = prepare_fold_data(\n",
    "#                 fold_data, categorical_cols, numerical_cols, time_col, target_col\n",
    "#             )\n",
    "            \n",
    "#             train_dl = DataLoader(FlightDataset(train_pd), batch_size=BATCH_SIZE, shuffle=True)\n",
    "#             val_dl = DataLoader(FlightDataset(val_pd), batch_size=BATCH_SIZE)\n",
    "            \n",
    "#             # --- MODEL ---\n",
    "#             model = ResFiLMMLP(cat_dims, emb_dims, len(numerical_cols), time_dim=8).to(DEVICE)\n",
    "#             optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n",
    "#             crit_reg = nn.L1Loss()\n",
    "#             crit_clf = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([4.0]).to(DEVICE))\n",
    "\n",
    "#             # --- TRAINING ---\n",
    "#             best_f2 = -1.0; best_model_state = None; early_stop_counter = 0\n",
    "            \n",
    "#             for epoch in range(NUM_EPOCHS):\n",
    "#                 train_loss = train_epoch(model, train_dl, optimizer, crit_reg, crit_clf, DEVICE)\n",
    "                \n",
    "#                 # --- ADDED: Evaluate on Train and Val ---\n",
    "#                 train_metrics = evaluate(model, train_dl, DEVICE)\n",
    "#                 val_metrics = evaluate(model, val_dl, DEVICE)\n",
    "                \n",
    "#                 # --- UPDATED PRINT ---\n",
    "#                 print(f\"Fold {fold} | Ep {epoch} | \"\n",
    "#                       f\"Train F2: {train_metrics['f2_score']:.3f} | Val F2: {val_metrics['f2_score']:.3f} | \"\n",
    "#                       f\"Train MAE: {train_metrics['mae']:.3f} | Val MAE: {val_metrics['mae']:.3f}\")\n",
    "                \n",
    "#                 # --- UPDATED LOGGING ---\n",
    "#                 mlflow.log_metrics({\n",
    "#                     \"train_loss\": train_loss,\n",
    "#                     \"train_f2\": train_metrics['f2_score'],\n",
    "#                     \"train_mae\": train_metrics['mae'],\n",
    "#                     \"val_f2\": val_metrics['f2_score'], \n",
    "#                     \"val_mae\": val_metrics['mae'],\n",
    "#                     \"val_rmse\": val_metrics['rmse'], \n",
    "#                     \"val_auc\": val_metrics['auc']\n",
    "#                 }, step=epoch)\n",
    "                \n",
    "#                 if val_metrics['f2_score'] > best_f2:\n",
    "#                     best_f2 = val_metrics['f2_score']\n",
    "#                     best_model_state = copy.deepcopy(model.state_dict())\n",
    "#                     early_stop_counter = 0\n",
    "#                     mlflow.log_metrics({\"best_val_f2_so_far\": best_f2}, step=epoch)\n",
    "#                 else:\n",
    "#                     early_stop_counter += 1\n",
    "#                     if early_stop_counter >= PATIENCE:\n",
    "#                         print(\"  Early stopping triggered.\"); break\n",
    "\n",
    "#             if best_model_state:\n",
    "#                 model.load_state_dict(best_model_state)\n",
    "#                 mlflow.pytorch.log_model(model, f\"model_fold_{fold}_best\")\n",
    "#                 print(f\"  >> Fold {fold} Best Val F2: {best_f2:.4f}\")\n",
    "            \n",
    "#             cv_summary[\"best_val_f2\"].append(best_f2)\n",
    "#             cv_summary[\"final_val_mae\"].append(val_metrics['mae'])\n",
    "\n",
    "#     print(f\"\\n=== CV Complete ===\\nAvg Best F2: {np.mean(cv_summary['best_val_f2']):.4f}\")\n",
    "#     mlflow.log_metric(\"cv_avg_best_f2\", np.mean(cv_summary['best_val_f2']))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "NN_MLP_Pipeline_MK2",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
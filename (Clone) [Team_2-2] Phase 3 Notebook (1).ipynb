{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f60f6794-b509-44c6-b3bc-0a78d894b6f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# **Cleared for Takeoff:** _Moderate to Severe Flight Delay Classification_\n",
    "### Team 2-2\n",
    "Daniel Costa (daniel_costa@berkeley.edu)<br>\n",
    "Ryan Farhat-Sabet (ryan_farhat-sabet@berkeley.edu)<br>\n",
    "Ankush Garg (ankush-garg@berkeley.edu) <br>\n",
    "Maia Kennedy (maia_kennedy@berkeley.edu)<br>\n",
    "Stephanie Owyang (seowyang@berkeley.edu)<br><br>\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/stephanie-cal/mids261-final_project/main/team_picture.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1f41df99-17d7-407e-b8b2-9e42c5320ae5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Phase Leader Plan\n",
    "\n",
    "| Week | Leader | \n",
    "| :--- | :--- | \n",
    "| 10/27 (Phase 1) | Stephanie Owyang | \n",
    "| 11/3 | Daniel Costa|\n",
    "| 11/10 (Fall Break) | Ankush Garg| \n",
    "| 11/17 (Phase 2) | Ankush Garg| \n",
    "| 11/24 (Thanksgiving Break)| Maia Kennedy|\n",
    "| 12/1 | Ryan Farhat-Sabet|\n",
    "| 12/8 (Phase 3) | Daniel Costa | \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5cda683b-ceb6-4818-8144-5dc4cc9fbdee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Phase 3: Credit Assignment Plan - Team Member View\n",
    "\n",
    "| **Team Member** | **Task** | **Est. Hours** |\n",
    "| :--- | :--- | :--- |\n",
    "| **[Ankush Garg]** | Evolve abstract for Phase 3 | 1 |\n",
    "| | Error analysis to identify problem areas for the model | 3 |\n",
    "| | Model stability analysis across multiple years | 3 |\n",
    "| | Presentation prep - EDA| 4 |\n",
    "| | Evolving XGB regressor and code adaptations | 6 |\n",
    "| | Training/CV splits | 1 |\n",
    "| | Report write-up and QA | 4 |\n",
    "| | **Subtotal** | **22** |\n",
    "| **Daniel Costa** | Slide prep/visualizatoins for presentation | 3 |\n",
    "| | 2-Stage Quantile Model Development/Tuning | 18 |\n",
    "| | Results Parsing and Write-Up | 3 |\n",
    "| | Metrics/Algorithms Write-Up | 2 |\n",
    "| | Team Lead - Meeting setup, write-up consolidation | 5 |\n",
    "| | Graph Feature Engineering | 4 |\n",
    "| | **Subtotal** | **35** |\n",
    "| **Ryan Farhat-Sabet** | EDA | 3 |\n",
    "| | Building CV and OOF strategy for stacked models | 10 |\n",
    "| | Training final XGBoost model and writing outputs to intermediary dataset | 10 |\n",
    "| | Building first stacked MLP model | 20 |\n",
    "| | Slide prep for presentation | 2 |\n",
    "| | Write-up for report | 4 |\n",
    "| | **Subtotal** | **49** |\n",
    "| **Maia Kennedy** | Developed code for additional feature engineering for NN | 2 |\n",
    "| | Multi-tower neural network architecture design and implementation  | 18 |\n",
    "| | Multi-tower neural network hyperparameter tuning | 15 |\n",
    "| | Multi-tower model training and inference | 10 |\n",
    "| | Prepare overall team slide template and organize flow for FP3 presentation | 3 |\n",
    "| | Author modelling pipeline, conclusion and multi-tower neural network architecture sections for report | 5 |\n",
    "| | **Subtotal** | **53** |\n",
    "| **Stephanie Owyang** | Phase 3 presentation preparation | 2 |\n",
    "| | Feature Engineering EDA and Analysis | 6 |\n",
    "| | Developed code for feature engineering (6 features)| 4 |\n",
    "| | Error Analysis | 2 |\n",
    "| | Feature Importance | 3 |\n",
    "| | Feature selection and Feature engineering write-up | 2 |\n",
    "| | Project Description write-up | 2 |\n",
    "| | Leakage write-up | 2 |\n",
    "| | Final Formatting and git integration| 2 |\n",
    "| | **Subtotal** | **25** |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5ffbb8d6-b169-4af8-abc2-c1d01d02d0c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Abstract\n",
    "\n",
    "Air traffic management faces significant challenges from flight delays, which create airspace congestion, complicate traffic flow sequencing, and strain runway capacity. This project develops a model to predict flight delays of 15 minutes or more within a 2-hour prediction window, enabling proactive operational adjustments by `air traffic controllers`. We use a custom-joined dataset combining a flights dataset from the Department of Transportation (DOT) and weather dataset from the National Oceanic and Atmospheric Administration (NOAA). Our exploratory analysis revealed significant class imbalance, with only 18.29% of flights experiencing delays of 15 minutes or more, necessitating careful consideration of both precision and recall in our evaluation approach.\n",
    "\n",
    "We established a baseline using an XGBoost Regressor trained on engineered features through sliding-window cross-validation, achieving a test MAE of 14.32 minutes and F2 score of 0.54 when applying a 15-minute threshold for binary classification. Building on this foundation, we developed multi-stage architectures in which a regressor predicts delay magnitude and a downstream classifier determines whether delays exceed 15 minutes. We evaluated several approaches including a 2-tiered XGBoost Regressor paired with Random Forest classifier, 2-tiered XGBoost Regressor with Multilayer Perceptron classifier, and a multi-tower Deep Neural Network. \n",
    "\n",
    "The Two-Tower neural network achieved strong performance, with an optimied F2 score of 0.6187 and an MAE of 11.33 minutes (improvement of 2.88 minutes over the baseline). The best overall performance was obtained by the two-stage quantile XGBoost model, which achieved a treshold-optimized F2 score of 0.621, representing an improvement of approximately 0.08 over the baseline. These results demonstrate that multi-stage modeling and treshold optimization meaningfully improve recall-focused delay prediciton in highly imbalanced settings.\n",
    "\n",
    "Key engineered features driving performance include weather imputation using a kriging-interpolated weather join, temporal features such as origin delay in the last 4 hours and average route delay, and graph-based features including airport connectivity ratings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fd91e83f-4f41-4d66-bd8c-1877e16c54c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Project Description\n",
    "\n",
    "We are building a machine learning algorithm dedicated to providing Air Traffic Control (ATC) and Airspace Managers with high-accuracy, advance forecasting of flight delays. Recognizing that about 78% [1, 2] of disruptions occur on the ground (at the gate or during taxi), our model shifts the operational paradigm from reactive response to proactive intervention. By processing real-time data on flight schedules, terminal congestion, weather, and historical ground movement, the system accurately predicts the probability and severity of a delay before the aircraft even leaves the gate, and communicates the result in a clear binary status to the air traffic controllers.\n",
    "\n",
    "The core metric of this algorithm is predicting a delay that exceeds the 15 minutes from the aircraft’s scheduled departure time. This is not arbitrary, 15 minutes is the regulatory standard used by the FAA and the Bureau of Transportation Statistics (BTS) to officially define a flight as \"delayed\" and is the key benchmark for all official On-Time Performance reporting. For ATC, this threshold represents the critical point where a delay moves from a minor event to a disruption requiring immediate operational change, such as coordinating with the Traffic Management Unit for slot adjustments, implementing ground holds, or adjusting taxi sequences. By targeting this specific, actionable metric, our model ensures its output is immediately relevant to controller decision-making.\n",
    "\n",
    "This predictive capability offers immense operational and financial value. Delays cost the U.S. aviation system over $30 billion each year, and every minute of delay in the air can cost an airline $100 [3]. With an average of 78% of delays occurring before takeoff and 60% [2] at the gate, our model provides the controller with the necessary minutes to hours of advance warning. This lead time transforms reactive event management into proactive decision support. For example, by accurately forecasting a greater than or equal to 15-minute gate delay, ATC can coordinate with ramp controllers to hold non-delayed traffic for optimal sequencing, reducing taxi queue times, and minimizing the risk of the delay propagating across sectors. \n",
    "\n",
    "Our model is designed to be fully compatible with upcoming ATC infrastructure improvements in the FAA’s Next Generation Air Transportation System (NextGen) that focus on advanced automation and decision support systems. These tools shift the cognitive burden from the controller's memory and manual calculations to computers, enabling more proactive and precise traffic management. Some of these upgrades include: \n",
    "- **Terminal Flight Data Manager (TFDM)**: This system modernizes control tower operations by replacing paper flight progress strips with digital displays. It automates surface metering and provides controllers and airlines with a common, real-time view of departure queues and predicted taxi times, reducing runway and ramp congestion.\n",
    "- **Trajectory-Based Operations (TBO)**: Instead of constantly reacting to an aircraft's current position, TBO allows controllers and systems to predict a precise, four-dimensional (latitude, longitude, altitude, and time) trajectory for every flight. This enables more accurate and proactive flow management decisions long before the aircraft enters a congested area, reducing the need for stressful, last-minute vectoring and holding.\n",
    "- **Time Based Flow Management (TBFM)**: A tool that schedules arrivals to airports far in advance, giving air traffic controllers in en-route centers (ARTCCs) the target time when an aircraft needs to cross a specific point.\n",
    "\n",
    "We are strategically positioned to function as a crucial predictive layer within the FAA's NextGen upgrades. The model's refined delay forecast can be directly consumed by the Terminal Flight Data Manager (TFDM), enhancing the accuracy of its surface metering and digitized flight progress strips. For Time Based Flow Management (TBFM), our output provides a more realistic input for calculating accurate target times for en-route constraints. Furthermore, by providing a validated, data-driven delay status, our model supports the long-term vision of Trajectory-Based Operations (TBO), ensuring that the 4D flight path predictions are based on the most accurate and current information from the ground.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "23505007-7126-4736-b259-0accfd5bc6a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Data Description\n",
    "We began by reviewing the three primary datasets and their supporting auxiliary sources: (1) a comprehensive U.S. passenger flight dataset, (2) an airport reference dataset containing airport codes and locations, (3) a detailed weather dataset with observations recorded at specific stations and times, and (4) a companion weather-station dataset providing station locations. We also considered an existing combined flight–weather dataset (OPTW), but because its construction methodology is unknown, we did not use it. Instead, we created our own custom, transparent flight-weather join tailored to the needs of this project.\n",
    "\n",
    "<br>\n",
    "\n",
    "- Flight Dataset\n",
    "    - source: [Department of Transportation (DOT)](https://www.transtats.bts.gov/Tables.asp?QO_VQ=EFD&QO_anzr=Nv4yv0r%FDb0-gvzr%FDcr4s14zn0pr%FDQn6n&QO_fu146_anzr=b0-gvzr)\n",
    "    - size (3-month dataset): 1,403,471 x 109 (0.09GB)\n",
    "    - size (1-year dataset): 7,422,037 x 109 (0.58GB)\n",
    "    - size (5-year dataset): 31,746,841 x 109 (2.41GB)\n",
    "- Airport Codes Dataset\n",
    "    - source: [Department of Transportation (DOT)](https://datahub.io/core/airport-codes)\n",
    "    - size: 57,421 x 12 (0.01GB)\n",
    "\n",
    "- Weather Dataset\n",
    "    - source: [National Oceanic and Atmospheric Administration repository (NOAA)](https://www.ncei.noaa.gov/pub/data/cdo/documentation/LCD_documentation.pdf)\n",
    "    - size (3-month dataset): 30,528,602 x 124 (1.09GB)\n",
    "    - size (1-year dataset): 131,937,550 x 124 (4.73GB)\n",
    "    - size (5-year dataset): 639,726,637 x 124 (23.35GB)\n",
    "\n",
    "- Weather Station Dataset\n",
    "    - size: 5,004,169 x 12 (0.05GB)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cbbfad0f-c54d-43bf-bbba-3332b38d6712",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d2e9c310-5e6c-4002-bd9f-bc2e15ab1e27",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We performed this analysis on the 3-month data first before expanding it to the 1-year data, then finally to the 5-year data. For ease of readability, we have included charts for the 5-year data below. For a full data dictionary of all features for each dataset and the 3-month/1-year charts, please refer to the EDA notebooks in the appendix.\n",
    "\n",
    "#### Flights Dataset\n",
    "\n",
    "The flight dataset comes from the Department of Transportation. It includes extensive flight metadata, including date and time information, departure/arrival airport and location data, airline information, and flight route status and timing. To call out a few specific ones that looked particularly interesting:\n",
    "- `FL_DATE` (date of the flight) and other date variables (`QUARTER`, `MONTH`, `DAY_OF_MONTH`, `DAY_OF_WEEK`)\n",
    "- `OP_CARRIER` (flight airline carrier)\n",
    "- `ORIGIN/DEST_AIRPORT` (airport each flight flew out of and landed into)\n",
    "- `DEP_TIME` (time of flight departure)\n",
    "- `DISTANCE` (flight route distance)\n",
    "\n",
    "As far as completeness of this dataset, we noticed the data was duplicated, so we first had to drop all duplicates. We then had to decide on a unique key to identify each flight, so we landed (no pun intended) on the combination of flight date, airline, flight number, and origin airport (some flights will have the same flight number on the same day if the aircraft is continuing to a different destination, so that origin airport part is crucial for uniqueness). All of the above-mentioned features have very few nulls, as do most of the flight metadata around takeoff and landing, including if a flight was delayed and for how long. Many of the fields having to do with when a flight was diverted are haphazardly populated at best because so few flights are diverted, so we decided to ignore those fields. Cause of delay fields also have many nulls, but this is because not all flights are delayed, so we can impute these values with zeros.\n",
    "<br><br>\n",
    "<img src=\"https://raw.githubusercontent.com/stephanie-cal/mids261-final_project/main/phase3_images/df_flights_nulls.png\">\n",
    "\n",
    "We did take a look at flight patterns over the course of the year, and we could see that most airlines seemed to run flights along similar schedules, with one day a week consistently seeing fewer flights. It's pretty much a given that date is going to be an important variable for predicting delays given that flight volume changes throughout the year, but day of week seems likely to be an important predictor as well.\n",
    "<br><br>\n",
    "<img src=\"https://raw.githubusercontent.com/stephanie-cal/mids261-final_project/main/phase3_images/df_flights_by_carrier.png\">\n",
    "<br><br>\n",
    "\n",
    "#### Airports Dataset\n",
    "\n",
    "Since the airport code dataset itself is mainly supplemental, we really only need enough data to join it to the flight data, so identity code and name should be enough. Before we joined it to the flight data, we examine it as its own dataset. There are 57,421 airports in the dataset itself. Some other identity-related fields have lots of nulls, but of those features with no or few nulls, the important ones we found interesting were:\n",
    "- Coordinates\n",
    "- Type (ex: `small_airport`, `medium_airport`, `large_airport`)\n",
    "- Elevation\n",
    "\n",
    "All airports have coded coordinates and type, and most open passenger airports have elevation, so no data deletion will be necessary here.\n",
    "<br><br>\n",
    "<img src=\"https://raw.githubusercontent.com/stephanie-cal/mids261-final_project/main/phase3_images/df_airports_nulls.png\">\n",
    "\n",
    "We can also see the distribution of the types of airports represented in this dataset in the chart below. As we can see, small airports account for more than half of the airports in the dataset:\n",
    "<br><br>\n",
    "<img src=\"https://raw.githubusercontent.com/stephanie-cal/mids261-final_project/main/phase3_images/df_airports_types.png\">\n",
    "\n",
    "The dataset itself has more than just US airports, so once we filtered it down, we could then join it together with the 5-year flights data to visualize more trends. We first looked at the number of flights, delayed and not delayed, by airport type, and we noticed that most flights were flying out of large airports, but of those that were delayed, more of those delayed flights flew out of large airports compared to medium airports (19% vs 16%). Airport size could be an important variable to use in our prediction model.\n",
    "<br><br>\n",
    "<img src=\"https://raw.githubusercontent.com/stephanie-cal/mids261-final_project/main/phase3_images/df_flights_by_airport_and_delay.png\">\n",
    "\n",
    "We then looked at the number of flights by airport itself. We saw that a few airports see the most flights in the country, and they are mostly concentrated around major metropolitan areas. We imagine larger flight volume means more opportunities for delays, so this could be another important variable we could use later on. Here we can see all flights for top 10 airports.\n",
    "<br><br>\n",
    "<img src=\"https://raw.githubusercontent.com/stephanie-cal/mids261-final_project/main/phase3_images/df_flights_top_airports.png\">\n",
    "<br><br>\n",
    "\n",
    "#### Weather Dataset\n",
    "\n",
    "Moving on to the weather and weather station data, the data is taken from the National Centers for Environmental Information, and it is the messiest of the bunch. We first realized this data was not limited to just the US, so we had to narrow down the data ourselves. We then identified a whole host of features we liked to potentially include:\n",
    "- Hourly data (wind speed, wind direction, dry bulb temp, relative humidity, visibility, sky conditions, precipitation)\n",
    "- Daily and Monthly data (temperature max/min/avg, sunrise/sunset, weather, snow, precipitation, wind)\n",
    "\n",
    "As far as completeness of this dataset, there is a lot of null data. A lot of this stems from the features themselves and how each datapoint is recorded. Each line is a point in time, so it could be the weather report at a very specific minute, whereas a lot of the data, especially a lot of the null data, is at the daily/monthly level. We should be able to impute and copy some of the daily/monthly metrics into the hourly data to fill in some of the nulls there. The hourly data that is null mainly stems from specific weather patterns that aren't always present, like precipitation, so we should be able to impute zero values for some of those variables.\n",
    "<br><br>\n",
    "<img src=\"https://raw.githubusercontent.com/stephanie-cal/mids261-final_project/main/phase3_images/df_weather_nulls.png\">\n",
    "\n",
    "Not all stations report in equal time increments as well. The below chart showcases this very well, as we can see spikes in reporting each day as most stations will at least have one daily report, and we can see a super spike each month as almost all stations report at least one report a month. It is interday that we get variation in how often each station reports. Since we will be doing our own custom join of the weather data with the flight data, we have a strategy to capture the most accurate weather reading at the time of each flight, but we will get to that in a moment.\n",
    "<br><br>\n",
    "<img src=\"https://raw.githubusercontent.com/stephanie-cal/mids261-final_project/main/phase3_images/df_weather_over_time.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f1218d81-e13a-4dca-a2e4-41ffb52b7183",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Custom Join\n",
    "\n",
    "#### Weather–Flight Data Integration and Spatial Imputation\n",
    "\n",
    "To enrich each flight record with localized weather conditions, we constructed a custom spatial–temporal join between multiple data sources. The workflow incorporated data cleaning, feature engineering, spatial proximity modeling, and a streamlined Kriging-based imputation strategy.\n",
    "\n",
    "#### Data Ingestion and Preprocessing\n",
    "\n",
    "We began by ingesting the flights dataset, removing duplicate records, and constructing a unique `flight_id` derived from flight attributes. The weather and station datasets were then ingested and cleaned. From the raw NOAA hourly observations, we...\n",
    "\n",
    "- Parsed the `HourlySkyConditions` field to extract numerical oktas (cloud cover) and cloud layer elevation.  \n",
    "- Transformed `HourlyWindSpeed` and `HourlyWindDirection` into orthogonal northward and eastward components, enabling linear imputation. \n",
    "- Retained only weather features with at least **80% non-null coverage** to ensure reliability.\n",
    "\n",
    "#### Spatial Mapping Between Airports and Weather Stations\n",
    "\n",
    "We then established spatial relationships between airports and nearby weather stations. Using latitude–longitude coordinates and the haversine distance formula, we identified all stations within a **50 km radius** of each airport, consistent with methods used by the [FAA](https://www.faa.gov/documentLibrary/media/Advisory_Circular/AC_150_5220-16E_w-chg1.pdf). These distances formed the basis for computing spatial interpolation weights.\n",
    "\n",
    "To approximate a Kriging system, we solved for spatial coefficients using these pairwise distances and normalized the final weights so that they summed to 1 for each airport, reflecting each station’s relative influence.\n",
    "\n",
    "#### Temporal Alignment and Time-Zone Normalization\n",
    "\n",
    "Because airports and their neighboring weather stations may span multiple time zones, all timestamps in both the flight and weather datasets were converted to GMT. This ensured that temporal joins were consistent across locations.\n",
    "\n",
    "#### Weather–Flight Join and Multi-Stage Aggregation\n",
    "\n",
    "The integration proceeded in several stages:\n",
    "\n",
    "1. **Airport–Station Association:**  \n",
    "   Joined each flight to its airport’s set of neighboring stations and precomputed spatial weights.\n",
    "\n",
    "2. **Weather Association:**  \n",
    "   Merged hourly weather readings by matching station IDs and filtering to observations between **2 and 12 hours prior to departure**. The lower bound is a constraint of this assignment, and the upper bound is supported by [weather imputation literature from the FAA](https://www.faa.gov/documentLibrary/media/Advisory_Circular/AC_150_5220-16E_w-chg1.pdf).\n",
    "\n",
    "3. **Temporal Aggregation:**  \n",
    "   For each station, aggregated weather measurements across the time window, normalizing by the number of non-null readings to avoid overweighting sparse data.\n",
    "\n",
    "4. **Spatial Imputation:**  \n",
    "   Computed a dot product between the station-level aggregated weather vectors and the station weights for that airport to generate the final imputed weather features.\n",
    "\n",
    "#### Computational Shortcut and Edge-Case Handling\n",
    "\n",
    "A practical challenge arises when a neighboring station lacks valid readings in the 2–12 hour window. In such cases the original Kriging weights, computed under the assumption of complete station availability, no longer apply. Recomputing weights dynamically for each flight would drastically increase runtime.\n",
    "\n",
    "To address this, we adopted a computational shortcut:  \n",
    "**renormalize the spatial weights over only the stations with available readings**, preserving their relative influence without re-solving the Kriging system.\n",
    "\n",
    "Across the 5-year dataset of **41.45 million flights**, only **34 flights** fell into a scenario where all stations with high positive weights lacked readings while mostly small negative-weight stations remained. Under this shortcut, these cases produced unrealistic imputed values (e.g., temperatures exceeding thousands of degrees Fahrenheit).\n",
    "\n",
    "To ensure physical plausibility, **all imputed weather values were clipped to the observed minimum and maximum ranges** found in the original weather dataset.\n",
    "\n",
    "Below is a summary of the join runs for all datasets:\n",
    "\n",
    "| Dataset | Number of Flights     | Join Runtime (m) | % Flights with 1+ Non-Null Readings | Parquet Filesize |\n",
    "|---------|---------------|----------|------------|---------|\n",
    "| 3 months      | 1,356,814     | 6.29 | 99.9967    | 215MB   |\n",
    "| 6 months      | 2,818,553     | 8.91      | 99.9981    | 460MB   |\n",
    "| 1 year     | 7,268,230     | 16.61    | 99.9965    | 1.082GB |\n",
    "| 5 years     | 41,449,407    | 64.73   | 99.9984    | 6.173GB  |\n",
    "\n",
    "This custom join reduced the number of null values  among our imputed features by 83%, and results from our baseline XGBRegressor model demonstrate our custom join's utility. Without the custom-joined features, our baseline model with naive classification head had an F2 of 0.5305. Addding these features improved F2 by 1.2% to 0.537.\n",
    "\n",
    "#### Final Joined EDA\n",
    "\n",
    "With all our data joined now, we can do a final round of EDA. We're once again only including charts for the 5-year data below. We can see the final dimensions of our joined cleaned dataset are 42,430,592 x 121. We can also see that most of the hourly weather data we've attached to each flight is filled in and not null, so our custom join has helped to provide important supplementary data on each flight!\n",
    "<br><br>\n",
    "<img src=\"https://raw.githubusercontent.com/stephanie-cal/mids261-final_project/main/phase3_images/df_custom_joined_nulls.png\">\n",
    "\n",
    "We can also run an additional version of our top flights by airport chart for delayed flights specifically, and we see a similar trend as before, with most delayed flights originating from large metropolitan areas.\n",
    "<br><br>\n",
    "<img src=\"https://raw.githubusercontent.com/stephanie-cal/mids261-final_project/main/phase3_images/custom_joined_top_delayed_flights_airports.png\">\n",
    "\n",
    "Let's look at a few new graphs, starting with a chart of the number of delayed flights by hour of each day, split out by airline. We can see a clear trend where delays pile up as each day goes on, starting with some delays in the morning and growing more and more until it reaches a peak in the early evening, before dropping rapidly as the night goes on. We can also see a few standout airlines with quite a few delays, Southwest and American being the most noticeable. Both this time component and airline itself are likely to be important variables to include in our model.\n",
    "<br><br>\n",
    "<img src=\"https://raw.githubusercontent.com/stephanie-cal/mids261-final_project/main/phase3_images/custom_joined_delayed_flight_times_airline.png\">\n",
    "\n",
    "We also wanted to look at the average daily delay rate over the course of the year, and while there weren't any obvious patterns, we can definitely see that there are times of the year when there are spikes or sustained spikes in delay rates that would be important for our model to consider.\n",
    "<br><br>\n",
    "<img src=\"https://raw.githubusercontent.com/stephanie-cal/mids261-final_project/main/phase3_images/custom_joined_delay_rate_2.png\">\n",
    "\n",
    "Finally, we had to do a gut check and look at all of our continuous variables to make sure we were accounting for any highly correlated variables. We could see very high correlation between the elapsed time, air time, and distance variables, so we wouldn't have use for all of them in our final model. Arrival delay and departure delay were also highly correlated, unsurprisingly, as a departure delay is almost certain to cause an arrival delay. Both arrival and departure delay were somewhat correlated to carrier and late aircraft delays, which made sense, but was still important to account for nonetheless. Lastly, the weather features had a bit more interaction amongst each other, most notably as wet bulb, dry bulb, and dew point temperature were all highly correlated. Once again, this is something we could keep an eye on as we tested features for our final model.\n",
    "<br><br>\n",
    "<img src=\"https://raw.githubusercontent.com/stephanie-cal/mids261-final_project/main/phase3_images/custom_joined_correlation.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7c8a36fb-6add-4842-bb8a-dfeee7801d39",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Feature Engineering\n",
    "Feature engineering is an important step for air traffic delay prediction, as it transforms raw flight data into the informative variables that our models can use to discern patterns. By engineering new features such as converting categorical airport codes into quantified hub classifications, calculating complex graph features like weighted out-degree, or extracting temporal patterns like lagged delays, we provide the model with actionable intelligence. This process elevates the model's ability to distinguish between noise and signal, ultimately increasing its predictive power, interpretability, and robustness in capturing the non-linear, systemic drivers of flight delays. We broke up the features into five feature families: categorical, weather, numerical, graph and temporal.\n",
    "Most features are fully derived from the Kriging-based custom join, and others use one column from the joined dataset and an external database or source.\n",
    "\n",
    "Here are some of the features we engineeredfor each family. A full list of engineered features is in the appendix. All feature engineering EDA was performed on the final training dataset from 2015 -2018. \n",
    "\n",
    "### Categorical\n",
    "A categorical feature is a variable whose values represent distinct groups or labels rather than quantifiable measurements such as Airline Code (e.g., UA, AA, DL) or Departure City. \n",
    "\n",
    "One of the first features we made is `route`, which uses the DOT flight data and concatenates the origin and destination airport with a hyphen. There were over 14000 unique routes, with many of the most popular routes going to large metropolitan areas like San Francisco, Los Angeles and New York as seen in the top 20 routes bar graph below.<br>\n",
    "<br>\n",
    "<img src=\"https://raw.githubusercontent.com/stephanie-cal/mids261-final_project/main/p3_images/top20_routes.png\">\n",
    "\n",
    "We introduced an Airline Category feature to test the hypothesis that carrier size significantly affects flight delays. Using the DOT's Financial Classification criteria, which categorizes carriers based on annual operating revenue, we grouped all unique airlines. This allowed us to transform carrier identity into a predictive, discrete variable that accounts for the varying operational scale.\n",
    "\n",
    "DOT  Financial Classification Criteria:\n",
    "- Major Airlines (Group 1): Scheduled service with annual operating revenue of over $1$ billion. This group includes the legacy carriers (American, Delta, United) and large low-cost carriers (Southwest).\n",
    "- National Airlines (Group2): Scheduled service with annual operating revenue between $100$ million and $1$ billion.\n",
    "- Regional Airlines (Group 3): Scheduled service with annual operating revenue less than $100$ million\n",
    "\n",
    "This table shows where each airline is categorized:\n",
    "| Category Code | Airline Category | Carrier Codes |\n",
    "|:---:|:---|:---|\n",
    "| 1 | Major Airlines | AA, AS, B6, DL, HA, UA, WN |\n",
    "| 2 | Regional Airlines | 9E, EV, MQ, OH, OO, US, VX, YV, YX |\n",
    "| 3 | Ultra Low Cost Airlines | F9, G4, NK |\n",
    "\n",
    "Here is a distribution of the airline categories, a majority of which are major carriers.<br>\n",
    "<img src=\"https://raw.githubusercontent.com/stephanie-cal/mids261-final_project/main/p3_images/airline_categories.png\">\n",
    "\n",
    "#### Weather\n",
    "The weather features come from NOAA and are further imputed by the Kriging-based custom join. NOAA provides comprehensive aviation weather features, ranging from surface observations like visibility, wind, and temperature to forecasted hazards such as turbulence, icing, and thunderstorms. These metrics can help delay prediction as they capture the operational constraints like reduced airport capacity under low visibility that directly increase flight times and risk of disruption. From the 124 weather measurement categories, we selected 12 and engineered an additional 4.\n",
    "\n",
    "- Hourly:\n",
    "  - Dry and Wet Bulb Temperature\n",
    "  - Dew Point Temperature\n",
    "  - Relative Humidity\n",
    "  - Altimiter SEtting\n",
    "  - Visibility\n",
    "  - Station Pressure\n",
    "  - Precipitation\n",
    "  - Cloud Coverage\n",
    "  - Cloud Elevation\n",
    "  - Wind Speed\n",
    "\n",
    "- 3 hour delta:\n",
    "  - Visibility\n",
    "  - Station Pressure\n",
    "  - Wind Speed\n",
    "  - Precipitation\n",
    "\n",
    "All weather features needed to be lagged in order to avoid leakage, so only the most up to date weather at the time or prediction could be used for the model decision. \n",
    "\n",
    "We selected hourly visibility to inspect, noting that the vast majority of operations (over 90%) occur above the five-mile threshold, with only 12% reported as clear (over ten miles), possibly constrained by sensor limits. The most significant predictive break, however, occurs at three miles of visibility. This level triggers the enforcement of Instrument Flight Rules (IFR), where pilots navigate solely by instruments. Because IFR mandates increased separation distance between arriving aircraft, any volume of flights occurring under this constraint leads to a rapid, non-linear increase in arrival queue length and significantly protracted landing approach times. Although an important threshold, it applies to less than 3% of flights.\n",
    "\n",
    "**Hourly Visibility (2015-2018)** <br>\n",
    "<img src=\"https://raw.githubusercontent.com/stephanie-cal/mids261-final_project/main/p3_images/hourly_visibility.png\">\n",
    "\n",
    "\n",
    "### Numerical\n",
    "Numerical features are quantitative variables that represent measurable quantities, allowing for mathematical operations and ranging over a set of continuous or discrete values. This category encompasses all quantitative inputs, including time-related variables (temporal features) and the output of complex computations (like graph metrics), which the model uses to capture scalable relationships within the data.\n",
    "\n",
    "**Carrier Airline Ratings** <br>\n",
    "The Airline Rating feature is a numerical, composite metric designed to capture an airline's latent operational quality or customer sentiment in a single, quantifiable score. Unlike basic on-time performance (which is just one number), this rating attempts to summarize the complex, unstructured data that truly reflects an airline's systemic efficiency and service reliability. Since there is no published comparison score of different airline’s customer ratings, it was created using a Large Language Model (LLM) by having the model act as a sophisticated \"judge\" or \"sentiment engine.\" \n",
    "\n",
    "The LLM was asked to rate each airline carrier on a likert scale from 1-5, where 5 is ‘very satisfied’ and 1 is ‘very dissatisfied’. These scores were based on studies like J.D Power North America Airline Satisfaction Study and the American Customer Satisfaction Index (ACSI), and focus on customer loyalty, service reputation, amenities, change fees, and overall satisfaction. Below is a bar graph of each carriers rating:\n",
    "<br>\n",
    "<img src=\"https://raw.githubusercontent.com/stephanie-cal/mids261-final_project/main/p3_images/carrier_ratings.png\">\n",
    "\n",
    "#### Graph\n",
    "Graph features are numeric representations of graph structure and properties that capture characteristics of individual nodes like degree,centrality or the entire graph, like density. They allow algorithms that primarily operate on vector data to utilize the complex relational information inherent in graph data structures.\n",
    "\n",
    "**PageRank**<br>\n",
    "PageRank captures an airport’s global importance in the flight network, reflecting its exposure to and influence on system-wide delay propagation. Values were determined after 20 iterations.<br>\n",
    "<img src=\"https://raw.githubusercontent.com/stephanie-cal/mids261-final_project/main/p3_images/pagerank_by_state.png\">\n",
    "\n",
    "The visualization of PageRank by state shows that Georgia has high network importance, suggesting ATL (Atlanta International Airport) is a common gateway or transfer point for domestic and international flights. It also highlights which states have major hubs, favoring central states that are likely transfer points, like Illinois, Colorado, compared to coastal states with known major hubs like California and New York.\n",
    "\n",
    "**Out-Degree (weighted and unweighted)** <br>\n",
    "Weighted-out degree (Total Outbound Traffic Volume) <br>\n",
    "The weighted out-degree is the sum of flights departing that location, making it a measure of the outbound traffic volume and operational load. In the air traffic network, a high weighted out-degree signifies that the airport is a major hub and can be a significant transmitter of delays throughout the system. When disruptions occur at these high-volume airports, the large number of immediately affected outgoing flights creates a severe and widespread delay cascade across the nation.\n",
    "\n",
    "Out-degree (Number of Unique Destinations) <br>\n",
    "The out-degree is the total count of unique flight destinations it serves, making it a measure of the connectivity and its reach across the network. A high out-degree indicates an airport is a major hub connecting to many different cities, which enhances travel options but increases operational diversity and complexity. This widespread connectivity means the airport is highly relevant for transfer traffic and, if delayed, can cause minor disruption to a large number of downstream markets.\n",
    "\n",
    "Looking at Out-Degree v.s Weighted Out Degree for the training dataset covering 2015-2018, effectively visualizes the Hub-and-Spoke Network structure by showing a large cluster of spokes, close to 0, that funnel traffic into a smaller number of hubs like Atlanta, Dulles, or JFK, that serve many destinations and handle a higher flight volumes.<br>\n",
    "<img src=\"https://raw.githubusercontent.com/stephanie-cal/mids261-final_project/main/p3_images/destinations_vs_flight_vol.png\">\n",
    "\n",
    "\n",
    "#### Temporal\n",
    "Temporal features provide chronological context into our models, capturing the dynamic nature of time-series data like air traffic and propagating delays. Some features are converted time-based information, like day of the week or time of day, into numerical variables that could reveal underlying seasonality, trends, and cyclical patterns. This allows models to learn when events are likely to occur.\n",
    "\n",
    "Two critical forms of temporal features are lagged and rolling features. Lagged features are past values of a variable X_(t-N) used to predict the current value X_t, directly capturing autocorrelation. In our case, N is at least 2 hours, since that is when we need to make our prediction, and X_t is the scheduled time of departure. \n",
    "\n",
    "Conversely, rolling features aggregate a statistic over a sliding time window of past observations. These features smooth out high-frequency noise and capture the local trend or volatility over a recent period. This allows the model to adapt quickly to recent, sustained changes in airport or airline performance caused by factors like continuous weather issues or temporary maintenance backlogs.\n",
    "\n",
    "**Cyclical Patterns**<br>\n",
    "The day of week sine and cosine features are created through a trigonometric transformation designed to solve the problem of modeling cyclical time. Time units like the day of the week (Monday through Sunday) are inherently circular, yet standard numerical representation, like encoding the days of the week 1 through 7, treats them linearly, causing a model to incorrectly perceive a large difference between Sunday (7) and Monday (1). To correct this, the raw day index is converted into two continuous numerical values (sine and cosine components) and plotted onto a unit circle . This ensures that the distance between the values at the beginning and end of the cycle is minimized, making the data mathematically accurate for models like linear regression or neural networks, which assume smooth, continuous relationships.\n",
    "<br>\n",
    "<img src=\"https://raw.githubusercontent.com/stephanie-cal/mids261-final_project/main/p3_images/dow_sincos.png\">\n",
    "\n",
    "This transformed feature is highly important because human and operational behavior is intensely cyclical and correlated with the day of the week. For instance, flight congestion and delays tend to peak on Fridays (end-of-week travel) and Sundays (weekend travel return) and are typically lower mid-week (Tuesdays/Wednesdays). \n",
    "\n",
    "\n",
    "**Previous Flight Delay (minutes)**<br>\n",
    "One of our key lagged features is the previous flight delay in minutes, designed to capture the inevitable propagation of delays across an airline's network. We engineered this feature by tracking the specific aircraft assigned to the scheduled flight and identifying its immediate preceding flight. If that preceding flight's arrival fell outside our standard two-hour cross-validation window, we incorporated its arrival delay (in minutes) as a predictor, quantifying the direct impact of aircraft rotation failures on the subsequent departure.\n",
    "\n",
    "Taking a closer look at previous flight delays compared to the top x carriers, routes and origin airports, we find additional trends between large and small airports, and short and long routes.\n",
    "<br>\n",
    "<img src=\"https://raw.githubusercontent.com/stephanie-cal/mids261-final_project/main/p3_images/prev_delay_vs_carrier_route_origin.png\">\n",
    "\n",
    "The impact of a previous flight's delay on subsequent operations varies based on the structural dynamics of the airline, the routing, and the airport environment. For major carriers, a previous delay is highly predictive of a follow-on delay because they rely on tightly scheduled aircraft and crew rotations in complex hub-and-spoke networks, leaving minimal buffer time for recovery. Regional carrier delays are often externally driven by their major partners, while low-cost carriers may absorb small delays better but risk severe cascading effects due to limited spare resources if the aircraft falls far behind schedule.\n",
    "\n",
    "The airport's characteristics dictate the final severity of the previous delay. At large, congested hubs, the delay is amplified by the sheer volume of competing traffic for limited resources like runways and taxiways, leading to higher subsequent delays. However, the most acute effect is often seen at gate constrained airports where the delayed arrival physically blocks the gate needed for the next departure, creating an unavoidable bottleneck. The degree to which the airport can facilitate quick turnarounds or offer alternative resources determines whether the previous delay remains isolated or triggers a larger system failure.\n",
    "\n",
    "When viewed by route, the preceding delay reveals different propagation mechanisms. High-volume, congested routes between major hubs show a direct, amplified carryover effect, as the delayed aircraft immediately encounters heavy traffic and slot restrictions at both the origin and destination. Short-haul routes flown repeatedly by the same aircraft demonstrate a cumulative effect, where even a minor morning delay can compound throughout the day's successive legs. The operational choreography inherent to the route structure dictates how efficiently the airline can recover the lost time before the next departure.\n",
    "\n",
    "Although the average pervious delay in carrier, origin airport and route are relatively low under 30 minutes, the maximum `pervious delay in minutes` is over 2400 minutes.\n",
    "\n",
    "Previous delay in minutes vs Carrier Table\n",
    "|       |   Avg_Prev_Delay |   Min_Prev_Delay |   Max_Prev_Delay |            Count |\n",
    "|:------|-----------------:|-----------------:|-----------------:|-----------------:|\n",
    "| count |         15       |               15 |           15     |     15           |\n",
    "| mean  |         12.2066  |               -1 |         1409.07  |      1.18911e+06 |\n",
    "| std   |          3.59481 |                0 |          515.867 |      1.21563e+06 |\n",
    "| min   |          4.63428 |               -1 |          705     |  95358           |\n",
    "| 25%   |         10.3788  |               -1 |         1052.5   | 283022           |\n",
    "| 50%   |         12.9756  |               -1 |         1289     | 568743           |\n",
    "| 75%   |         14.1834  |               -1 |         1692.5   |      1.80864e+06 |\n",
    "| max   |         17.605   |               -1 |         2482     |      4.05008e+06 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3135a509-af9c-4de5-aa65-269852fc87ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Machine Learning Pipeline\n",
    "\n",
    "#### Pipeline Overview\n",
    "\n",
    "The figure below illustrates the end-to-end machine learning pipeline used to implement our flight delay prediction system. The design follows Databricks’ modular workflows emphasizing reproducibility, efficiency, and operational realism. Each stage of the pipeline introduces formal checkpoints to ensure data integrity, model traceability, and temporal consistency throughout the experimentation and deployment lifecycle.\n",
    "\n",
    "The data ingestion stage consolidates multiple raw data sources into unified, high-performance Parquet tables. Converting CSVs into Parquet format improves I/O performance, reduces storage overhead, and optimizes large-scale joins required for feature enrichment. The ingestion stage produces a single, time-indexed dataset representing all relevant operational and environmental factors.\n",
    "\n",
    "The Data Engineering & Checkpointing stage prepares the dataset for modeling. The pipeline begins by performing a temporal split into training, validation, and test sets following a 70/10/20 ratio by time. To prevent data leakage and ensure causal validity, a two-hour exclusion gap is introduced between the training and validation windows. This ensures that no information about future flights leaks into the training process. Each major transformation step (splitting, cleaning, feature engineering) is checkpointed, enabling efficient re-runs and consistent intermediate outputs. Engineered features, such as lagged delay metrics, temporal indicators, and weather-derived variables, are stored in a Feature Store, allowing for reusability across models and experiments.\n",
    "\n",
    "The Modeling & Time-Series Cross-Validation stage formalizes model training and evaluation through a rolling window cross-validation scheme. The dataset is divided into five temporal folds, separated by a 2-hour gap. This design closely mirrors real-world airline operations, where predictions must be made two hours prior to departure, and future data cannot be accessed at prediction time. Each fold trains a complete preprocessing and modeling pipeline, applies hyperparameter tuning, and evaluates performance metrics. The final model is retrained on the full training period and stored alongside the fitted preprocessing pipeline to ensure complete reproducibility.\n",
    "\n",
    "The Evaluation, Deployment, and Monitoring stage validates the final model on a held-out test set that represents future, unseen data. Experiment tracking through MLflow records metrics, hyperparameters, and artifacts, enabling transparent model comparison. The best-performing models are registered in the Model Registry and deployed to production where predictions are generated at least two hours before scheduled departure. \n",
    "\n",
    "Monitoring and additional hyperparameter checks were implemented to confirm the lack of performance drift.\n",
    "Overall, this pipeline design emphasizes reproducibility, leakage prevention, and temporal validity. The inclusion of structured checkpoints and feature enhineering ensures that each experiment remains traceable and recoverable, while the rolling validation design provides an empirically sound foundation for model selection and performance reporting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1f8fa8fd-1d37-4e95-aeb4-e7833683ccd3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "<!-- ![261_ML_Pipeline5.png](/Workspace/Users/seowyang@berkeley.edu/261_ML_Pipeline5.png) -->\n",
    "<!-- \n",
    "<img src=\"https://raw.githubusercontent.com/stephanie-cal/mids261-final_project/main/261_ML_Pipeline5.png\"> -->\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/stephanie-cal/mids261-final_project/main/261_ML_Pipeline5.png\" width=\"400\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a36d8b09-d76f-4169-84ef-001418398b64",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Checkpoint Strategy\n",
    "\n",
    "<!-- ![checkpointing.png](/Workspace/Users/seowyang@berkeley.edu/checkpointing.png) -->\n",
    "<img src=\"https://raw.githubusercontent.com/stephanie-cal/mids261-final_project/main/checkpointing.png\">\n",
    "\n",
    "We're converting all CSVs into Parquet format for our data ingestion pipeline, which offers significant advantage in terms of storage efficiency and faster read/writes.\n",
    "\n",
    "For checkpoint strategy: we first split the combined dataset into train, test, and validation sets to prevent any data leakage, establishing this as our initial checkpoint. This step was critical because as it ensures that all subsequent transformations are fitted exclusively on training data, preventing any leakage. We then preprocessed the training data by applying techniques such as imputation for missing values, and standardization for numerical features, checkpointing the cleaned dataset at the stage. Then, we developed our feature engineering pipeline, carefully fitting on training data, and transforing the three splits accordingly, with another checkpoint capturing these fully transformed datasets. Throughout the preprocessing and feature engineering stages, we built and saved the complete training pipeline, encapsulating all transformation steps for reproducibility.\n",
    "\n",
    "We created and checkpointed cross-validation folds, and trained/validated our models, saving both the training and performance resutls. The best performing model was selected, trained with the best performing hyperparamters, and the pipeline was tested on the test data (unseen) to evaluate real-world performance.\n",
    "\n",
    "This systematic checkpointing approach creates clear recovery points throughout our workflow, allowing us to iterate efficiently on different stages without reprocessing from scratch, preventing data leakage, and ensuring reliable model evaluation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "19115e13-3c09-4099-a2db-7c6422122a84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Cross Validation Strategy\n",
    "\n",
    "<!-- ![cross_validaiton.png](/Workspace/Users/seowyang@berkeley.edu/cross_validation.jpeg) -->\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/stephanie-cal/mids261-final_project/main/cross_validation.jpeg\">\n",
    "\n",
    "To evaluate model performance while preventing data leakage, we used a time-series cross-validation approach with a two-hour gap between training and validation periods.\n",
    "\n",
    "Since predictions must be made 2 hours prior to scheduled departure, we implemented a sliding window schema where each fold consists of three components: a training period, a 2-hour gap period, and a validation period. The 2-hour gap ensures that no information from after the prediction time can inadvertently influence model training. This gap is critical for flight delay prediction because delays cascade through airline networks. For example, a flight delayed at 3:00 PM may affect subsequent departures. Without a temporal gap, the model could learn from information that would not be available at prediction time. When predicting a 4:00 PM departure at 2:00 PM, we can only use delay information from flights that departed before 2:00 PM—not the 3:00 PM flight's outcome. The rolling window approach maintains consistent training set sizes across folds, ensuring stable model performance and comparable computational requirements.\n",
    "\n",
    "Our validation strategy uses 5 folds with rolling windows across our datasets spanning 3 months, 1 year, and 5 year, respectively. The table below summarizes the cross-validation parameters for both datasets:\n",
    "| **Parameter** | **3-Month Data** | **1-Year Data** | **5-Year Data** |\n",
    "|--------------|------------------|-----------------|-----------------|\n",
    "| Number of folds | 5 | 5 | 10 |\n",
    "| Training window | 30 days | 4 months | 20 months |\n",
    "| Gap period | 2 hours | 2 hours | 2 hours |\n",
    "| Validation window | 7 days | 1 month | 5 months |\n",
    "| Step size (fold advancement) | ~8 days | ~36 days | ~2.5 months | \n",
    "\n",
    "**Note:** Each fold consists of a training period, followed by a 2-hour gap to prevent data leakage, and then a validation period. Consecutive folds start at intervals defined by the step size, creating substantial overlap that ensures comprehensive temporal coverage while maintaining prediction integrity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "60e875f0-e16d-4df0-ad25-c709f7fb015f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Data Leakage \n",
    "Data leakage occurs when a model uses information during training that would not be available during the time of prediction. Since we are making our predictions 2 hours before the time of departure, we need to make sure we are not using any information that would not be available at the time of prediction, like the actual departure or arrival time, or if it actually rains tomorrow. Leakage may lead to unrealistically good performance during testing, but poor results in production.\n",
    "\n",
    "The integrity of our flight delay prediction model hinges on a rigorous data leakage prevention strategy, ensuring our model is evaluated under operationally realistic constraints. Our feature set is strictly defined by information available two hours prior to scheduled departure (T- 2 hours). For flight data, we exclusively rely on scheduled and historical attributes, deliberately omitting any post-departure metrics (e.g., actual taxi times, final delay values) which are the target or derived consequences. Similarly, our weather data combines the latest observed conditions at T- 2 hours with the most current forecast products for the departure and arrival times, strictly prohibiting the use of observed weather that occurs after the prediction time window.\n",
    "\n",
    "\n",
    "\n",
    "#### Data Split and Feature Engineering Considerations\n",
    "Mitigation of statistical leakage is achieved by enforcing the \"Split First, Preprocess Later\" protocol. This mandates that all feature transformations (scaling, normalization, imputation) are fitted solely on the training data within each cross-validation fold, and these fitted parameters are then applied to the validation and test sets. \n",
    "\n",
    "To address temporal leakage inherent to time-series cross-validation, especially concerning lagged features and rolling statistics, we utilize a defined temporal look-back window. This window guarantees that the calculation for any feature in the validation set is restricted to historical data that precedes the exact prediction moment (T- 2 hours), thereby preventing look-ahead bias from the validation set's future outcomes or the target variable's status during the critical 2-hour gap . Furthermore, we mitigate feature selection leakage by embedding the feature selection process within the cross-validation loop, ensuring the importance metrics used to select features are derived only from the in-fold training data. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "67edbe97-bf88-4418-929e-789a59981732",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Feature Selection \n",
    "After preparing the train, test and validation splits, we moved to feature selection. The custom join handled many of the features with more than 20% nulls in the combined join, and ignoring cancelled flights handled most of the remaining nulls. Our target variable is `DEP_DELAY` during regression describing the difference between actual departure and the scheduled departure time in minutes, and `DEP_DEL15` during classification, a binary value saying if the flight was delayed or not. Since we will need to make this prediction 2 hours before the scheduled departure, we implemented strict feature filtering to prevent data leakage. This involved excluding any columns containing data that would be unavailable at the time of prediction. So from our final joined dataset with 121 possible features, these filters helped to narrow down our actually usable fields.\n",
    "\n",
    "Because most delays are due to weather, maintenance, or late-arriving aircraft, we relied on the provided weather data and constructed lagged or derived features to model operational constraints. These engineered features specifically capture the potential cascade effect of maintenance problems or disruptions caused by late inbound flights.\n",
    "\n",
    "Our feature selection methodology deliberately favored an exhaustive, high-dimensional inclusion to address the non-linear dynamics inherent in air traffic flow. In complex systems like flight prediction, the interaction effects between seemingly minor variables like specific gate availability, down-route weather, previous flight delay, are often highly predictive. Since the precise combination of relevant factors can shift significantly for external or future datasets, our broad feature set ensures the model has the necessary explanatory redundancy and degrees of freedom to capture emergent patterns. This approach is key to building a reliable multitower neural network architecture that can generalize across the diverse operational environments it will ultimately encounter.\n",
    "\n",
    "\n",
    "#### Phase 1 Baseline\n",
    "For the baseline model, we used a total of 34 input features spanning flight metadata, weather conditions, immediate system status, and aggregate engineered history.\n",
    "\n",
    "| Feature Family | Number of Input Features |\n",
    "| :--- | :--- |\n",
    "| Categorical | 4 |\n",
    "| Numerical | 8 |\n",
    "| Temporal | 11 |\n",
    "| Weather Conditions | 11 |\n",
    "\n",
    "In our baseline model, the XGB regressor improved by nearly 10% when adding weather condition features, and an additional 29% when adding temporal features when comparing the validation mean absolute error for 2015. The experimentation for the baseline model is detailed in the next section.\n",
    "\n",
    "Baseline Features used:\n",
    "\n",
    "| Group | Columns |\n",
    "|---|---|\n",
    "| categorical_cols | `OP_CARRIER`, `ORIGIN_AIRPORT_SEQ_ID`, `DEST_AIRPORT_SEQ_ID`, `route` |\n",
    "| numerical_cols | `QUARTER`, `MONTH`, `YEAR`, `DAY_OF_MONTH`, `DAY_OF_WEEK`, `CRS_DEP_MINUTES`, `CRS_ELAPSED_TIME`, `DISTANCE` |\n",
    "| temporal_cols | `utc_timestamp`, `prev_flight_delay_in_minutes`, `prev_flight_delay`, `origin_delays_4h`, `delay_origin_7d`, `delay_origin_carrier_7d`, `delay_route_7d`, `flight_count_24h`, `LANDING_TIME_DIFF_MINUTES`, `AVG_ARR_DELAY_ORIGIN`, `AVG_TAXI_OUT_ORIGIN` |\n",
    "| weather_cols | `HourlyDryBulbTemperature`, `HourlyDewPointTemperature`, `HourlyRelativeHumidity`, `HourlyAltimeterSetting`, `HourlyVisibility`, `HourlyStationPressure`, `HourlyWetBulbTemperature`, `HourlyPrecipitation`, `HourlyCloudCoverage`, `HourlyCloudElevation`, `HourlyWindSpeed` |\n",
    "\n",
    "\n",
    "#### Phase 3 Models\n",
    "In our phase 3 models, we used a total of 67 features.\n",
    "\n",
    "| Feature Family | Number of Input Features |\n",
    "| :--- | :--- |\n",
    "| Categorical | 8 |\n",
    "| Numerical | 5 |\n",
    "| Temporal | 28 |\n",
    "| Graph | 9 |\n",
    "| Weather | 16 |\n",
    "\n",
    "The addition of graph features to the baseline XGB regressor further improved performance by an additional 6%. Since we fouond temporal features were the most effective when looking at the baseline regressor, and we know a majority of delays occur on the ground, we continued to develop temporal features.\n",
    "\n",
    "Final features used:\n",
    "| Category | Total | Columns |\n",
    "|---|---:|---|\n",
    "| graph | 9 | `page_rank`, `out_degree`, `in_degree`, `weighted_out_degree`, `weighted_in_degree`, `betweenness_unweighted`, `closeness`, `betweenness`, `N_RUNWAYS` |\n",
    "| numerical | 5 | `avg_origin_dep_delay`, `avg_dest_arr_delay`, `avg_daily_route_flights`, `avg_route_delay`, `avg_hourly_flights` |\n",
    "| temporal | 28 | `QUARTER`, `MONTH`, `DAY_OF_MONTH`, `DAY_OF_WEEK`, `FL_DATE`, `utc_timestamp`, `CRS_DEP_MINUTES`, `origin_delays_4h`, `prev_flight_delay_in_minutes`, `prev_flight_delay`, `delay_origin_7d`, `delay_origin_carrier_7d`, `delay_route_7d`, `flight_count_24h`, `LANDING_TIME_DIFF_MINUTES`, `AVG_ARR_DELAY_ORIGIN`, `AVG_TAXI_OUT_ORIGIN`, `IS_HOLIDAY`, `IS_HOLIDAY_WINDOW`, `year`, `dep_hour`, `day_of_year`, `dep_hour_sin`, `dep_hour_cos`, `dow_sin`, `dow_cos`, `doy_sin`, `doy_cos` |\n",
    "| categorical | 8 | `flight_uid`, `OP_UNIQUE_CARRIER`, `OP_CARRIER_AIRLINE_ID`, `OP_CARRIER`, `TAIL_NUM`, `route`, `AIRPORT_HUB_CLASS`, `AIRLINE_CATEGORY`, `RATING` |\n",
    "| weather | 16 | `HourlyDryBulbTemperature`, `HourlyDewPointTemperature`, `HourlyRelativeHumidity`, `HourlyAltimeterSetting`, `HourlyVisibility`, `HourlyStationPressure`, `HourlyWetBulbTemperature`, `HourlyPrecipitation`, `HourlyCloudCoverage`, `HourlyCloudElevation`, `HourlyWindSpeed`, `HourlyVisibility_3h_change`, `HourlyStationPressure_3h_change`, `HourlyDryBulbTemperature_3h_change`, `HourlyWindSpeed_3h_change`, `HourlyPrecipitation_3h_change`, `ground_flights_last_hour`, `arrivals_last_hour`, `utc_ts_sec` |\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cf8ef224-db0d-4277-af33-462802f129a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Machine Learning Algorithms and Metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eec37c78-d405-4e8f-9127-178b30ec9919",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Phase 1: Baseline Model Experimentation\n",
    "In order to select the a basline model to tune, we tested a few algorithms and documented their results in MLFlow. \n",
    "\n",
    "The cluster used was:\n",
    "- Runtime: `16.4ML, Spark 3.5.2, Scala 2.12`\n",
    "- Driver: `m5d.xlarge, 16GB, 4 cores`\n",
    "- Workers (2-8): `m5d.xlarge, 32-128GB, 8-32 cores`\n",
    "\n",
    "The runtimes for each model with all 33 input features:\n",
    "- Random Forest: 15.9 minutes\n",
    "- XGBoost: 9.7 minutes\n",
    "\n",
    "In total, we ran 6 different experiments on the 1 year dataset with different regression algorithms and number of input features to establish our baseline model. The first set of features was only the flight metadata, the second set was the flight metadata and weather conditions, and the third was the flight metadata, weather conditions and engineered features.\n",
    "\n",
    "Experimental Results for Random Forest (1 year)\n",
    "\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/stephanie-cal/mids261-final_project/main/ph2_rf_experiments.png\">\n",
    "\n",
    "Experimental Results for XGBoost (1 year)\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/stephanie-cal/mids261-final_project/main/phase2_xgb_experiments.png\">\n",
    "\n",
    "\n",
    "Metrics for performance tracking used were:\n",
    "\n",
    "Mean absolute error:\n",
    "$$\n",
    "\\text{MAE} = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i|\n",
    "$$\n",
    "\n",
    "Root Mean Squared Error:\n",
    "$$\\text{RMSE} = \\sqrt{\\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\hat{y}_i)^2}$$\n",
    "\n",
    "The iterative evaluation of our modeling pipeline demonstrates that the incorporation of hourly weather conditions and domain-specific engineered features is critical for achieving significant improvements in predictive metrics. This success validates our core strategy, the final model must synthesize information from three distinct domains, moving beyond simple static scheduling to accurately capture the dynamic operational reality of the air travel system. \n",
    "\n",
    "The inclusion of weather conditions and the immediate system status features, such as the turnaround time gap and the aircraft's flight sequence progress, allows the model to shift from baseline prediction to a condition-based risk assessment, effectively capture the potential for cascading delays.The rolling aggregates act as crucial proxies for system congestion and momentum, providing the model with a robust, low-noise signal of underlying systemic risks. \n",
    "\n",
    "As these features collectively prove essential in reducing unexplained variance and maximizing predictive fidelity, we will proceed with the full 33-feature set for all subsequent model development and hyperparameter tuning efforts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7065984d-a8f4-4d8b-bc35-8204d303e79a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Phase 2: Base Architecture Selection\n",
    "\n",
    "During our EDA, we identified the noisy nature of our data, with skewed distributions and lots of missing data. To address this, we built upon our initial linear regression model to include ensemble-based methods such as XGBoost and Random Forests so that our model topology could better adapt to our training set. Below is a summary of the model components used in Phase 2 (and later on in Phase 3) as well as their loss functions\n",
    "\n",
    "**Regression Models (Tier 1)**\n",
    "- Implementation: `Linear Regression`, `RandomForest`, `XGBoost`\n",
    "- Loss Function: Mean Absolute Error (MAE)\n",
    "$$\n",
    "\\text{MAE} = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i|\n",
    "$$\n",
    "\n",
    "**Classification Models**\n",
    "- Implementation: `Random Forest`, `XGBoost`, `MLPClassifer (Stage 3)`\n",
    "-  Loss Function: Multi-Class Cross Entropy\n",
    "\n",
    "$$\n",
    "\\text{Cross-Entropy Loss} = -\\sum_{i=1}^{N} \\sum_{c=1}^{C} y_{i,c} \\log(p_{i,c})\n",
    "$$\n",
    " \n",
    "\n",
    "## Phase 3: Advanced Architectures\n",
    "\n",
    "**Stacked XGBRegressor and MLP Classifier Model**\n",
    "\n",
    "After constucting our baseline classification model, our next step was to try a more complex architecture to see if it would improve performance. Our client asked specifically to incorporate a neural net model, so we experimented with a multi-layer perceptron model. We kept the baseline XGBoost regressor to predict flight delay time in minutes, and then added that prediction as a feature along with our original features to feed into an MLP model for the final classification. We played around with a simple MLP classifier model that didn't take the XGBoost output as well, but performance was always the same or worse than with the stacked architecture, so we abandoned single MLP experiments after validating on the 3-month dataset. This stacked XGB regressor/MLP classifier architecture would lay the foundation for the next iteration of our model, the multi-task neural tower model. Wall times averaged 13.6 minutes on the 5-year dataset for training and inference.\n",
    "\n",
    "**Multi-Task Neural Tower Architecture**\n",
    "\n",
    "Our next model architecture employed a custom **Multi-Task Deep Learning Architecture** tailored for tabular flight data, centered on a **ResFiLMMLP** (Residual Network with Feature-wise Linear Modulation) backbone.\n",
    "\n",
    "The architecture processes inputs through three specialized streams:\n",
    "1.  **Categorical Embeddings:** High-cardinality categorical features (e.g., `OP_UNIQUE_CARRIER`, `route`) are passed through learnable embedding layers.\n",
    "2.  **Numerical Processing:** Continuous features are fed into a tower of **Residual Blocks** (Linear --> GELU --> Dropout) to capture non-linear interactions.\n",
    "3.  **Temporal Encoding:** We utilize a **Time2Vec** layer to encode `CRS_DEP_MINUTES`. Unlike standard scalar normalization, Time2Vec learns periodic, sinusoidal representations, effectively capturing cyclical delay patterns (e.g., morning vs. evening congestion).\n",
    "\n",
    "A key innovation is the use of **FiLM (Feature-wise Linear Modulation)**. The processed numerical representations generate scale (γ) and shift (β) parameters that modulate the categorical embeddings. This allows the model to dynamically adjust the importance of carrier or route characteristics based on numerical conditions like weather or congestion. The network concludes with two parallel heads: a **Regression Head** (predicting delay minutes) and a **Classification Head** (predicting probability of delay as defined as being more than 15 mins).\n",
    "\n",
    "_Figure 1: Neural Tower Architecture Graph_<br>\n",
    "<img src=\"https://raw.githubusercontent.com/stephanie-cal/mids261-final_project/main/p3_images/nn_tower_arch.png\">\n",
    "\n",
    "**Wall Times (5-Year Dataset)**\n",
    "\n",
    "* **Tuning:** This took approximately 8.5 hours.\n",
    "* **Final Training:** The final production run on the full 5-year training set using the optimized hyperparameters took 3.43 hours, processing batches of 4096 efficiently on the cluster.\n",
    "* **Inference:** The model demonstrates high efficiency. Final inference on the **Validation Set (4.4M rows)** took **95.92 seconds**, and the **Test Set (6.4M rows)** took **142.73 seconds**.\n",
    "\n",
    "**2-Stage XGB Quantile Regressor and Classifier (Daniel)**\n",
    "\n",
    "Towards creating a model aligned with our downstream binary classification task, we thought of creating class labels of on-time (< 15min delay) and delayed (>= 15min delay) and using binary cross-entropy to optimize for class separability rather than residual regression error. However, binning flights with continuous delay features into 2 broad classes condenses a significant amount of signal from the data, and forfeits useful information that could better calibrate our model. To reconcile the continuous response variable with our discrete classification goal, we next used a 2-stage XGBoost quantile regression and classification model.\n",
    "\n",
    "At a high level, the first stage of our model leverages the continuous delay signal to identify easy-to-classify examples and automatically classify them. Ambiguous data points are then sent to a separate classifier that can fine-tune the decision boundary for these hard-to-classify examples, without allowing the easier examples to dominate the loss. Below is a more detailed look at this model architecture.\n",
    "\n",
    "In the first stage, we fit both an upper and lower quantile XGBoost regression model to construct a coarse conditional delay distribution for our input features. Any data points whose upper quantile estimate lies below our 15 minute delay threshold are automatically classified as on-time, while those whose lower quantile estimate is above 15 are automatically classified as delayed. \n",
    "\n",
    "In the second stage, we create binary labels for each remaining example and add the upper and lower quantile estimates as features to be leveraged in the final prediction. Since our training set is both large and imbalanced, we undersample the majority class before passing to the final classifier to balance learning between the two classes. Note that >10% of examples advanced to the second stage in all experiments, guaranteeing large enough training data for undersampling even after the quantile regression filtering in stage 1.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/stephanie-cal/mids261-final_project/main/p3_images/2stagemodel.png\">\n",
    "\n",
    "Wall time for training and inference on the train/val splits for the 5-year dataset averaged 20.4 minutes.\n",
    "\n",
    "\n",
    "Below are the cluster specifications for all experiment wall-times in Phase 3:\n",
    "- Runtime: `16.4 LTS, Spark 3.5.2, Scala 2.12`\n",
    "- Driver: `m5d.2xlarge, 32GB, 8 cores`\n",
    "- Workers: `6-12`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "32438866-3969-45e4-baa9-f2faa1b6301c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Metrics\n",
    "\n",
    "##### Prediction Consequences\n",
    "\n",
    "In order to deliver a model that best suits the needs of air traffic controllers, we need to optimize for metrics that account for both the costs and benefits of our predictions, as well as the class balance of our dataset.\n",
    "\n",
    "For example, consider the following outcomes of our final model:\n",
    "\n",
    "- **True Positive**: We correctly predict that a flight will be delayed. This could allow air traffic controllers to re-allocate gates and runways, potentially saving several minutes for dozens of flights - a huge net gain especially during congestion.\n",
    "- **True Negative**: We correctly predict that a flight will be on time. This will help air traffic controllers to focus on other flights more likely to need their attention.\n",
    "- **False Positive**: We incorrectly predict that a flight will be delayed. This could cause several planes to be re-routed unnecessarily and cause the flight in question to circle the skies waiting to land and waiting to park at their previously allocated gate. This mistake could snowball to downstream flights and occupy stand-by space that other genuinely delayed flights may need.\n",
    "- **False Negative**: We incorreclty predict that a flight will be on time. This will cause collisions with subsequent flights, and require last-minute re-allocation of gates/runway space. These collisions can easily pile up throughout the day, and can leave both air traffic controllers and passengers blindsided.\n",
    "\n",
    "##### Class Balance\n",
    "\n",
    "Additionally, it is important that our metrics account for the fact only around 18.29% of flights are delayed by 15 minutes or more, leading to significant class imbalance*. We don't want our validation metrics to lead us to chose a model that fails to learn the important minority class.\n",
    "\n",
    "<!-- ![response_dist.png](/Workspace/Users/seowyang@berkeley.edu/response_dist.png) -->\n",
    "\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"https://raw.githubusercontent.com/stephanie-cal/mids261-final_project/main/response_dist.png\">\n",
    "</div>\n",
    "\n",
    "_*For the multi-tower neural network model, to handle class imbalance, where delays are much rarer than on-time flights, the tower architecture uses a weighted loss function for the classification head. Specifically, it applies a `pos_weight` of 4.0 in the BCE with logits loss, effectively telling the model that misclassifying a rare \"delay\" instance is more costly than misclassifying a common \"on-time\" instance._\n",
    "\n",
    "##### North Star Metric\n",
    "\n",
    "Between class imbalance and prediction consequences, the former will inform our choice of validation metric for model selection, while the latter will help us choose a classification threshold that leverages our model's predictive power while also optimizing for airport safety KPIs. To this end, a few of validation metrics are disqualified:\n",
    "\n",
    "- Accuracy: This metric is notoriously sensitive to class imbalance, and could lead us to optimize for a model that fails to learn the minority class at all by simply always predicting the majority class.\n",
    "- ROC-AUC: Similarly, this metric does not handle class imbalance well. Integrating under a curve plotting true-positive rate (TP / (TP + FN)) against false-positive rate (FP / (FP + TN)). If the negative class is large and a model largely favors classifying records as negative, the FPR will have a much larger denominator and comparable numerator, causing X to tend smaller than Y - inflating the ROC-AUC integral of this faulty model and hurting downstream KPIs.\n",
    "\n",
    "\n",
    "Between false positives and false negatives, we believe that false negatives are the more harmful. Failing to prepare for delays causes collisions that result in congestion - a nightmare scenario that will leave air traffic controllers scrambling. For this reason, we favor recall above precision. Still, precision is desirable as predicting too many delays will cause controllers to free up unnecessary space, stalling operations across the airspace. To strike a proper balance between precision and recall, we optimize our delay classifier using F2 as our north-star metric.\n",
    "\n",
    "While using PR-AUC as a validation metric would help us select a model that would address class balance while having optimal average recall across thresholds, this approach may not help us find the best singular threshold for our classifier to optimize F2. Thus, we use threshold-tuned F2 as our validation metric, helping us to select a model that will give us the best balance between precision and recall, addressing class balance, **and** finding a suitable threshold. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8045b093-299d-4956-b352-c96f5fe5b065",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Hyperparameter Tuning\n",
    "\n",
    "**Baseline Hyperparameter Tuning:** We performed hyperparameter tuning on our baseline model, the XGBoost Regressor, using cross-validation with the time-series splitting scheme described above. We selected these hyperparameters to balance model complexity and generalization: `max_depth` [4, 6] controls individual tree complexity, `n_estimators` [20, 50, 100] determines ensemble size, and `learning_rate` [0.05, 0.1] provides step size shrinkage to prevent overfitting. These ranges represent commonly effective values for XGBoost while remaining computationally feasible for our 5-fold cross-validation approach.\n",
    "We validated each combination using our 5-fold cross-validation approach with 2-hour gaps, resulting in 12 total parameter combinations. The results are shown in the table below.\n",
    "\n",
    "\n",
    "Based on our findings, we selected the XGBoost Regressor configuration from row 1 (max_depth=6, n_estimators=100, learning_rate=0.05), achieving validation MAE of 17.565 with standard deviation of 0.823.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/stephanie-cal/mids261-final_project/main/p3_images/CV_RESULTS_1.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a4109011-29ce-459f-9755-5bf4ad9f6af9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Temporal Validation: Testing 1-Year XGB Regressor Across Multiple Years**\n",
    "\n",
    "The same XGB regressor with (max_depth=6, n_estimators=100, learning_rate=0.05) was used to produce the delay in minutes for the 5-year dataset, spanning from 2015-2019. The reasoning is two fold:\n",
    "\n",
    "1. Using these parameters allowed us to leverage the XGB regressor optimized on the 2015 data (1-year training set).\n",
    "2. To assess temporal stability and potential data/model drift, we tested the model on the last 20% of each subsequent year (2016, 2017, 2018, 2019). Our results show that our model is very robust, as demonstrated by an MAE range of only 2.26 minutes (11.97 to 14.23) and a standard deviation of 1.02 minutes. This consistency indicates that the model generalizes well across different years with minimal performance degradation.\n",
    "\n",
    "| Year | Test_MAE |\n",
    "|------|----------|\n",
    "| 2015 | 14.23    |\n",
    "| 2016 | 12.43    |\n",
    "| 2017 | 11.97    |\n",
    "| 2018 | 13.37    |\n",
    "| 2019 | 14.18    |\n",
    "\n",
    "**Mean MAE:** 13.24  \n",
    "**Std Dev:** 1.02  \n",
    "**Range:** 2.26\n",
    "\n",
    "**Implications for Stacked Model Architecture**: The temporal stability demonstrated above justified our decision to use these same hyperparameters (max_depth=6, n_estimators=100, learning_rate=0.05) when training on the expanded 5-year dataset (2015-2019). Since the model maintained consistent performance across different years with minimal degradation, we could confidently apply these hyperparameters to the larger temporal window without additional tuning.\n",
    "\n",
    "For the stacked model pipeline (details below), we retrained the XGB regressor using these validated hyperparameters on the full 5-year dataset. The predictions from this 5-year XGB model then served as input features for our downstream MLP classifier.\n",
    "This approach ensured that:\n",
    "\n",
    "- The hyperparameters were thoroughly validated across multiple years before scaling to the full dataset\n",
    "- The MLP classifier received predictions from an XGB model trained on the same temporal range (2015-2019)\n",
    "\n",
    "This validation was critical before proceeding to the stacked-model hyperparameter tuning phase, as the quality of the XGB predictions directly impacts the downstream classication task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "015f744f-ec5b-48a8-984f-1e8c53f6dad8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**XGBoost Stacked Models: CV Strategy & Hyperparameter Tuning**\n",
    "\n",
    "We had to change our CV strategy when we began working with the stacked models and generating predictions to be used in later models. To prevent data leakage, we had to predict delay time from the XGBoost model using held out data from the out-of-fold datasets; this made sure the model did not see the answer when making it's predictions, so when the time came for the MLP model to use that prediction as an input feature, it would not know the actual delay time. All in all, this was only required for generating the output predictions, but training the XGBoost only took 536 seconds.\n",
    "\n",
    "Once we got to the MLP classification model, we now had to clean our data before passing it in to the neural network. We did not have to scale our features for the XGBoost regressor, but we did for all of our neural net models. We also had to downsample our data to ensure the dataset was balanced and prevent the model from simply predicting the majority class.\n",
    "\n",
    "The majority of our hyperparameter tuning for these models happened within the confines of the hidden layers on the MLP model. We tested 4 different hidden layer combinations on our cross-validation data: [32], [64, 32], [128, 64], and [64, 32, 16]. Validation F2 score was higher for wider hidden layers, so [128, 64] performed the best in our testing. Once we had identified this, training the MLP model only took a bit longer than the XGBoost model at 816 seconds. We did also have to tune the classification decision threshold, and for this first simpler MLP architecture, a threshold of 0.45 ended up maximizing F2.\n",
    "\n",
    "**Neural Net Tower: CV Strategy & Hyperparameter Tuning)**\n",
    "\n",
    "The multi-tiered neural network architecture required it's own hyperparamter tuning technique. We adopted a **hybrid evaluation strategy** to balance computational efficiency with robustness:\n",
    "* **Hyperparameter Tuning (Subset CV):** To expedite tuning on the massive 5-year dataset, we performed Cross-Validation on a **representative subset of 3 folds** (First, Middle, and Last chronological folds) rather than the full set. This provided a reliable signal for hyperparameter performance without the cost of a full 10-fold run.\n",
    "\n",
    "We utilized **Optuna** for automated hyperparameter optimization, running **8 trials** to explore the search space. The objective function maximized the **F2 Score** and **MAE** on the validation folds.\n",
    "\n",
    "Key tuned parameters included:\n",
    "* **Learning Rate:** Log-uniform search between 1×10⁻⁴ and 5×10⁻³\n",
    "* **Architecture Dims:** Tuning `time_dim` (4, 8, 16) and `batch_size` (1024, 2048, 4096).\n",
    "* **Regularization:** Distinct dropout rates were tuned for embeddings (`emb_drop`), numerical blocks (`num_drop`), and the final heads (`final_drop`) to prevent overfitting.\n",
    "* **Loss Balancing:** The `alpha` parameter was tuned (range 0.3–0.7) to balance the gradients between Regression (L1) and Classification (BCE) losses.\n",
    "\n",
    "The best hyperparameter configuration found was **Trial 3**, which achieved the highest Validation F2 score.\n",
    "\n",
    "_Best Hyperparameters (from Trial 3)_\n",
    "\n",
    "* **Learning Rate (`lr`):** `0.000156` \n",
    "* **Batch Size:** `4096`\n",
    "* **Alpha (Loss Weight):** `0.342`\n",
    "    * *Note: This means the loss function is weighted approx. 34% on Regression (L1) and 66% on Classification (BCE).*\n",
    "* **Time Dimension (`time_dim`):** `16`\n",
    "    * *Note: The Time2Vec layer uses 16 learned sinusoidal components.*\n",
    "* **Embedding Dropout (`emb_drop`):** `0.046`\n",
    "* **Numerical Dropout (`num_drop`):** `0.324`\n",
    "* **Final Dropout (`final_drop`):** `0.100`\n",
    "\n",
    "This configuration indicates that a larger batch size, a relatively low learning rate, and higher dropout on the numerical tower (vs. the embedding tower) were optimal.\n",
    "\n",
    "_Figure 2: Neural Network Tower: Hyperparameter Experimental Results based on Optuna hyperparameter tuning experiment_\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/stephanie-cal/mids261-final_project/main/p3_images/hyperparameter_tuning_nn_tower.png\">\n",
    "\n",
    "**2-Stage XGB Quantile Regressor + Classifier CV Strategy and Hyperparamter Tuning**\n",
    "\n",
    "We also used the hybrid evaluation strategy described above for the 2-Stage XGB Quantile Regressor + Classifier model. Beyond the XGB hyperparameter grid search detailed in **Baseline Hyperparameter Tuning**, the 2-stage model was tuned altogether with the following additional hyperparameters:\n",
    "\n",
    "- `quantile_low` : Quantile percentage used to define lower condiational delay distribution band. Values: `0` to `0.7` inclusive, with `0.05` increment. Best Value: `0.5`\n",
    "- `quantile_high` : Quantile percentage used to define upper conditional delay distribution band. Values: `0.7` to `1.0` inclusive, with `0.02` increment. Best Value: `0.98`\n",
    "- `min_child_weight`: Minimum number of examples for a leaf of the 2nd stage XGBClassifier. Values: `[1, 10, 20, 50]`. Best Value: `20`\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b464434b-c1f1-4770-b279-09e2ee1ea679",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Results\n",
    "\n",
    "### Phase 2: Selecting Base Regression Architecture (Non-Neural Net)\n",
    "\n",
    "#### Overview of Experimental Setup\n",
    "We partitioned the 3-month and 1-year (2015) datasets into a training/validation period (Jan-Sep) and a blind test period (Oct-Dec). Phase 2 focused on comparing regression models for predicting continuous flight delays, while Phase 3 will expand on the analysis to classification performance using thresholding at 15 minutes and evaluate the role of the two-stage pipeline.\n",
    "Towards implementing a quantile regression to identify easy/hard flights to classify during Phase 3, we based model selection in Phase 2 on validation MAE.\n",
    "\n",
    "#### Model Loss Comparison\n",
    "\n",
    "Several regression model families were evaluated during Phase 2, including Linear Regression, Random Forests, and XGBoost. The below table summarizes the best 5-fold cross-validated MAE results (in minutes delay) from each of our models on both the 3-month and 1-year datasets.\n",
    "\n",
    "| Dataset | 3 months | 1 year |\n",
    "|---------|----------|--------|\n",
    "| **Model** | **Train / Val MAE** | **Train / Val MAE** |\n",
    "| Linear Regression | 16.006 / 13.475 | 18.852 / 15.839 |\n",
    "| Random Forest Regressor | 13.860 / 15.79 | 15.382 / 17.565 |\n",
    "| **XGBoost Regressor** | 14.281 / **12.974** | 16.354 / **13.284** |\n",
    "\n",
    "Across models, XGBoost provided the best balance of predictive accuracy and training stability, achieving the lowest validation MAE on both datasets. This indicates meaningful non-linear interactions among our flight delay features.\n",
    "\n",
    "#### Cross-Validation\n",
    "\n",
    "\n",
    "During K-fold cross-validation on the training set, the selected XGBoost regressor with:\n",
    "\n",
    "- `max_depth = 6`  \n",
    "- `n_estimators = 100`  \n",
    "- `learning_rate = 0.05`  \n",
    "\n",
    "achieved:\n",
    "\n",
    "- **avg validation MAE = 13.284**  \n",
    "- **avg training MAE = 16.354**  \n",
    "- **std of validation MAE = 0.823**\n",
    "\n",
    "indicating moderate fold-to-fold consistency. These results suggest that the model captured sufficient complexity without severe overfitting, making it the clear choice as the Stage-1 predictor in our two-stage system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f5011dca-21d4-4398-8a02-9269d20778de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Blind Test Performance (Final Quarter of the Dataset)\n",
    "\n",
    "On the held-out blind test set representing the final quarter of the year, the selected model achieved: **Test MAE = 14.32**\n",
    "\n",
    "This constitutes an improvement relative to both the training and validation MAE, indicating strong generalization and suggesting that the distributions in the test quarter are not substantially more volatile than those in the training period. The low MAE also demonstrates that the model captures the central tendency of flight delays, which will be essential for the quantile-based decision-making in Phase 3.\n",
    "\n",
    "| | Dataset | MAE | RMSE |\n",
    "|---|---|---|---|\n",
    "| **0** | Train | 15.621 | 41.894 |\n",
    "| **1** | Test | 14.320 | 40.577 |\n",
    "| **2** | Validation | 12.451 | 34.694 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "30244459-d85a-447c-815a-e37f2c341041",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Regressor-as-Classifier Baseline\n",
    "To establish a baseline against which we will compare Phase 3's two-stage pipeline, we evaluated the classification performance of the regression model by thresholding predicted delay at 15 minutes. This aligns with our downstream task of classifying all flights as either \"On-Time\" or \"Delayed.\"\n",
    "\n",
    "Because failing to identify a meaningful delay (false negative) is more costly than incorrectly predicting one (false positive), we use **F₂ score** as our north-star metric.\n",
    "\n",
    "On the blind test set:\n",
    "\n",
    "- **Precision = 0.4533**  \n",
    "- **Recall = 0.5645**  \n",
    "- **F₂ = 0.5374**\n",
    "\n",
    "The model exhibits moderate recall, consistent with expectations for MAE-optimized quantile regression, which tends to reduce underestimation errors. Precision is lower, highlighting the trade-off inherent to regression-based classifiers. Phase 3 will introduce our second-stage classifier to address this limitation.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d40fd4fc-2aa2-4894-9666-2820fa687cfb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Phase 3\n",
    "#### Overview of Experimental Setup\n",
    "We partitioned the 5-year dataset into a training/validation period (2015-2018 inclusive) and a blind test period (2019). To optimize our binary classifier for F2, we used a threshold-tuned F2 score as our validation metric.\n",
    "\n",
    "#### Model Loss Comparison\n",
    "\n",
    "We evaluated 3 additional model architectures during Phase 3: XGBoost Regressor + MLP Classifier, Two-Tower Neural Network, and 2-Stage Quantile XGBoost Regressor + Classifier. The below table summarizes the best 5-fold cross-validated F2 results  from each of our models on the 5-year dataset.\n",
    "\n",
    "| Model | Train F2 | Test F2 |\n",
    "|---------|----------|--------|\n",
    "| XGBoost Regressor + MLP Classifier | 0.533 | 0.539 |\n",
    "| Multi-Tower Neural Network | 0.621 | 0.619 |\n",
    "| **2-Stage Quantile XGBoost Regressor + Classifier** | **0.622** | **0.621** |\n",
    "\n",
    "Across models, the 2-Stage Quantile XBGoost Regressor + Classifier demonstrated the highest threshold-tuned F2 score. All models learned patterns in training data that generalized well, with test F2 at most 0.32% less than that of train F2. This presents a significant improvement over the XGBoost Regressor + naive classification head baseline of 0.537, and demonstrates the usefulness of this 2-stage approach.\n",
    "\n",
    "Also of note, the quantile regression caught 5.24% of all truly delayed flights in the test set with an F2 of 0.93. The discussion will delve into the consequences of this observation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fb0dd53f-7fc3-444f-bba1-a516d42f3b21",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Feature Importance\n",
    "To judge the predictive power of our features, we used the mutual importance score to measure the dependency between each feature and the target variable. This score quantifies the reduction in uncertainty of target variable Y that results from knowing feature X. \n",
    "\n",
    "The formula for Mutual Information between **X** and **Y** is:\n",
    "$$MI(X, Y) = H(X) + H(Y) - H(X, Y)$$\n",
    "where **H(X)** and **H(Y)** are the individual entropies (uncertainty) of **X** and **Y**, and **H(X, Y)** is the joint entropy of **X** and **Y**.\n",
    "We chose to use MI because of its ability to handle non-linear relationships, an advantage over linear correlation methods. It can detect complex, non-monotonic dependencies like when a feature's impact changes direction ensuring that features with subtle but strong predictive power are not overlooked. For example, if a delay risk is high at very low and very high temperatures. The MI calculation methods can effectively handle diverse data types, including continuous and categorical variables, without requiring manual discretization, preserving the fidelity of the original data.  Since MI does not account for feature-to-feature dependencies, it frequently assigns high scores to redundant features, suggesting that multiple, highly correlated variables are all individually important.\n",
    "\n",
    "**Mutual importance for top 15 training features - train set (2015-2018)**\n",
    "| \\# | Feature | MI Score | Feature Family |\n",
    "| :--- | :--- | :--- |:--- |\n",
    "| 1 | prev\\_flight\\_delay\\_in\\_minutes | 0.1134 | Temporal \n",
    "| 2 | route\\_indexed | 0.0877 | Categorical\n",
    "| 3 | avg\\_route\\_delay | 0.0873 | Temporal\n",
    "| 4 | RATING | 0.0843 | Numerical\n",
    "| 5 | OP\\_CARRIER\\_AIRLINE\\_ID | 0.0832 | Categorical\n",
    "| 6 | TAIL\\_NUM\\_indexed | 0.0761 | Categorical\n",
    "| 7 | avg\\_daily\\_route\\_flights | 0.0743 | Temporal\n",
    "| 8 | prev\\_flight\\_delay | 0.0688 | Temporal\n",
    "| 9 | page\\_rank | 0.0487 | Graph\n",
    "| 10 | weighted\\_out\\_degree | 0.0487 | Graph\n",
    "| 11 | weighted\\_in\\_degree | 0.0486 | Graph\n",
    "| 12 | betweenness\\_unweighted | 0.0479 | Graph\n",
    "| 13 | flight\\_count\\_24h | 0.0472 | Temporal\n",
    "| 14 | avg\\_origin\\_dep\\_delay | 0.0456 | Temporal\n",
    "| 15 | closeness | 0.0455 | Graph\n",
    "\n",
    "The Mutual Information (MI) analysis clearly identifies the most important features driving your flight delay prediction model, and these top features strongly align with the knowledge that the majority of delays occur on the ground, particularly at the gate. The top four features previous flight delay in minutes, route, average route delay, and carrier rating together capture the critical concepts of cascading congestion, structural risk, and historical performance.\n",
    "\n",
    "The highest-ranking feature, previous flight delay in minutes (MI Score: 0.113), is a direct measure of cascading delay, which is the single most influential factor in air travel disruption. Given that 60% of delays occur at the gate, this feature tracks whether the inbound flight that the aircraft is scheduled to fly next was itself delayed. A delay at the gate prevents the aircraft from being turned around and dispatched on time, directly causing the next flight to be late. This immediate, time-dependent link explains why its MI score is significantly higher than any other feature, highlighting that the temporal flow of aircraft congestion is the primary mechanism of delay propagation.\n",
    "\n",
    "The next three features, route, average route delay, and carrier rating, capture the structural and historical risks inherent in the flight schedule. Route (MI Score: 0.088) represents the specific origin-destination pair, and its high score shows that some routes are inherently more delay-prone than others due to traffic volume, slot restrictions, or regional weather patterns. Similarly, average route delay (MI Score: 0.087) provides the long-term historical average delay for that exact route, effectively capturing its baseline reliability. Finally, carrier rating (MI Score: 0.084) is an engineered feature summarizing the quality or historical performance of the flight's operating carrier, which is also reflected by the high ranking of carrier airline (MI Score: 0.083). These features collectively tell the model: \"Regardless of the current minute's weather, this specific route, flown by this specific carrier, is historically unreliable and prone to gate-level delays.\"\n",
    "\n",
    "The remaining features in the top 15 primarily reinforce these concepts, especially the Operational and Graph features which measure system load. Features like average daily flight routes (MI Score: 0.074) and flight count in the last 24 hours (MI Score: 0.047) quantify the traffic volume, increasing the probability of a delay when the airport is busy. Furthermore, the strong performance of the Graph features (Page Rank, Betweenness, Closeness) demonstrates the importance of network connectivity. For instance, pageRank (MI Score: 0.049) identifies critical, high-traffic hub airports whose disruptions quickly propagate across the network, justifying the investment in modeling the airline system as a complex graph.\n",
    "\n",
    "We removed the top feature, since its score was so high, it is indicative of leakage. The Mutual Importance (MI) score of 2.80 for HourlyAltimeterSetting is highly indicative of data leakage because this value is orders of magnitude greater than the scores for strong predictors like previous flight delay (0.11). This suggests an almost perfect, non-causal relationship with the target, likely stemming from the feature being a proxy for the outcome itself. The leakage is likely caused by imputation strategy, where a value like 2.80 was used to encode missing pressure readings that only occur during extreme weather events or flight cancellations, making the feature effectively a hidden \"cancelled/delayed\" flag.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1328f0ab-c08f-43e0-b185-b752113d05b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Error Analysis\n",
    "\n",
    "### 2-Stage XGB Quantile Regressor Analysis\n",
    "\n",
    "Overall the model performs decently well with an F2 score of 0.62 providing a reasonable balance between correctly identifying true delays and minimizing false alarms, but there is still some room to improve.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/stephanie-cal/mids261-final_project/main/p3_images//cm_2_stage_test.png\">\n",
    "\n",
    "The confusion matrix reveals a model that achieves a moderate overall Accuracy of 67.67% but suffers from an imbalance in its error types. The model excels at identifying negative outcomes, correctly predicting 3.41 million flights as On-Time (True Negatives). Conversely, it misses a significant portion of the positive outcomes, resulting in 1.81 million False Negatives (FN), flights that were delayed but were incorrectly predicted as on-time. This high FN count means the model only achieves 34.36% Recall, successfully identifying just over one-third of all actual delays.\n",
    "\n",
    "This poor Recall indicates that the model is conservative, while its high precision of 77.81% ensures that when the model does predict a delay, it is correct nearly four out of five times, its reluctance to predict a delay results in it missing the majority of true risk events. The low False Positive (FP) count of 270,965 further confirms this conservative bias and the model avoids false alarms. However, this comes at the cost of missing millions of actual delays. We can try adjusting the classification threshold a bit lower to balance the recall and precision tradeoff.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/stephanie-cal/mids261-final_project/main/p3_images/cm_source_2_stage_test.png\">\n",
    "\n",
    "The stacked bar graph shows the decision source of the prediction, and gives a closer look at how the two-stage classification system contributes to the model's performance. The analysis reveals a significant reliance on the primary classifier for generating True Negatives (TN), contributing an overwhelming 99.17% of all correct \"On-Time\" predictions. Conversely, the True Positives (TP) are almost evenly split, with the main classifier contributing 53% and the high quantile rule contributing a substantial 47%.\n",
    "\n",
    "The False Negatives (FN), are almost exclusively driven by the main classifier, which accounts for 99.84% of these errors, while the False Positives (FP) are also largely driven by the classifier (92.36%). To improve the model, the focus must be on improving the main classifier's recall either by lowering its internal classification threshold or implementing cost-sensitive learning to stop it from generating the large volumes of incorrect FN.\n",
    "\n",
    "\n",
    "### Neural Net Tower\n",
    "\n",
    "The multitower model presents a trade-off in its predictive performance, achieving a moderate overall accuracy of 68.30% while excelling at recognizing the majority class. The model correctly identifies 3.47 million flights as On-Time (True Negatives) and offers highly reliable alerts, with 76.87% precision. This means that when the model predicts a delay, the warning is trustworthy. However, this reliability comes at the expense of high risk aversion, demonstrated by the large volume of errors in the positive class. The model generates 1.76 million False Negatives (FNs), resulting in a low recall (True Positive Rate) of only 34.74%.\n",
    "This performance profile confirms that the multitower model is overly conservative and possesses a strong bias toward the negative (\"On-Time\") outcome. \n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/stephanie-cal/mids261-final_project/main/p3_images/cm_multitower_test.png\">\n",
    "\n",
    "#### Analysis of False Negatives\n",
    "**When the Model Misses Delays**\n",
    "\n",
    "We analyzed false negatives—cases where the model predicts no delay but the flight is actually delayed. This analysis is critical because we've optimized the model for recall: missing a delay prediction (a false negative) is more costly to our operations than a false alarm. Two major categories stood out in our analysis, carriers and routes.\n",
    "\n",
    "##### Airline Carriers\n",
    "<img src=\"https://raw.githubusercontent.com/stephanie-cal/mids261-final_project/main/p3_images/Error_rates_given_airlines.png\">\n",
    "\n",
    "Our analysis revealed significant carrier-specific performance issues. When the model predicts \"no delay,\" it's wrong 6-13% of the time, depending on the carrier. Budget and regional airlines show the highest error rates:\n",
    "\n",
    "- Frontier: Wrong 12.76% of the time (nearly double the rate of major carriers)\n",
    "- ExpressJet: Wrong 9.66% of the time  \n",
    "- Mesa Airlines: Wrong 9.28% of the time\n",
    "\n",
    "In contrast, the model performs better with legacy carriers:\n",
    "- Delta: Wrong 6.13% of the time\n",
    "- Southwest: Wrong 7.08% of the time\n",
    "\n",
    "What makes this more concerning is the severity: when the model misses delays for budget and regional carriers, those delays average 80-90+ minutes—indicating major operational disruptions rather than minor inconveniences.\n",
    "\n",
    "The root cause appears to be that the model treats all carriers similarly (assigning comparable probability scores of 0.23-0.27), failing to distinguish that budget and regional airlines operate with tighter schedules and less operational buffer. When things go wrong for these carriers, delays cascade more severely, but the model hasn't learned this pattern.\n",
    "\n",
    "To address this issue, we recommend incorporating carrier-specific features that capture operational patterns unique to each airline. For example, adding metrics like \"delays by this carrier in the last 24 hours\" or \"carrier's average delay recovery time\" would provide the model with signals to distinguish between airlines with different operational characteristics.\n",
    "\n",
    "##### Routes\n",
    "Focusing on routes, the false negative (FN) rate reaches 99.8% and the top 15 routes all have false negative rates over 95%. This shows very strong model bias toward the negative class ('no delay'), making the model less effective in these cases. Taking a closer look at these routes reveal where our model struggles. There are a few patterns in these routes:\n",
    "- Route length - mainland to Hawaii, coast to coast routes\n",
    "- Hub to smaller airports\n",
    "- Low sample size\n",
    "\n",
    "**Top 15 Routes by FN Rate**\n",
    "| Route  |   False Negative Rate (%) |   Total Flights |\n",
    "|:--------|--------------:|----------------:|\n",
    "| DFW-HNL |         99.8  |            1175 |\n",
    "| DFW-OGG |         99.75 |             923 |\n",
    "| IAH-HNL |         99.67 |             651 |\n",
    "| SRQ-EWR |         99.54 |             513 |\n",
    "| DFW-SJU |         99.16 |             595 |\n",
    "| ORD-OGG |         97.33 |             336 |\n",
    "| SLC-OGG |         97.22 |              82 |\n",
    "| DEN-LIH |         96.97 |              76 |\n",
    "| ORD-HNL |         96.42 |             772 |\n",
    "| BOS-SJC |         96.09 |             313 |\n",
    "| PHX-LIH |         95.73 |             493 |\n",
    "| SBN-EWR |         95.65 |              67 |\n",
    "| MDW-RNO |         95.45 |             363 |\n",
    "| EWR-SMF |         95.42 |             399 |\n",
    "| BWI-SJC |         95.3  |             329 |\n",
    "\n",
    "**Route Length**<br>\n",
    "The majority of the routes are characterized by significant flight lengths, particularly those connecting the continental U.S. to Hawaii (HNL, OGG, LIH). There are 8 flights to Hawaii in the top 15 routes. Long-haul flights naturally accumulate and compound delays due to an extended vulnerability window. Over the course of 5 to 10 hours, factors such as slight initial weather conditions, minor air traffic control metering delays across multiple zones, and stricter crew duty time limits are more likely to push the flight past the official delay threshold. Our model does not effectively integrate and predict the cascading, systematic risks that occur over large distances resulting in the model conservatively predicting “on-time”, even when the real-world probability of delay is increasing.\n",
    "\n",
    "**Hub to Smaller Airport**<br>\n",
    "The pattern of routes connecting major carrier hubs (like Dallas/DFW or O'Hare/ORD) to smaller, less-resourced destination airports contributes significantly to the problem. At major hubs, there is often operational flexibility like spare gates, maintenance crews, and reserve aircraft to absorb unexpected ground delays. However, at smaller destinations, especially remote ones like Hawaiian airports, operational buffers are minimal. If a specific gate is occupied, or a minor maintenance issue arises upon arrival, the smaller airport has limited capacity to recover, virtually guaranteeing a departure delay for the next leg. Although we have features to determine the size of the origin and destination airports, the model still has trouble assessing the risk.\n",
    "\n",
    "**Low Sample Size**<br>\n",
    "The high false negative rate is structurally compounded by the low volume of data available for these specific routes. These high false negative rate routes average only about 470 flights, significantly below the overall mean number of flights per route (over 1,500 with a maximum flights per route of over 20,000). Models trained on large datasets naturally optimize their parameters to minimize error where the most data exists. Consequently, the model underfits the unique, higher-risk operational dynamics of these sparse routes and defaults to the majority outcome—\"no delay.\" The massive imbalance between the \"on-time\" and \"delayed\" outcomes is amplified by the scarcity of data points for these less popular routes, causing the model to conservatively and incorrectly predict \"on-time\" every time.\n",
    "\n",
    "**Next Steps**<br>\n",
    "The high false negative rate is a clear indicator that the model is too conservative and lacks sensitivity to critical, non-linear operational factors present on long-haul, low-volume routes. To improve the model, we can tackle this in two places. First, feature engineering should include long-range forecasting variables, better representations of operational slack at destination airports, and a metric capturing compounding delay risk over distance. Second, model tuning should be adjusted. We can explore different classification probability thresholds for predicting a delay. These results suggest it should be lowered. Another avenue is to use cost-sensitive learning, where false negatives are assigned a higher penalty since the error is more important to avoid. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "94aff3b4-e10b-4292-ab7d-6db75fa3ceeb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Discussion\n",
    "\n",
    "#### Experiments and Pipelines\n",
    "\n",
    "The experiments conducted across Phases 2 and 3 provide several insights into the performance and viability of the pipelines considered:\n",
    "\n",
    "**Regression-only pipeline**\n",
    "- Performs well in predicting continuous delays (low MAE).  \n",
    "- Underperforms as a classifier, with modest precision and only moderate recall.  \n",
    "- Not ideal when operational priorities emphasize catching all true delays.\n",
    "\n",
    "**Two-stage XGBoost Regressor + MLP Classifier pipeline (Ryan)**\n",
    "- Test-F2 score was almost identical to our naive baseline.\n",
    "- After balancing, model only predicts one class, so doesn't actually \"learn\".\n",
    "- Simply providing predicted delay times is not enough to help classification\n",
    "\n",
    "\n",
    "**Two Tower Neural Network Pipeline**\n",
    "- Jointly models delay magnitude and delay occurrence, preserving the continuous delay signal while directly optimizing the 15-minute classification objective.\n",
    "- Learns context-dependent carrier and route effects via FiLM modulation and Time2Vec temporal encoding, capturing non-linear and cyclical delay patterns.\n",
    "- Avoids error propagation inherent to two-stage pipelines, but introduces additional model complexity and reduced interpretability compared to tree-based approaches.\n",
    "\n",
    "\n",
    "**Two-Stage XGBoost pipeline with Quantile Filtering**\n",
    "- Quantile regression maintains the full richness of the continuous delay signal.  \n",
    "- The second stage focuses classification efforts on the “uncertain band” where delays around the 15-minute threshold are most ambiguous.  \n",
    "- Expected to improve precision and reduce unnecessary interventions while maintaining high recall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a8185565-f74a-4345-b4c9-80d7217c2acc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "#### Broader Perspective and Implications\n",
    "\n",
    "From a broader modeling perspective, these results highlight several important themes:\n",
    "\n",
    "- **Temporal generalization matters.** Training on three quarters and testing on the final quarter mimics real-world deployment and demonstrates that, given features addressing seasonality, the model can handle natural seasonal shifts in flight traffic patterns.\n",
    "\n",
    "- **Operational costs shape metric use.** Since false negatives (missed delays) appear more harmful than false positives, metrics such as F₂ are more appropriate than accuracy or F₁.\n",
    "\n",
    "- **Model architecture must reflect the problem structure.** Preserving continuous delay prediction enables more nuanced downstream decision-making than pure classification. The two-stage structure leverages this continuity while still addressing the discrete operational threshold.\n",
    "\n",
    "- **Nonlinear models are more suited to flight data than linear models.** Our Neural Network and XGBoost's performance suggests that flight-delay prediction is inherently non-linear and influenced by interactions among weather, congestion, aircraft characteristics, and scheduling features.\n",
    "\n",
    "- **Segmenting data into easy and hard predictions may help define more reliable decision boundaries** Among the test set examples identified as ambiguous by the 2-stage quantile model, the F2 for our final model was 0.486, whereas a simple XGBClassifier had an F2 of just 0.431. This means that not only did our final model have significantly better overall performance than any baselines, but also that filtering out ambiguous cases before performing classification helps the loss focus on defining a better boundary among difficult examples.\n",
    "\n",
    "\n",
    "#### Consequences for Air Traffic Controllers\n",
    "\n",
    "An overall F2 score of 0.621 indicates that the delay classifier provides meaningful—but not uniform—operational value for air traffic controllers. Because F2 emphasizes recall, this performance suggests the model successfully identifies a large share of true delay events, supporting early intervention to prevent downstream congestion and resource conflicts. Importantly, this aggregate metric masks a high-confidence subset of predictions identified using the first stage of the quantile regressor: approximately 5.24% of flights fall into a regime where the classifier achieves an F2 score of 0.93. For air traffic controllers, this subset represents a particularly valuable signal—flights for which the model’s delay predictions are both highly reliable and actionable. In practice, these high-certainty cases could be prioritized for proactive measures such as gate reassignment, sequencing adjustments, or targeted communication, allowing controllers to act decisively with minimal risk of unnecessary intervention. The remaining flights, where predictive uncertainty is higher, reinforce the role of the model as a decision-support system rather than an automated controller, guiding attention without replacing human judgment in ambiguous situations.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6830f070-31d5-4d75-b838-bbddd1d1e4b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b1fd178c-d0cb-4db0-8691-f3cf2ede0919",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "This project set out to build a dependable machine-learning system capable of forecasting flight delays, a task central to air traffic controllers who must anticipate congestion and maintain safe, efficient traffic flow. We hypothesized that moving beyond standard regression baselines to domain-aware, recall-optimized architectures, enriched with kriging-interpolated weather features, would provide the spatial and temporal fidelity required for operationally meaningful predictions.\n",
    "The experimental results support this hypothesis. \n",
    "\n",
    "Mutual Information analysis confirmed that the most robust predictive signals were driven by operational congestion and historical patterns, led by prior delay in minutes (MI: 0.11), followed by high-cardinality features such as route (MI: 0.088) and aircraft tail number (MI: 0.076). Our development process was intentionally rigorous and iterative, exploring a range of increasingly sophisticated architectures. These included two-stage pipelines that fed regression outputs into downstream classifiers, as well as specialized “ambiguous case” routing strategies in which flights with borderline delay predictions were isolated and passed to secondary models for higher-recall scrutiny. While these approaches yielded targeted performance gains, they also introduced additional architectural complexity.\n",
    "\n",
    "Across all evaluated models, the strongest overall performance was achieved by the two-stage quantile XGBoost architecture, which reached a threshold-optimized Test F2 score of 0.621. Notably, this model also identified a high-confidence subset of flights—approximately 5.24% of all cases—for which it achieved an F2 score of 0.93, highlighting its ability to surface especially actionable delay signals. In parallel, the Two-Tower Neural Network delivered competitive performance with an optimized Test F2 score of 0.6187, demonstrating that shared representation learning across regression and classification tasks can effectively denoise highly imbalanced delay signals.\n",
    "These findings underscore a central contribution of this work: reliable delay prediction depends as much on objective alignment as on model sophistication. By explicitly optimizing for the F2 score—deliberately prioritizing Recall over Precision—we aligned model behavior with operational reality. In air traffic management, the cost of a false negative (missing a true delay) manifests as congestion and safety risk, whereas a false positive typically results only in a conservative buffer.\n",
    "\n",
    "Overall, this project delivers more than a predictive model; it establishes a framework for decision support grounded in both domain knowledge and operational priorities. From kriging-based weather integration to ambiguity-aware modeling and recall-driven optimization, the system is designed not merely to predict delays, but to support air traffic controllers in managing the complexity of modern airspace. Future work will focus on integrating these high-signal predictions into real-time flow management tools to enable effective “manage-by-exception” strategies.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "436a6b4a-aed5-4e37-9e32-0fef4e9adfd2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Gap Analysis\n",
    "| Area | Current State | Desired State | Gap | Action Plan |\n",
    "| :--- | :--- | :--- | :--- | :--- |\n",
    "| **Data and Features** | 2-hour sliding window | Find optimum sliding window | Only one time gap has been tested | Test other time gaps for the window |\n",
    "| **Data and Features** | Airline mergers and bankruptcy not handled (index stability risk) | Stable indexing for high-cardinality features | Current indexing relies on simple sequential mapping that may break with new/deleted entities | Implement entity embeddings to handle changes in high-cardinality categorical features |\n",
    "| **Data and Features** | Airline ratings data leakage risk | Airline ratings that respect the cross validation time window | Ratings are generated from all past data prior to 2025. | Make this a time-series aware metric creating a look-up table by year or month. |\n",
    "| **Model Performance and Quality** | Improve recall (78%) (multi-tower NN) | Target 80% recall | Continue to reduce false negatives | Rebalance strategies, and threshold tuning |\n",
    "| **Model Performance and Quality** | Improve best F2 scores (two-stage model) | Target F2 > 0.65 on the test set | The current F2 score still leaves a significant number of true delays undiscovered. | Look into tuning the classification threshold by looking at the precision-recall curve. |\n",
    "| **Operationalization (MLOps)** | Reduce training time (over 4 hours for multi-tower NN) | Reduce training time by 25% | Balance between load and caching data into memory, and available memory | Check data encoding efficiency and I/O bottleneck |\n",
    "| **Operationalization (MLOps)** | Model monitoring and alerting | Continuous monitoring for data model and concept drift | Lack of automated alerting means production performance degradation may go unnoticed. | Develop and define drift detection and automate alert system |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "20843e57-588a-43b5-845a-3b413f8f4afa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# References\n",
    "[1] Joint Economic Committee Majority Staff. Flight Delays Cost Passengers, Airlines, and the U.S. Economy Billions. Joint Economic Committee; May 2008. https://www.jec.senate.gov/public/_cache/files/47e8d8a7-661d-4e6b-ae72-0f1831dd1207/yourflighthasbeendelayed0.pdf. Accessed November 23, 2025.\n",
    "\n",
    "[2] LendingTree. Worst On-Time Arrival Performances in 2023. LendingTree; August 14, 2023. https://www.lendingtree.com/credit-cards/study/airlines-on-time-arrivals/. Accessed November 23, 2025.\n",
    "\n",
    "[3] Airlines for America. U.S. Passenger Carrier Delay Costs. Airlines for America. https://www.airlines.org/dataset/u-s-passenger-carrier-delay-costs/. Published October 3, 2025. Accessed Novemeber 21, 2025."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c37024f2-3500-4bf4-bee7-252869aae530",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Appendix\n",
    "\n",
    "* Team Planning & Gantt Chart\n",
    "* Link to GitHub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a5bd4f22-d4d4-43a6-b286-9b5011f61dc9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### List of Engineered Features\n",
    "| Feature name | Family | What |\n",
    "|---|---|---|\n",
    "| dep_hour_sin/cos<br>dow_sin/cos<br>doy_sin/cos | Temporal Feature | Converted linear time features (Departure Hour, Day of Week, Day of Year) into continuous Sine and Cosine pairs |\n",
    "| ground_flights_last_hour, arrivals_last_hour | Temporal Feature - airport congestion | Created real-time traffic volume metrics for both the Origin and Destination airports |\n",
    "| PageRank | Graph feature | Captures an airport’s global importance in the flight network, reflecting its exposure to and influence on system-wide delay propagation. Values were determined after 20 iterations. |\n",
    "| Weighted / Unweighted In-Degree | Graph feature | Measures the volume or diversity of inbound connections, indicating susceptibility to upstream delays from multiple origins. The unweighted degree is determined by the number of distinct inbound origins, and the weighted degree is determined by the number of inbound flights. |\n",
    "| Weighted / Unweighted Out-Degree | Graph feature | Reflects the number or intensity of outbound dependencies, capturing operational complexity and downstream scheduling pressure. |\n",
    "| Average Daily Flights per Runway | Graph feature | Serves as a proxy for airport throughput and runway demand. Runway data was gathered from the Bureau of Transportation Statistics under the U.S. Department of Transportation on November 27, 2025 at https://geodata.bts.gov/datasets/usdot::runways/about. |\n",
    "| Weighted / Unweighted Betweenness Centrality | Graph feature | Identifies airports that act as network bottlenecks where delays are more likely to cascade across routes. |\n",
    "| Closeness Centrality | Graph feature | Measures how rapidly delays can propagate to or from an airport through the network. Distances calculated using haversine formula on airport lat/long. |\n",
    "| Average Origin / Destination Delay | Graph feature | Encodes persistent, location-specific operational and environmental factors affecting delay likelihood. |\n",
    "| Average Daily Flights on Route | Graph feature | Captures route-level congestion risk driven by high traffic density. |\n",
    "| Average Delay on Route | Graph feature | Reflects recurring, route-specific delay patterns not explained by airport-level characteristics. |\n",
    "| Origin delays in the last 7 days | Temporal Feature | Number of flights delayed at the origin in the last 7 days |\n",
    "| Previous flight delayed | Temporal Feature | Binary variable indicating if the previous incoming flight was delayed |\n",
    "| Previous flight delayed (in minutes) | Temporal Feature | Delay in minutes indicating how much the previous incoming flight was delayed by |\n",
    "| Origin delays last 4 hours | Temporal Feature | Number of delays at the origin in the last 4 hours |\n",
    "| Delay origin (7d) | Temporal Feature | Number of flights delayed at the origin airport in the last 7 days |\n",
    "| Previous flight delay (minutes) | Temporal Feature | Delay duration of the previous flight in minutes |\n",
    "| Previous flight delay | Temporal Feature | True/False, was the previous flight delayed |\n",
    "| Origin delays (4h) | Temporal Feature | Number of delays at the origin airport in the last 4 hours |\n",
    "| Route | Temporal Feature | Concatenate ORIGIN and DESTINATION |\n",
    "| Delay route (7d) | Temporal Feature | Number of delays on route in the last 7 days |\n",
    "| Flight count (24h) | Temporal Feature | Number of flights a single plane has flown in the last 24 hours |\n",
    "| Landing time difference in minutes | Temporal Feature | Time between scheduled landing time and next departure of a single plane |\n",
    "| Average Arrival Delays (origin) | Temporal Feature | 7 day rolling average arrivals delay at the ORIGIN |\n",
    "| Average Taxi Out (origin) | Temporal Feature | 7 day rolling average taxi out time at ORIGIN |\n",
    "| Holiday | Temporal Feature | True/False, is the flight date a US holiday, or the super bowl? |\n",
    "| Within 3 days of a holiday | Temporal Feature | True/False Is the flight date within 3 days of a holiday? |\n",
    "| Airport hub class | Categorical | Based on FAA Hub categories<br><br>hub category = airport's annual passenger boardings/US annual passenger boardings<br><br>[0] Large Hub (P-L) Handles 1.00% or more of total U.S. annual boardings.<br>[1] Medium Hub (P-M) Handles 0.25% to less than 1.00% of total U.S. annual boardings.<br>[2] Small Hub (P-S) Handles 0.05% to less than 0.25% of total U.S. annual boardings.<br>[3] Non-Hub Primary (P-N) Handles less than 0.05% but has at least 10,000 annual boardings.<br>[4] Non-Primary Commercial Service (CS) Has between 2,500 and 10,000 annual boardings.<br>[5] Other Includes Reliever and General Aviation (GA) airports, or codes that were not valid airport identifiers. |\n",
    "| Airline carrier rating | Categorical | Use LLM to rate airline sentiment/perception on liechert scale 1-5 in some llm based on: customer satisfaction, amenities, change fees, overall flight experience |\n",
    "| Airline category | Categorical | Use FAA/DOT categorization of airline sizes (major, national, regional) |\n",
    "| Hourly visibility change (3h) | Weather Feature | Difference current measurement, and 3 hours ago |\n",
    "| Hourly station pressure change (3h) | Weather Feature | Difference current measurement, and 3 hours ago |\n",
    "| Hourly dry bulb temperature change (3h) | Weather Feature | Difference current measurement, and 3 hours ago |\n",
    "| Hourly precipitation change (3h) | Weather Feature | Difference current measurement, and 3 hours ago |\n",
    "| Arrivals in the last hour | Temporal Feature | Counts arrivals in the last hour at ORIGIN |\n",
    "| Grounded flights in the last hour | Temporal Feature | Counts grounded flights in the last hour at ORIGIN |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2ebe017c-6249-4cea-8ac7-313a9769a73a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Team Planning: Gantt Chart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e70f5d4f-7566-4968-b888-af7e1cc184f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "To streamline progress monitoring and accountability, we have developed a dynamic project tracker that provides real-time updates on task ownership and internal deadlines. The project is structured into phased milestones, each aligned with client deliverables, submission requirements, and upcoming requests.\n",
    "Leadership rotates weekly, designating a different team member as lead to encourage shared ownership and diverse decision-making perspectives. Regular check-ins are scheduled to maintain alignment and foster collaboration across workstreams.\n",
    "\n",
    "Phases are interdependent by design. For example, the initial exploratory data analysis (EDA) conducted this week establishes the foundation for deeper analytical work and advanced EDA during subsequent feature engineering stages. This progressive approach ensures that insights evolve alongside the modeling pipeline.\n",
    "\n",
    "Finally, we have collectively discussed team priorities, individual development goals, and preferred working styles to optimize task allocation, leverage existing expertise, and provide opportunities for growth in new technical areas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9a9b9041-978e-4dbf-912f-c853a6a68278",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<!-- ![261_Gantt_Chart_Project_Plan.png](/Workspace/Users/seowyang@berkeley.edu/261_Gantt_Chart_Project_Plan.png) -->\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/stephanie-cal/mids261-final_project/main/261_Gantt_Chart_Project_Plan.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "782fb393-4eb6-4ad8-9589-e1adbdf39aa8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Link to the code repository: \n",
    "https://github.com/stephanie-cal/mids261-final_project\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "(Clone) [Team_2-2] Phase 3 Notebook",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
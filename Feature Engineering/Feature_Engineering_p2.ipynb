{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Flight Delay Prediction: Feature Engineering Pipeline\n",
        "\n",
        "**Author:** Daniel Costa  \n",
        "**Course:** W261 - Machine Learning at Scale  \n",
        "**Institution:** UC Berkeley\n",
        "\n",
        "---\n",
        "\n",
        "## Overview\n",
        "\n",
        "This notebook implements a comprehensive feature engineering pipeline for flight delay prediction using PySpark. The pipeline processes one year of flight data and creates temporal, operational, and weather-based features designed to predict departure delays **2 hours before scheduled departure**.\n",
        "\n",
        "### Key Features Engineered\n",
        "\n",
        "| Feature | Description | Window |\n",
        "|---------|-------------|--------|\n",
        "| `origin_delays_4h` | Count of delays (>15 min) at origin airport | 4h to 2h before flight |\n",
        "| `prev_flight_delay` | Binary flag if aircraft's previous flight was delayed | Previous flight |\n",
        "| `prev_flight_delay_in_minutes` | Delay duration of aircraft's previous flight | Previous flight |\n",
        "| `delay_origin_7d` | Sum of departure delays at origin | 7 days to 4h before |\n",
        "| `delay_origin_carrier_7d` | Sum of delays by origin + carrier | 7 days to 4h before |\n",
        "| `delay_route_7d` | Sum of delays on same route | 7 days to 4h before |\n",
        "| `flight_count_24h` | Number of flights per aircraft per day | Same day |\n",
        "| `AVG_ARR_DELAY_ORIGIN` | Rolling average arrival delay at origin | 7 days to 4h before |\n",
        "| `AVG_TAXI_OUT_ORIGIN` | Rolling average taxi-out time at origin | 7 days to 4h before |\n",
        "\n",
        "### Data Leakage Prevention\n",
        "\n",
        "All features respect a **2-hour prediction buffer** to ensure no information from within 2 hours of departure is used, simulating real-world prediction conditions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 1. Configuration & Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# CONFIGURATION\n",
        "# =============================================================================\n",
        "\n",
        "# Paths\n",
        "GROUP_SECTION = \"2\"\n",
        "GROUP_NUMBER = \"2\"\n",
        "BASE_PATH = f\"dbfs:/student-groups/Group_{GROUP_SECTION}_{GROUP_NUMBER}\"\n",
        "DATASET_NAME = \"1_year_custom_joined\"\n",
        "\n",
        "INPUT_PATH = f\"{BASE_PATH}/{DATASET_NAME}/graph_feature_splits\"\n",
        "OUTPUT_PATH = f\"{BASE_PATH}/{DATASET_NAME}/feature_eng/training_splits\"\n",
        "\n",
        "# Train/Validation/Test Split Ratios\n",
        "TRAIN_RATIO = 0.70\n",
        "VALIDATION_RATIO = 0.10\n",
        "TEST_RATIO = 0.20  # Implicit: 1 - TRAIN_RATIO - VALIDATION_RATIO\n",
        "\n",
        "# Time Windows (in seconds)\n",
        "SECONDS_2_HOURS = 2 * 60 * 60      # 7,200\n",
        "SECONDS_4_HOURS = 4 * 60 * 60      # 14,400\n",
        "SECONDS_7_DAYS = 7 * 24 * 60 * 60  # 604,800\n",
        "\n",
        "# Delay Threshold (minutes)\n",
        "DELAY_THRESHOLD_MINUTES = 15\n",
        "\n",
        "# MLflow Configuration\n",
        "EXPERIMENT_NAME = \"/Shared/team_2_2/mlflow-baseline\"\n",
        "RANDOM_SEED = 42"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# IMPORTS\n",
        "# =============================================================================\n",
        "\n",
        "from typing import List\n",
        "from pyspark.sql import DataFrame, Window\n",
        "from pyspark.sql.functions import (\n",
        "    col, lit, when, count, sum as spark_sum, avg, lag, lead,\n",
        "    concat, lpad, to_timestamp, floor, row_number, max as spark_max,\n",
        "    coalesce\n",
        ")\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "import mlflow\n",
        "\n",
        "print(f\"MLflow version: {mlflow.__version__}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# MLFLOW SETUP\n",
        "# =============================================================================\n",
        "\n",
        "spark.conf.set(\"spark.databricks.mlflow.trackMLlib.enabled\", \"true\")\n",
        "\n",
        "try:\n",
        "    experiment = mlflow.get_experiment_by_name(EXPERIMENT_NAME)\n",
        "    if experiment is None:\n",
        "        experiment_id = mlflow.create_experiment(EXPERIMENT_NAME)\n",
        "        print(f\"Created new experiment with ID: {experiment_id}\")\n",
        "    else:\n",
        "        print(f\"Using existing experiment: {experiment.name}\")\n",
        "    mlflow.set_experiment(EXPERIMENT_NAME)\n",
        "except Exception as e:\n",
        "    print(f\"Error with experiment setup: {e}\")\n",
        "    fallback_path = f\"/Users/{dbutils.notebook.entry_point.getDbutils().notebook().getContext().userName().get()}/default\"\n",
        "    mlflow.set_experiment(fallback_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 2. Helper Functions\n",
        "\n",
        "Reusable functions for feature engineering to ensure consistency across train/validation/test sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# WEATHER COLUMNS (Required - rows with nulls will be dropped)\n",
        "# =============================================================================\n",
        "\n",
        "WEATHER_COLUMNS = [\n",
        "    'HourlyDryBulbTemperature',\n",
        "    'HourlyDewPointTemperature',\n",
        "    'HourlyRelativeHumidity',\n",
        "    'HourlyAltimeterSetting',\n",
        "    'HourlyVisibility',\n",
        "    'HourlyStationPressure',\n",
        "    'HourlyWetBulbTemperature',\n",
        "    'HourlyPrecipitation',\n",
        "    'HourlyCloudCoverage',\n",
        "    'HourlyCloudElevation',\n",
        "    'HourlyWindSpeed'\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def add_utc_timestamp(df: DataFrame) -> DataFrame:\n",
        "    \"\"\"\n",
        "    Create a UTC timestamp from FL_DATE and CRS_DEP_TIME.\n",
        "    \n",
        "    CRS_DEP_TIME is stored as an integer (e.g., 1430 for 2:30 PM),\n",
        "    so we pad it and combine with the flight date.\n",
        "    \"\"\"\n",
        "    return df.withColumn(\n",
        "        \"utc_timestamp\",\n",
        "        to_timestamp(\n",
        "            concat(\n",
        "                col(\"FL_DATE\"),\n",
        "                lit(\" \"),\n",
        "                lpad(col(\"CRS_DEP_TIME\").cast(\"string\"), 4, \"0\")\n",
        "            ),\n",
        "            \"yyyy-MM-dd HHmm\"\n",
        "        )\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def add_departure_minutes(df: DataFrame) -> DataFrame:\n",
        "    \"\"\"\n",
        "    Convert CRS_DEP_TIME (HHMM format) to minutes since midnight.\n",
        "    \n",
        "    Example: 1430 -> 14*60 + 30 = 870 minutes\n",
        "    \"\"\"\n",
        "    return df.withColumn(\n",
        "        \"CRS_DEP_MINUTES\",\n",
        "        (floor(col(\"CRS_DEP_TIME\") / 100) * 60 + (col(\"CRS_DEP_TIME\") % 100))\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def add_rolling_count_feature(\n",
        "    df: DataFrame,\n",
        "    partition_cols: List[str],\n",
        "    condition_col: str,\n",
        "    threshold: float,\n",
        "    new_col: str,\n",
        "    window_start: int,\n",
        "    window_end: int\n",
        ") -> DataFrame:\n",
        "    \"\"\"\n",
        "    Add a rolling count of records meeting a threshold condition.\n",
        "    \n",
        "    Args:\n",
        "        df: Input DataFrame\n",
        "        partition_cols: Columns to partition the window by\n",
        "        condition_col: Column to apply threshold condition to\n",
        "        threshold: Value threshold for counting\n",
        "        new_col: Name for the new feature column\n",
        "        window_start: Window start in seconds (negative = past)\n",
        "        window_end: Window end in seconds (negative = past)\n",
        "    \n",
        "    Returns:\n",
        "        DataFrame with new count feature\n",
        "    \"\"\"\n",
        "    window = Window \\\n",
        "        .partitionBy(*partition_cols) \\\n",
        "        .orderBy(col(\"utc_timestamp\").cast(\"long\")) \\\n",
        "        .rangeBetween(window_start, window_end)\n",
        "    \n",
        "    return df.withColumn(\n",
        "        new_col,\n",
        "        count(when(col(condition_col) > threshold, 1)).over(window)\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def add_rolling_sum_feature(\n",
        "    df: DataFrame,\n",
        "    partition_cols: List[str],\n",
        "    agg_col: str,\n",
        "    new_col: str,\n",
        "    window_start: int,\n",
        "    window_end: int,\n",
        "    default_value: float = 0\n",
        ") -> DataFrame:\n",
        "    \"\"\"\n",
        "    Add a rolling sum feature with null handling.\n",
        "    \n",
        "    Args:\n",
        "        df: Input DataFrame\n",
        "        partition_cols: Columns to partition the window by\n",
        "        agg_col: Column to sum\n",
        "        new_col: Name for the new feature column\n",
        "        window_start: Window start in seconds (negative = past)\n",
        "        window_end: Window end in seconds (negative = past)\n",
        "        default_value: Value to use when sum is null\n",
        "    \n",
        "    Returns:\n",
        "        DataFrame with new sum feature\n",
        "    \"\"\"\n",
        "    window = Window \\\n",
        "        .partitionBy(*partition_cols) \\\n",
        "        .orderBy(col(\"utc_timestamp\").cast(\"long\")) \\\n",
        "        .rangeBetween(window_start, window_end)\n",
        "    \n",
        "    return df.withColumn(\n",
        "        new_col,\n",
        "        coalesce(spark_sum(agg_col).over(window), lit(default_value))\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def add_rolling_avg_feature(\n",
        "    df: DataFrame,\n",
        "    partition_cols: List[str],\n",
        "    agg_col: str,\n",
        "    new_col: str,\n",
        "    window_start: int,\n",
        "    window_end: int,\n",
        "    default_value: float = 0\n",
        ") -> DataFrame:\n",
        "    \"\"\"\n",
        "    Add a rolling average feature with null handling.\n",
        "    \n",
        "    Args:\n",
        "        df: Input DataFrame\n",
        "        partition_cols: Columns to partition the window by\n",
        "        agg_col: Column to average\n",
        "        new_col: Name for the new feature column\n",
        "        window_start: Window start in seconds (negative = past)\n",
        "        window_end: Window end in seconds (negative = past)\n",
        "        default_value: Value to use when average is null\n",
        "    \n",
        "    Returns:\n",
        "        DataFrame with new average feature\n",
        "    \"\"\"\n",
        "    window = Window \\\n",
        "        .partitionBy(*partition_cols) \\\n",
        "        .orderBy(col(\"utc_timestamp\").cast(\"long\")) \\\n",
        "        .rangeBetween(window_start, window_end)\n",
        "    \n",
        "    return df.withColumn(\n",
        "        new_col,\n",
        "        coalesce(avg(agg_col).over(window), lit(default_value))\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def add_previous_flight_delay_features(df: DataFrame) -> DataFrame:\n",
        "    \"\"\"\n",
        "    Add features based on the aircraft's previous flight delay.\n",
        "    \n",
        "    Creates:\n",
        "        - prev_flight_delay_in_minutes: Delay of previous flight (-1 if none)\n",
        "        - prev_flight_delay: Binary flag (1 if previous flight delayed >15 min)\n",
        "    \"\"\"\n",
        "    window = Window.partitionBy(\"TAIL_NUM\").orderBy(\"utc_timestamp\")\n",
        "    \n",
        "    return df \\\n",
        "        .withColumn(\n",
        "            \"prev_flight_delay_in_minutes\",\n",
        "            coalesce(lag(\"DEP_DELAY_NEW\", 1).over(window), lit(-1))\n",
        "        ) \\\n",
        "        .withColumn(\n",
        "            \"prev_flight_delay\",\n",
        "            when(col(\"prev_flight_delay_in_minutes\") > DELAY_THRESHOLD_MINUTES, 1).otherwise(0)\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def add_route_feature(df: DataFrame) -> DataFrame:\n",
        "    \"\"\"\n",
        "    Create a route identifier by combining origin and destination.\n",
        "    \n",
        "    Example: ORIGIN='SFO', DEST='LAX' -> route='SFO-LAX'\n",
        "    \"\"\"\n",
        "    return df.withColumn(\n",
        "        \"route\",\n",
        "        concat(col(\"ORIGIN\"), lit(\"-\"), col(\"DEST\"))\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def add_flight_count_feature(df: DataFrame) -> DataFrame:\n",
        "    \"\"\"\n",
        "    Count the number of flights per aircraft per day.\n",
        "    \n",
        "    This captures how busy an aircraft's schedule is.\n",
        "    \"\"\"\n",
        "    window = Window \\\n",
        "        .partitionBy(\"TAIL_NUM\", \"FL_DATE\") \\\n",
        "        .orderBy(col(\"utc_timestamp\").cast(\"long\"))\n",
        "    \n",
        "    return df.withColumn(\n",
        "        \"flight_count_24h\",\n",
        "        count(\"*\").over(window)\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def add_turnaround_time_feature(df: DataFrame) -> DataFrame:\n",
        "    \"\"\"\n",
        "    Calculate time between landing and next scheduled departure.\n",
        "    \n",
        "    Uses WHEELS_ON (actual landing time) and next flight's CRS_DEP_TIME.\n",
        "    Returns -999 for the last flight of the day (no next flight).\n",
        "    \"\"\"\n",
        "    window = Window \\\n",
        "        .partitionBy(\"TAIL_NUM\") \\\n",
        "        .orderBy(col(\"WHEELS_ON\").cast(\"long\"))\n",
        "    \n",
        "    return df \\\n",
        "        .withColumn(\n",
        "            \"_next_scheduled_dep\",\n",
        "            lead(\"CRS_DEP_TIME\", 1).over(window)\n",
        "        ) \\\n",
        "        .withColumn(\n",
        "            \"LANDING_TIME_DIFF_MINUTES\",\n",
        "            coalesce(\n",
        "                (col(\"_next_scheduled_dep\").cast(\"long\") - col(\"WHEELS_ON\").cast(\"long\")) / 60,\n",
        "                lit(-999)\n",
        "            )\n",
        "        ) \\\n",
        "        .drop(\"_next_scheduled_dep\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 3. Feature Engineering Pipeline\n",
        "\n",
        "Main function that applies all feature engineering transformations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def engineer_features(df: DataFrame) -> DataFrame:\n",
        "    \"\"\"\n",
        "    Apply all feature engineering transformations to a DataFrame.\n",
        "    \n",
        "    This function is applied identically to train, validation, and test sets\n",
        "    to ensure consistency.\n",
        "    \n",
        "    Args:\n",
        "        df: Input DataFrame with raw flight data\n",
        "    \n",
        "    Returns:\n",
        "        DataFrame with all engineered features\n",
        "    \"\"\"\n",
        "    print(\"Starting feature engineering pipeline...\")\n",
        "    \n",
        "    # Drop rows with missing weather data\n",
        "    df = df.dropna(subset=WEATHER_COLUMNS)\n",
        "    print(\"  ✓ Dropped rows with missing weather data\")\n",
        "    \n",
        "    # Basic time features\n",
        "    df = add_departure_minutes(df)\n",
        "    print(\"  ✓ Added CRS_DEP_MINUTES\")\n",
        "    \n",
        "    # Route identifier\n",
        "    df = add_route_feature(df)\n",
        "    print(\"  ✓ Added route feature\")\n",
        "    \n",
        "    # Previous flight delay features\n",
        "    df = add_previous_flight_delay_features(df)\n",
        "    print(\"  ✓ Added previous flight delay features\")\n",
        "    \n",
        "    # Origin delays in 4h-2h window\n",
        "    df = add_rolling_count_feature(\n",
        "        df,\n",
        "        partition_cols=[\"ORIGIN_AIRPORT_SEQ_ID\"],\n",
        "        condition_col=\"DEP_DELAY_NEW\",\n",
        "        threshold=DELAY_THRESHOLD_MINUTES,\n",
        "        new_col=\"origin_delays_4h\",\n",
        "        window_start=-SECONDS_4_HOURS,\n",
        "        window_end=-SECONDS_2_HOURS\n",
        "    )\n",
        "    print(\"  ✓ Added origin_delays_4h\")\n",
        "    \n",
        "    # 7-day delay sums (with 4-hour buffer)\n",
        "    df = add_rolling_sum_feature(\n",
        "        df,\n",
        "        partition_cols=[\"ORIGIN_AIRPORT_SEQ_ID\"],\n",
        "        agg_col=\"DEP_DELAY_NEW\",\n",
        "        new_col=\"delay_origin_7d\",\n",
        "        window_start=-SECONDS_7_DAYS,\n",
        "        window_end=-SECONDS_4_HOURS\n",
        "    )\n",
        "    print(\"  ✓ Added delay_origin_7d\")\n",
        "    \n",
        "    df = add_rolling_sum_feature(\n",
        "        df,\n",
        "        partition_cols=[\"ORIGIN_AIRPORT_SEQ_ID\", \"OP_UNIQUE_CARRIER\"],\n",
        "        agg_col=\"DEP_DELAY_NEW\",\n",
        "        new_col=\"delay_origin_carrier_7d\",\n",
        "        window_start=-SECONDS_7_DAYS,\n",
        "        window_end=-SECONDS_4_HOURS\n",
        "    )\n",
        "    print(\"  ✓ Added delay_origin_carrier_7d\")\n",
        "    \n",
        "    df = add_rolling_sum_feature(\n",
        "        df,\n",
        "        partition_cols=[\"route\"],\n",
        "        agg_col=\"DEP_DELAY_NEW\",\n",
        "        new_col=\"delay_route_7d\",\n",
        "        window_start=-SECONDS_7_DAYS,\n",
        "        window_end=-SECONDS_4_HOURS\n",
        "    )\n",
        "    print(\"  ✓ Added delay_route_7d\")\n",
        "    \n",
        "    # Flight count per aircraft per day\n",
        "    df = add_flight_count_feature(df)\n",
        "    print(\"  ✓ Added flight_count_24h\")\n",
        "    \n",
        "    # Turnaround time\n",
        "    df = add_turnaround_time_feature(df)\n",
        "    print(\"  ✓ Added LANDING_TIME_DIFF_MINUTES\")\n",
        "    \n",
        "    # 7-day rolling averages\n",
        "    df = add_rolling_avg_feature(\n",
        "        df,\n",
        "        partition_cols=[\"ORIGIN_AIRPORT_SEQ_ID\"],\n",
        "        agg_col=\"ARR_DELAY\",\n",
        "        new_col=\"AVG_ARR_DELAY_ORIGIN\",\n",
        "        window_start=-SECONDS_7_DAYS,\n",
        "        window_end=-SECONDS_4_HOURS\n",
        "    )\n",
        "    print(\"  ✓ Added AVG_ARR_DELAY_ORIGIN\")\n",
        "    \n",
        "    df = add_rolling_avg_feature(\n",
        "        df,\n",
        "        partition_cols=[\"ORIGIN_AIRPORT_SEQ_ID\"],\n",
        "        agg_col=\"TAXI_OUT\",\n",
        "        new_col=\"AVG_TAXI_OUT_ORIGIN\",\n",
        "        window_start=-SECONDS_7_DAYS,\n",
        "        window_end=-SECONDS_4_HOURS\n",
        "    )\n",
        "    print(\"  ✓ Added AVG_TAXI_OUT_ORIGIN\")\n",
        "    \n",
        "    print(\"Feature engineering complete!\")\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 4. Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load pre-split datasets\n",
        "print(f\"Loading data from: {INPUT_PATH}\")\n",
        "\n",
        "train_df = spark.read.parquet(f\"{INPUT_PATH}/train.parquet\")\n",
        "validation_df = spark.read.parquet(f\"{INPUT_PATH}/validation.parquet\")\n",
        "test_df = spark.read.parquet(f\"{INPUT_PATH}/test.parquet\")\n",
        "\n",
        "print(f\"\\nDataset sizes:\")\n",
        "print(f\"  Train:      {train_df.count():,} rows\")\n",
        "print(f\"  Validation: {validation_df.count():,} rows\")\n",
        "print(f\"  Test:       {test_df.count():,} rows\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cache datasets for performance\n",
        "train_df = train_df.cache()\n",
        "validation_df = validation_df.cache()\n",
        "test_df = test_df.cache()\n",
        "\n",
        "print(\"Datasets cached.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 5. Apply Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\" * 60)\n",
        "print(\"TRAINING SET\")\n",
        "print(\"=\" * 60)\n",
        "train_df = engineer_features(train_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\" * 60)\n",
        "print(\"VALIDATION SET\")\n",
        "print(\"=\" * 60)\n",
        "validation_df = engineer_features(validation_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\" * 60)\n",
        "print(\"TEST SET\")\n",
        "print(\"=\" * 60)\n",
        "test_df = engineer_features(test_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 6. Validation & Quality Checks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def check_null_counts(df: DataFrame, name: str) -> None:\n",
        "    \"\"\"Display null counts for all columns in a DataFrame.\"\"\"\n",
        "    print(f\"\\nNull counts for {name}:\")\n",
        "    null_counts = df.select(\n",
        "        [count(when(col(c).isNull(), c)).alias(c) for c in df.columns]\n",
        "    )\n",
        "    display(null_counts)\n",
        "\n",
        "\n",
        "def verify_column_consistency(train: DataFrame, val: DataFrame, test: DataFrame) -> bool:\n",
        "    \"\"\"Verify that all datasets have the same columns.\"\"\"\n",
        "    train_cols = set(train.columns)\n",
        "    val_cols = set(val.columns)\n",
        "    test_cols = set(test.columns)\n",
        "    \n",
        "    if train_cols == val_cols == test_cols:\n",
        "        print(\"✓ All datasets have consistent columns\")\n",
        "        return True\n",
        "    else:\n",
        "        print(\"✗ Column mismatch detected!\")\n",
        "        print(f\"  Only in train: {train_cols - val_cols - test_cols}\")\n",
        "        print(f\"  Only in validation: {val_cols - train_cols - test_cols}\")\n",
        "        print(f\"  Only in test: {test_cols - train_cols - val_cols}\")\n",
        "        return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify column consistency across all datasets\n",
        "verify_column_consistency(train_df, validation_df, test_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check for nulls in validation set (representative sample)\n",
        "check_null_counts(validation_df, \"Validation\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display sample of engineered features\n",
        "ENGINEERED_FEATURE_COLS = [\n",
        "    \"ORIGIN\", \"DEST\", \"route\", \"FL_DATE\", \"CRS_DEP_TIME\", \"CRS_DEP_MINUTES\",\n",
        "    \"origin_delays_4h\", \"prev_flight_delay\", \"prev_flight_delay_in_minutes\",\n",
        "    \"delay_origin_7d\", \"delay_origin_carrier_7d\", \"delay_route_7d\",\n",
        "    \"flight_count_24h\", \"LANDING_TIME_DIFF_MINUTES\",\n",
        "    \"AVG_ARR_DELAY_ORIGIN\", \"AVG_TAXI_OUT_ORIGIN\"\n",
        "]\n",
        "\n",
        "print(\"Sample of engineered features:\")\n",
        "display(train_df.select(ENGINEERED_FEATURE_COLS).limit(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 7. Save Processed Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save feature-engineered datasets\n",
        "# Set SAVE_DATA = True when ready to write to DBFS\n",
        "SAVE_DATA = False\n",
        "\n",
        "if SAVE_DATA:\n",
        "    print(f\"Saving datasets to: {OUTPUT_PATH}\")\n",
        "    checkpoint_dataset(train_df, f\"{DATASET_NAME}/feature_eng/training_splits/train\")\n",
        "    checkpoint_dataset(validation_df, f\"{DATASET_NAME}/feature_eng/training_splits/validation\")\n",
        "    checkpoint_dataset(test_df, f\"{DATASET_NAME}/feature_eng/training_splits/test\")\n",
        "    print(\"\\n✓ All datasets saved successfully!\")\n",
        "else:\n",
        "    print(\"⚠ SAVE_DATA is False. Set SAVE_DATA = True to save datasets to DBFS.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 8. Summary\n",
        "\n",
        "### Features Created\n",
        "\n",
        "| Category | Feature | Description |\n",
        "|----------|---------|-------------|\n",
        "| **Time** | `CRS_DEP_MINUTES` | Scheduled departure as minutes since midnight |\n",
        "| **Route** | `route` | Origin-Destination pair (e.g., \"SFO-LAX\") |\n",
        "| **Aircraft History** | `prev_flight_delay` | Binary: was previous flight delayed >15 min? |\n",
        "| **Aircraft History** | `prev_flight_delay_in_minutes` | Previous flight's delay duration |\n",
        "| **Aircraft History** | `flight_count_24h` | Number of flights for this aircraft today |\n",
        "| **Aircraft History** | `LANDING_TIME_DIFF_MINUTES` | Turnaround time to next flight |\n",
        "| **Airport History** | `origin_delays_4h` | Count of delays at origin (4h-2h window) |\n",
        "| **Airport History** | `delay_origin_7d` | Sum of delays at origin (7 day window) |\n",
        "| **Airport History** | `AVG_ARR_DELAY_ORIGIN` | Avg arrival delay at origin (7 day) |\n",
        "| **Airport History** | `AVG_TAXI_OUT_ORIGIN` | Avg taxi-out time at origin (7 day) |\n",
        "| **Carrier History** | `delay_origin_carrier_7d` | Sum of delays by carrier at origin (7 day) |\n",
        "| **Route History** | `delay_route_7d` | Sum of delays on route (7 day) |\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "The processed datasets are ready for model training. Proceed to the modeling notebook to:\n",
        "1. Encode categorical features\n",
        "2. Assemble feature vectors\n",
        "3. Train and evaluate models"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "348372bf-80e0-44c9-885b-7951db0851da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Additional Feature Engineering for Neural Network Models\n",
    "\n",
    "**Author:** Daniel Costa  \n",
    "**Project:** Flight Delay Prediction (W261 Final Project)  \n",
    "**Dataset:** US Domestic Flights (2015-2019)\n",
    "\n",
    "---\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook implements **additional feature engineering** specifically designed to enhance neural network model performance. These features capture temporal patterns, weather dynamics, and airport congestion that are particularly useful for deep learning architectures.\n",
    "\n",
    "### Features Created\n",
    "\n",
    "| Category | Features | Description |\n",
    "|----------|----------|-------------|\n",
    "| **Cyclic Time** | `dep_hour_sin/cos`, `dow_sin/cos`, `doy_sin/cos` | Sine/cosine encoding of time to capture periodicity |\n",
    "| **Weather Deltas** | `*_3h_change` (5 features) | 3-hour changes in weather conditions |\n",
    "| **Origin Congestion** | `ground_flights_last_hour` | Rolling count of departures at origin |\n",
    "| **Destination Congestion** | `arrivals_last_hour` | Rolling count of arrivals at destination |\n",
    "\n",
    "### Why Cyclic Encoding?\n",
    "\n",
    "Neural networks benefit from cyclic encoding of time features because it preserves the circular nature of time:\n",
    "- Hour 23 is close to hour 0 (not 23 units apart)\n",
    "- December is close to January\n",
    "- Saturday is close to Sunday\n",
    "\n",
    "```\n",
    "sin/cos encoding:  hour 0 ≈ hour 24  ✓\n",
    "linear encoding:   hour 0 ≠ hour 24  ✗\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Setup & Configuration](#1-setup--configuration)\n",
    "2. [Data Loading](#2-data-loading)\n",
    "3. [Feature Engineering Functions](#3-feature-engineering-functions)\n",
    "   - [3.1 Cyclic Time Features](#31-cyclic-time-features)\n",
    "   - [3.2 Weather Delta Features](#32-weather-delta-features)\n",
    "   - [3.3 Origin Congestion Features](#33-origin-congestion-features)\n",
    "   - [3.4 Destination Congestion Features](#34-destination-congestion-features)\n",
    "4. [Apply Feature Engineering](#4-apply-feature-engineering)\n",
    "5. [Save Results](#5-save-results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f5ca42b2-1faa-46f0-9daf-b5c20567f1c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1. Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f05bfc82-fd4a-491a-a017-28f3ccd80642",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.21.3\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Imports\n",
    "# -----------------------------------------------------------------------------\n",
    "from pyspark.sql import Window\n",
    "import pyspark.sql.functions as F\n",
    "import numpy as np\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Configuration\n",
    "# -----------------------------------------------------------------------------\n",
    "# Data paths\n",
    "BASE_PATH = \"dbfs:/student-groups/Group_2_2\"\n",
    "DATASET_NAME = \"5_year_custom_joined\"\n",
    "INPUT_DIR = f\"{BASE_PATH}/{DATASET_NAME}/fe_graph_and_holiday/training_splits\"\n",
    "OUTPUT_DIR = f\"{BASE_PATH}/{DATASET_NAME}/fe_graph_and_holiday_nnfeat/training_splits\"\n",
    "\n",
    "# Weather columns for delta features\n",
    "WEATHER_COLUMNS = [\n",
    "    \"HourlyVisibility\",\n",
    "    \"HourlyStationPressure\", \n",
    "    \"HourlyDryBulbTemperature\",\n",
    "    \"HourlyWindSpeed\",\n",
    "    \"HourlyPrecipitation\"\n",
    "]\n",
    "\n",
    "# Time constants\n",
    "SECONDS_PER_HOUR = 3600\n",
    "HOURS_PER_DAY = 24\n",
    "DAYS_PER_WEEK = 7\n",
    "DAYS_PER_YEAR = 365"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6bdd5143-0a0a-469d-87ea-62b5179d2d68",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b76c216-b170-417d-826f-bb549cc3b4c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load train/validation/test splits\n",
    "train_df = spark.read.parquet(f\"{INPUT_DIR}/train.parquet/\")\n",
    "val_df = spark.read.parquet(f\"{INPUT_DIR}/validation.parquet/\")\n",
    "test_df = spark.read.parquet(f\"{INPUT_DIR}/test.parquet/\")\n",
    "\n",
    "print(f\"Train:      {train_df.count():,} rows\")\n",
    "print(f\"Validation: {val_df.count():,} rows\")\n",
    "print(f\"Test:       {test_df.count():,} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "112314a8-b1f3-4bc6-9195-61aacb0148fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 3. Feature Engineering Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Cyclic Time Features\n",
    "\n",
    "Convert time features (hour, day of week, day of year) to sine/cosine pairs to preserve their circular nature for neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f447807a-8b8b-40bb-ba75-f68b1deedcb5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def add_time_features(df):\n",
    "    \"\"\"\n",
    "    Add cyclic time features using sine/cosine encoding.\n",
    "    \n",
    "    Creates 6 new features:\n",
    "    - dep_hour_sin, dep_hour_cos: Hour of departure (0-24 cycle)\n",
    "    - dow_sin, dow_cos: Day of week (0-7 cycle)\n",
    "    - doy_sin, doy_cos: Day of year (0-365 cycle)\n",
    "    \n",
    "    Args:\n",
    "        df: PySpark DataFrame with CRS_DEP_MINUTES, DAY_OF_WEEK, utc_timestamp\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with cyclic time features added\n",
    "    \"\"\"\n",
    "    # Extract base time values\n",
    "    df = df.withColumn(\"dep_hour\", F.col(\"CRS_DEP_MINUTES\") / 60.0)\n",
    "    df = df.withColumn(\"day_of_year\", F.dayofyear(\"utc_timestamp\").cast(\"double\"))\n",
    "\n",
    "    # Hour of day: sin/cos encoding (24-hour cycle)\n",
    "    df = df.withColumn(\n",
    "        \"dep_hour_sin\", \n",
    "        F.sin(2 * F.lit(np.pi) * F.col(\"dep_hour\") / HOURS_PER_DAY)\n",
    "    )\n",
    "    df = df.withColumn(\n",
    "        \"dep_hour_cos\", \n",
    "        F.cos(2 * F.lit(np.pi) * F.col(\"dep_hour\") / HOURS_PER_DAY)\n",
    "    )\n",
    "\n",
    "    # Day of week: sin/cos encoding (7-day cycle)\n",
    "    df = df.withColumn(\n",
    "        \"dow_sin\", \n",
    "        F.sin(2 * F.lit(np.pi) * F.col(\"DAY_OF_WEEK\") / DAYS_PER_WEEK)\n",
    "    )\n",
    "    df = df.withColumn(\n",
    "        \"dow_cos\", \n",
    "        F.cos(2 * F.lit(np.pi) * F.col(\"DAY_OF_WEEK\") / DAYS_PER_WEEK)\n",
    "    )\n",
    "\n",
    "    # Day of year: sin/cos encoding (365-day cycle)\n",
    "    df = df.withColumn(\n",
    "        \"doy_sin\", \n",
    "        F.sin(2 * F.lit(np.pi) * F.col(\"day_of_year\") / DAYS_PER_YEAR)\n",
    "    )\n",
    "    df = df.withColumn(\n",
    "        \"doy_cos\", \n",
    "        F.cos(2 * F.lit(np.pi) * F.col(\"day_of_year\") / DAYS_PER_YEAR)\n",
    "    )\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Weather Delta Features\n",
    "\n",
    "Compute 3-hour changes in weather conditions to capture weather trends that may indicate developing delays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0642d3fa-fcc0-4061-83c8-8f49fa0ab7a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def add_weather_deltas(df):\n",
    "    \"\"\"\n",
    "    Add 3-hour weather change features for trend detection.\n",
    "    \n",
    "    Captures weather dynamics by computing the difference between current\n",
    "    and 3-hour-ago values. Rapidly changing weather (e.g., dropping visibility,\n",
    "    rising wind speed) often precedes delays.\n",
    "    \n",
    "    Creates 5 new features:\n",
    "    - HourlyVisibility_3h_change\n",
    "    - HourlyStationPressure_3h_change\n",
    "    - HourlyDryBulbTemperature_3h_change\n",
    "    - HourlyWindSpeed_3h_change\n",
    "    - HourlyPrecipitation_3h_change\n",
    "    \n",
    "    Args:\n",
    "        df: PySpark DataFrame with weather columns and utc_timestamp\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with weather delta features added\n",
    "    \"\"\"\n",
    "    w = Window.partitionBy(\"ORIGIN_AIRPORT_SEQ_ID\").orderBy(\"utc_timestamp\")\n",
    "    \n",
    "    for col_name in WEATHER_COLUMNS:\n",
    "        lag_col = F.lag(col_name, 3).over(w)\n",
    "        delta_col = F.col(col_name) - lag_col\n",
    "        \n",
    "        # Return None if lag value doesn't exist (avoids bias from defaulting to 0)\n",
    "        df = df.withColumn(\n",
    "            f\"{col_name}_3h_change\",\n",
    "            F.when(lag_col.isNull(), F.lit(None)).otherwise(delta_col)\n",
    "        )\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Origin Congestion Features\n",
    "\n",
    "Count departing flights in the past hour at the origin airport to capture ground congestion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5bb79ceb-9ff4-4fc9-872c-fca2778a0a09",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def add_origin_congestion_features(df):\n",
    "    \"\"\"\n",
    "    Add origin airport congestion feature.\n",
    "    \n",
    "    Counts the number of flights departing from the same origin airport\n",
    "    in the previous hour. High congestion indicates potential ground delays,\n",
    "    gate availability issues, and runway queuing.\n",
    "    \n",
    "    Creates 1 new feature:\n",
    "    - ground_flights_last_hour: Count of departures in past 60 minutes\n",
    "    \n",
    "    Args:\n",
    "        df: PySpark DataFrame with ORIGIN_AIRPORT_SEQ_ID and utc_timestamp\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with origin congestion feature added\n",
    "    \"\"\"\n",
    "    # Convert timestamp to seconds for range-based window\n",
    "    df = df.withColumn(\"utc_ts_sec\", F.col(\"utc_timestamp\").cast(\"long\"))\n",
    "\n",
    "    # Rolling 1-hour window ending at current row\n",
    "    w = (Window\n",
    "         .partitionBy(\"ORIGIN_AIRPORT_SEQ_ID\")\n",
    "         .orderBy(\"utc_ts_sec\")\n",
    "         .rangeBetween(-SECONDS_PER_HOUR, 0))\n",
    "\n",
    "    # Count flights in window, subtract 1 to exclude current row\n",
    "    df = df.withColumn(\n",
    "        \"ground_flights_last_hour\",\n",
    "        F.count(\"utc_ts_sec\").over(w) - 1\n",
    "    )\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Destination Congestion Features\n",
    "\n",
    "Count arriving flights in the past hour at the destination airport to capture airspace and arrival congestion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed1df849-d374-444a-a875-635fb5e14107",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def add_dest_congestion_features(df):\n",
    "    \"\"\"\n",
    "    Add destination airport congestion feature.\n",
    "    \n",
    "    Counts the number of flights arriving at the same destination airport\n",
    "    in the previous hour. High arrival congestion can cause holding patterns,\n",
    "    diversions, and cascading delays.\n",
    "    \n",
    "    Creates 1 new feature:\n",
    "    - arrivals_last_hour: Count of arrivals in past 60 minutes\n",
    "    \n",
    "    Args:\n",
    "        df: PySpark DataFrame with DEST_AIRPORT_SEQ_ID and utc_timestamp\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with destination congestion feature added\n",
    "    \"\"\"\n",
    "    # Convert timestamp to seconds for range-based window\n",
    "    df = df.withColumn(\"utc_ts_sec\", F.col(\"utc_timestamp\").cast(\"long\"))\n",
    "    \n",
    "    # Rolling 1-hour window ending at current row\n",
    "    w = (Window\n",
    "         .partitionBy(\"DEST_AIRPORT_SEQ_ID\")\n",
    "         .orderBy(\"utc_ts_sec\")\n",
    "         .rangeBetween(-SECONDS_PER_HOUR, 0))\n",
    "    \n",
    "    # Count flights in window, subtract 1 to exclude current row\n",
    "    df = df.withColumn(\n",
    "        \"arrivals_last_hour\",\n",
    "        F.count(\"utc_ts_sec\").over(w) - 1\n",
    "    )\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Apply Feature Engineering\n",
    "\n",
    "Apply all feature transformations to train, validation, and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "406642bf-7462-46bf-a8f8-a9b06e290a34",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def apply_all_features(df):\n",
    "    \"\"\"\n",
    "    Apply all feature engineering transformations in sequence.\n",
    "    \n",
    "    Args:\n",
    "        df: Input PySpark DataFrame\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with all new features added\n",
    "    \"\"\"\n",
    "    return (df\n",
    "            .transform(add_time_features)\n",
    "            .transform(add_weather_deltas)\n",
    "            .transform(add_origin_congestion_features)\n",
    "            .transform(add_dest_congestion_features))\n",
    "\n",
    "\n",
    "# Apply to all splits\n",
    "print(\"Applying feature engineering...\")\n",
    "train_df_fe = apply_all_features(train_df)\n",
    "val_df_fe = apply_all_features(val_df)\n",
    "test_df_fe = apply_all_features(test_df)\n",
    "print(\"✓ Feature engineering complete\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Save Results\n",
    "\n",
    "Save the enriched datasets to parquet for use in neural network training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39127673-372a-4bc1-84e3-4b3286df2e12",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpointed 5_year_custom_joined/fe_graph_and_holiday_nnfeat/training_splits/train\n",
      "Checkpointed 5_year_custom_joined/fe_graph_and_holiday_nnfeat/training_splits/val\n",
      "Checkpointed 5_year_custom_joined/fe_graph_and_holiday_nnfeat/training_splits/test\n"
     ]
    }
   ],
   "source": [
    "# Create output directory\n",
    "dbutils.fs.mkdirs(OUTPUT_DIR)\n",
    "\n",
    "# Save each split\n",
    "print(\"Saving datasets...\")\n",
    "train_df_fe.write.mode(\"overwrite\").parquet(f\"{OUTPUT_DIR}/train.parquet\")\n",
    "val_df_fe.write.mode(\"overwrite\").parquet(f\"{OUTPUT_DIR}/val.parquet\")\n",
    "test_df_fe.write.mode(\"overwrite\").parquet(f\"{OUTPUT_DIR}/test.parquet\")\n",
    "\n",
    "print(f\"✓ Saved to: {OUTPUT_DIR}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "This notebook adds **13 new features** specifically designed for neural network models:\n",
    "\n",
    "| Feature | Description |\n",
    "|---------|-------------|\n",
    "| `dep_hour_sin`, `dep_hour_cos` | Cyclic encoding of departure hour |\n",
    "| `dow_sin`, `dow_cos` | Cyclic encoding of day of week |\n",
    "| `doy_sin`, `doy_cos` | Cyclic encoding of day of year |\n",
    "| `HourlyVisibility_3h_change` | 3-hour change in visibility |\n",
    "| `HourlyStationPressure_3h_change` | 3-hour change in pressure |\n",
    "| `HourlyDryBulbTemperature_3h_change` | 3-hour change in temperature |\n",
    "| `HourlyWindSpeed_3h_change` | 3-hour change in wind speed |\n",
    "| `HourlyPrecipitation_3h_change` | 3-hour change in precipitation |\n",
    "| `ground_flights_last_hour` | Origin airport departure congestion |\n",
    "| `arrivals_last_hour` | Destination airport arrival congestion |\n",
    "\n",
    "### Key Design Decisions\n",
    "\n",
    "1. **Cyclic Encoding**: Sine/cosine pairs preserve the circular nature of time for neural networks\n",
    "2. **Weather Deltas**: 3-hour window captures weather trends without requiring excessive lookback\n",
    "3. **Congestion Windows**: 1-hour rolling windows balance recency with statistical stability\n",
    "4. **Null Handling**: Missing lag values return `None` rather than 0 to avoid bias"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Additional_FE_NN_MK",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

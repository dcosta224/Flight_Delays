{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "33522dac-766e-4b34-a31c-32101aef2e13",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Multi-tower neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ce9ab048-8dcf-444c-af06-6d4505303562",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d920560c-696d-4dff-b153-a60ed48f98d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.21.3\n",
      "Created new experiment with ID: 1920497510759002\n"
     ]
    }
   ],
   "source": [
    "# Standard library\n",
    "import copy\n",
    "import os\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "# Data processing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# PySpark\n",
    "from pyspark.sql import Window\n",
    "import pyspark.sql.functions as sf  # Alias to avoid conflict with torch.nn.functional\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import fbeta_score, roc_auc_score, mean_squared_error, mean_absolute_error\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# MLflow for experiment tracking\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"MLflow version: {mlflow.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "200b4f72-de11-4b6b-8980-cedcbcd80ace",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fedb30a4-2f2d-4020-95bf-1d814c25d370",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Training Configuration\n",
    "# =============================================================================\n",
    "RANDOM_SEED = 42\n",
    "NUM_EPOCHS = 20\n",
    "BATCH_SIZE = 2048\n",
    "LEARNING_RATE = 1e-3\n",
    "PATIENCE = 5  # Early stopping patience\n",
    "\n",
    "# Loss weighting: alpha * regression_loss + (1-alpha) * classification_loss\n",
    "LOSS_ALPHA = 0.5\n",
    "\n",
    "# Classification positive weight (for class imbalance)\n",
    "POS_WEIGHT = 4.0\n",
    "\n",
    "# Delay threshold for binary classification (minutes)\n",
    "DELAY_THRESHOLD = 15.0\n",
    "\n",
    "# =============================================================================\n",
    "# Data Paths\n",
    "# =============================================================================\n",
    "BASE_PATH = \"dbfs:/student-groups/Group_2_2\"\n",
    "TRAIN_PATH = f\"{BASE_PATH}/1_year_custom_joined/fe_graph_and_holiday_nnfeat/training_splits/train.parquet/\"\n",
    "VAL_PATH = f\"{BASE_PATH}/1_year_custom_joined/fe_graph_and_holiday_nnfeat/training_splits/val.parquet/\"\n",
    "TEST_PATH = f\"{BASE_PATH}/1_year_custom_joined/fe_graph_and_holiday_nnfeat/training_splits/test.parquet/\"\n",
    "CV_DATA_PATH = f\"{BASE_PATH}/1_year_custom_joined/fe_graph_and_holiday_nnfeat/cv_splits\"\n",
    "PREDICTIONS_PATH = f\"{BASE_PATH}/1_year_custom_joined/nn_predictions\"\n",
    "\n",
    "# =============================================================================\n",
    "# MLflow Configuration\n",
    "# =============================================================================\n",
    "EXPERIMENT_NAME = \"/Shared/team_2_2/mlflow-nn-tower-hyperparameters\"\n",
    "\n",
    "# =============================================================================\n",
    "# Feature Definitions\n",
    "# =============================================================================\n",
    "\n",
    "# Categorical features (for learned embeddings)\n",
    "CATEGORICAL_COLS = [\n",
    "    \"OP_UNIQUE_CARRIER\",       # Airline carrier code\n",
    "    \"ORIGIN_AIRPORT_SEQ_ID\",   # Origin airport\n",
    "    \"DEST_AIRPORT_SEQ_ID\",     # Destination airport\n",
    "    \"route\",                   # Route string (origin-dest pair)\n",
    "    \"AIRPORT_HUB_CLASS\",       # Hub classification\n",
    "    \"AIRLINE_CATEGORY\",        # Airline type\n",
    "]\n",
    "\n",
    "# Numerical features (to be standardized)\n",
    "NUMERICAL_COLS = [\n",
    "    # Flight characteristics\n",
    "    \"DISTANCE\",\n",
    "    \"CRS_ELAPSED_TIME\",\n",
    "    \n",
    "    # Historical delay features (Phase 2 Feature Engineering)\n",
    "    \"prev_flight_delay_in_minutes\",\n",
    "    \"origin_delays_4h\",\n",
    "    \"delay_origin_7d\",\n",
    "    \"delay_origin_carrier_7d\",\n",
    "    \"delay_route_7d\",\n",
    "    \"flight_count_24h\",\n",
    "    \"AVG_TAXI_OUT_ORIGIN\",\n",
    "    \"AVG_ARR_DELAY_ORIGIN\",\n",
    "    \n",
    "    # Graph-based airport features\n",
    "    \"in_degree\",\n",
    "    \"out_degree\",\n",
    "    \"weighted_in_degree\",\n",
    "    \"weighted_out_degree\",\n",
    "    \"betweenness\",\n",
    "    \"closeness\",\n",
    "    \"N_RUNWAYS\",\n",
    "    \n",
    "    # Weather features\n",
    "    \"HourlyVisibility\",\n",
    "    \"HourlyStationPressure\",\n",
    "    \"HourlyWindSpeed\",\n",
    "    \"HourlyDryBulbTemperature\",\n",
    "    \"HourlyDewPointTemperature\",\n",
    "    \"HourlyRelativeHumidity\",\n",
    "    \"HourlyAltimeterSetting\",\n",
    "    \"HourlyWetBulbTemperature\",\n",
    "    \"HourlyPrecipitation\",\n",
    "    \"HourlyCloudCoverage\",\n",
    "    \"HourlyCloudElevation\",\n",
    "    \n",
    "    # Congestion features (engineered)\n",
    "    \"ground_flights_last_hour\",\n",
    "    \"arrivals_last_hour\",\n",
    "    \n",
    "    # Cyclic time features\n",
    "    \"dow_sin\", \"dow_cos\",  # Day of week\n",
    "    \"doy_sin\", \"doy_cos\",  # Day of year\n",
    "]\n",
    "\n",
    "# Time feature for Time2Vec encoding\n",
    "TIME_COL = \"CRS_DEP_MINUTES\"\n",
    "\n",
    "# Target variable\n",
    "TARGET_COL = \"DEP_DELAY_NEW\"\n",
    "\n",
    "# Device configuration\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "60cc9a59-141d-4ac4-8867-abe88459507b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Datasets (Custom Join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d11348a9-4157-45eb-8e8e-a748e292c6d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th><th>modificationTime</th></tr></thead><tbody><tr><td>dbfs:/student-groups/Group_2_2/3_month_custom_joined/fe_graph_and_holiday/cv_splits/</td><td>cv_splits/</td><td>0</td><td>1765328209514</td></tr><tr><td>dbfs:/student-groups/Group_2_2/3_month_custom_joined/fe_graph_and_holiday/stacked_input_optimized/</td><td>stacked_input_optimized/</td><td>0</td><td>1765328209514</td></tr><tr><td>dbfs:/student-groups/Group_2_2/3_month_custom_joined/fe_graph_and_holiday/training_splits/</td><td>training_splits/</td><td>0</td><td>1765328209514</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "dbfs:/student-groups/Group_2_2/3_month_custom_joined/fe_graph_and_holiday/cv_splits/",
         "cv_splits/",
         0,
         1765328209514
        ],
        [
         "dbfs:/student-groups/Group_2_2/3_month_custom_joined/fe_graph_and_holiday/stacked_input_optimized/",
         "stacked_input_optimized/",
         0,
         1765328209514
        ],
        [
         "dbfs:/student-groups/Group_2_2/3_month_custom_joined/fe_graph_and_holiday/training_splits/",
         "training_splits/",
         0,
         1765328209514
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "path",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "size",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "modificationTime",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load pre-processed datasets with engineered features\n",
    "train_df = spark.read.parquet(TRAIN_PATH)\n",
    "val_df = spark.read.parquet(VAL_PATH)\n",
    "test_df = spark.read.parquet(TEST_PATH)\n",
    "\n",
    "print(f\"Training set:   {train_df.count():,} records\")\n",
    "print(f\"Validation set: {val_df.count():,} records\")\n",
    "print(f\"Test set:       {test_df.count():,} records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69ccf366-f086-406a-8afa-182831eb7c87",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# For cross-validation, load the CV splits dataset\n",
    "cv_df = spark.read.parquet(CV_DATA_PATH)\n",
    "folds = sorted([row['fold_id'] for row in cv_df.select(\"fold_id\").distinct().collect()])\n",
    "print(f\"Cross-validation folds available: {folds}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9b269132-a156-47bc-af76-95ba33f9a892",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a01576fb-da14-4197-9176-d28ee41b7c5c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def add_time_features(df):\n",
    "    \"\"\"\n",
    "    Add cyclic time features using sine/cosine encoding.\n",
    "    \n",
    "    This encoding preserves the cyclical nature of time (e.g., hour 23 is close to hour 0).\n",
    "    \n",
    "    Features added:\n",
    "        - dep_hour_sin/cos: Hour of day (24-hour cycle)\n",
    "        - dow_sin/cos: Day of week (7-day cycle)\n",
    "        - doy_sin/cos: Day of year (365-day cycle)\n",
    "    \"\"\"\n",
    "    df = df.withColumn(\"dep_hour\", sf.col(\"CRS_DEP_MINUTES\") / 60.0)\n",
    "    df = df.withColumn(\"day_of_year\", sf.dayofyear(\"utc_timestamp\").cast(\"double\"))\n",
    "    \n",
    "    # Cyclic encoding: x -> (sin(2π * x / period), cos(2π * x / period))\n",
    "    df = df.withColumn(\"dep_hour_sin\", sf.sin(2 * sf.lit(np.pi) * sf.col(\"dep_hour\") / 24))\n",
    "    df = df.withColumn(\"dep_hour_cos\", sf.cos(2 * sf.lit(np.pi) * sf.col(\"dep_hour\") / 24))\n",
    "    df = df.withColumn(\"dow_sin\", sf.sin(2 * sf.lit(np.pi) * sf.col(\"DAY_OF_WEEK\") / 7))\n",
    "    df = df.withColumn(\"dow_cos\", sf.cos(2 * sf.lit(np.pi) * sf.col(\"DAY_OF_WEEK\") / 7))\n",
    "    df = df.withColumn(\"doy_sin\", sf.sin(2 * sf.lit(np.pi) * sf.col(\"day_of_year\") / 365))\n",
    "    df = df.withColumn(\"doy_cos\", sf.cos(2 * sf.lit(np.pi) * sf.col(\"day_of_year\") / 365))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23237986-6977-4cd5-9dce-f4146dd24b82",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def add_weather_deltas(df):\n",
    "    \"\"\"\n",
    "    Add 3-hour weather change features to capture weather trends.\n",
    "    \n",
    "    Computes the difference between current weather values and values from 3 hours ago\n",
    "    at the same airport. Useful for detecting deteriorating or improving conditions.\n",
    "    \n",
    "    Features added (for each weather metric):\n",
    "        - {metric}_3h_change: Change from 3 hours prior\n",
    "    \"\"\"\n",
    "    WEATHER_COLS = [\n",
    "        \"HourlyVisibility\", \n",
    "        \"HourlyStationPressure\",\n",
    "        \"HourlyDryBulbTemperature\", \n",
    "        \"HourlyWindSpeed\",\n",
    "        \"HourlyPrecipitation\"\n",
    "    ]\n",
    "    \n",
    "    window = Window.partitionBy(\"ORIGIN_AIRPORT_SEQ_ID\").orderBy(\"utc_timestamp\")\n",
    "    \n",
    "    for col_name in WEATHER_COLS:\n",
    "        lag_col = sf.lag(col_name, 3).over(window)\n",
    "        delta = sf.col(col_name) - lag_col\n",
    "        df = df.withColumn(\n",
    "            f\"{col_name}_3h_change\",\n",
    "            sf.when(lag_col.isNull(), sf.lit(None)).otherwise(delta)\n",
    "        )\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "888ab275-70cc-4254-bdf7-d8b13159ce4d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def add_congestion_features(df):\n",
    "    \"\"\"\n",
    "    Add origin airport congestion feature.\n",
    "    \n",
    "    Counts the number of flights departing from the same origin airport\n",
    "    in the hour preceding the current flight (excluding the current flight).\n",
    "    \n",
    "    Features added:\n",
    "        - ground_flights_last_hour: Count of departures in past hour\n",
    "    \"\"\"\n",
    "    df = df.withColumn(\"utc_ts_sec\", sf.col(\"utc_timestamp\").cast(\"long\"))\n",
    "    \n",
    "    window = (Window\n",
    "              .partitionBy(\"ORIGIN_AIRPORT_SEQ_ID\")\n",
    "              .orderBy(\"utc_ts_sec\")\n",
    "              .rangeBetween(-3600, 0))  # 1 hour = 3600 seconds\n",
    "    \n",
    "    df = df.withColumn(\n",
    "        \"ground_flights_last_hour\",\n",
    "        sf.count(\"utc_ts_sec\").over(window) - 1  # Exclude current flight\n",
    "    )\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47f438f3-d136-4930-a04d-903c393d5d62",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def add_dest_congestion_features(df):\n",
    "    \"\"\"\n",
    "    Add destination airport congestion feature.\n",
    "    \n",
    "    Counts the number of flights arriving at the destination airport\n",
    "    in the hour preceding the current flight's scheduled departure.\n",
    "    \n",
    "    Features added:\n",
    "        - arrivals_last_hour: Count of arrivals at destination in past hour\n",
    "    \"\"\"\n",
    "    df = df.withColumn(\"utc_ts_sec\", sf.col(\"utc_timestamp\").cast(\"long\"))\n",
    "    \n",
    "    window = (Window\n",
    "              .partitionBy(\"DEST_AIRPORT_SEQ_ID\")\n",
    "              .orderBy(\"utc_ts_sec\")\n",
    "              .rangeBetween(-3600, 0))\n",
    "    \n",
    "    df = df.withColumn(\n",
    "        \"arrivals_last_hour\",\n",
    "        sf.count(\"utc_ts_sec\").over(window) - 1\n",
    "    )\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def apply_feature_engineering(df):\n",
    "    \"\"\"Apply all feature engineering transformations to a DataFrame.\"\"\"\n",
    "    return (df\n",
    "            .transform(add_time_features)\n",
    "            .transform(add_weather_deltas)\n",
    "            .transform(add_congestion_features)\n",
    "            .transform(add_dest_congestion_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b82b8b0a-7033-4560-b919-b5db69c8d055",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Apply feature engineering to all datasets\n",
    "train_df_fe = apply_feature_engineering(train_df)\n",
    "val_df_fe = apply_feature_engineering(val_df)\n",
    "test_df_fe = apply_feature_engineering(test_df)\n",
    "\n",
    "print(\"Feature engineering complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5cbefcee-b735-48ca-95b2-6e163e535fce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpointed 1_year_custom_joined/fe_graph_and_holiday_nnfeat/training_splits/train\n",
      "Checkpointed 1_year_custom_joined/fe_graph_and_holiday_nnfeat/training_splits/val\n",
      "Checkpointed 1_year_custom_joined/fe_graph_and_holiday_nnfeat/training_splits/test\n"
     ]
    }
   ],
   "source": [
    "<a name=\"preprocessing\"></a>\n",
    "## 4. Data Preprocessing\n",
    "\n",
    "### 4.1 Categorical Encoding\n",
    "\n",
    "Build category-to-index mappings from training data only, then apply to all datasets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e4f3792d-59b4-4cd5-a22b-061e8f4158ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Start here when running experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba7f898b-923b-4e98-b3a4-6c5b7fd232e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def build_category_maps(df: pd.DataFrame, categorical_cols: List[str]) -> Dict[str, Dict]:\n",
    "    \"\"\"\n",
    "    Build category-to-integer mappings from training data.\n",
    "    \n",
    "    Args:\n",
    "        df: Training DataFrame (Pandas)\n",
    "        categorical_cols: List of categorical column names\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary mapping column names to {category: index} dictionaries.\n",
    "        Index 0 is reserved for unknown (UNK) categories.\n",
    "    \"\"\"\n",
    "    cat_maps = {}\n",
    "    for col in categorical_cols:\n",
    "        unique_vals = df[col].astype(str).unique().tolist()\n",
    "        cat_maps[col] = {\"UNK\": 0, **{v: i + 1 for i, v in enumerate(sorted(unique_vals))}}\n",
    "    return cat_maps\n",
    "\n",
    "\n",
    "def apply_category_maps(df: pd.DataFrame, cat_maps: Dict, categorical_cols: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Encode categorical columns using pre-built mappings.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame to encode\n",
    "        cat_maps: Category mappings from build_category_maps()\n",
    "        categorical_cols: List of categorical column names\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with encoded categorical columns (unseen values -> 0)\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    for col in categorical_cols:\n",
    "        mapping = cat_maps[col]\n",
    "        df[col] = df[col].astype(str).apply(lambda x: mapping.get(x, 0))\n",
    "    return df\n",
    "\n",
    "\n",
    "def compute_embedding_dims(cat_maps: Dict, categorical_cols: List[str]) -> Tuple[List[int], List[int]]:\n",
    "    \"\"\"\n",
    "    Compute embedding dimensions using the rule of thumb: min(64, cardinality^0.3).\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (category_dimensions, embedding_dimensions)\n",
    "    \"\"\"\n",
    "    cat_dims = [len(cat_maps[c]) for c in categorical_cols]\n",
    "    emb_dims = [min(64, int(n ** 0.3)) for n in cat_dims]\n",
    "    return cat_dims, emb_dims"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8979212d-e8dc-4fda-817c-46260ec1f81f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Vectorization for Torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b37de593-0380-4d7f-8bd5-ec6ef6f1db48",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class FlightDataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset for flight delay prediction.\n",
    "    \n",
    "    Handles three input types:\n",
    "        - Categorical features (as long tensors for embeddings)\n",
    "        - Numerical features (as float tensors)\n",
    "        - Time feature (for Time2Vec encoding)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, df: pd.DataFrame, categorical_cols: List[str], \n",
    "                 numerical_cols: List[str], time_col: str, target_col: str):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            df: Preprocessed Pandas DataFrame\n",
    "            categorical_cols: List of categorical column names\n",
    "            numerical_cols: List of numerical column names\n",
    "            time_col: Name of the time column\n",
    "            target_col: Name of the target column\n",
    "        \"\"\"\n",
    "        self.cat = torch.tensor(df[categorical_cols].values, dtype=torch.long)\n",
    "        self.num = torch.tensor(df[numerical_cols].values, dtype=torch.float32)\n",
    "        self.time = torch.tensor(df[time_col].values, dtype=torch.float32).unsqueeze(1)\n",
    "        self.y = torch.tensor(df[target_col].values, dtype=torch.float32).unsqueeze(1)\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, ...]:\n",
    "        return self.cat[idx], self.num[idx], self.time[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d11bc6d-a016-4a03-8654-285e859b2f50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def prepare_data_for_training(\n",
    "    train_spark_df, \n",
    "    val_spark_df,\n",
    "    categorical_cols: List[str],\n",
    "    numerical_cols: List[str],\n",
    "    time_col: str,\n",
    "    target_col: str\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame, Dict, StandardScaler, List[int], List[int]]:\n",
    "    \"\"\"\n",
    "    Complete data preparation pipeline: Spark -> Pandas -> encoded/scaled.\n",
    "    \n",
    "    Fits encoders and scaler on training data only, then transforms both sets.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (train_pd, val_pd, cat_maps, scaler, cat_dims, emb_dims)\n",
    "    \"\"\"\n",
    "    # Select relevant columns and convert to Pandas\n",
    "    all_cols = categorical_cols + numerical_cols + [time_col, target_col]\n",
    "    train_pd = train_spark_df.select(all_cols).toPandas()\n",
    "    val_pd = val_spark_df.select(all_cols).toPandas()\n",
    "    \n",
    "    # Build and apply categorical encodings (fit on train only)\n",
    "    cat_maps = build_category_maps(train_pd, categorical_cols)\n",
    "    train_pd = apply_category_maps(train_pd, cat_maps, categorical_cols)\n",
    "    val_pd = apply_category_maps(val_pd, cat_maps, categorical_cols)\n",
    "    \n",
    "    # Fit and apply standard scaling (fit on train only)\n",
    "    scaler = StandardScaler()\n",
    "    train_pd[numerical_cols] = scaler.fit_transform(train_pd[numerical_cols])\n",
    "    val_pd[numerical_cols] = scaler.transform(val_pd[numerical_cols])\n",
    "    \n",
    "    # Compute embedding dimensions\n",
    "    cat_dims, emb_dims = compute_embedding_dims(cat_maps, categorical_cols)\n",
    "    \n",
    "    return train_pd, val_pd, cat_maps, scaler, cat_dims, emb_dims\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "43519b71-56cc-47f7-817b-0ec1cefece77",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Encode categoricals; build encoder dictionary for train data only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6aaf9a0-ea34-4070-a2ce-9f3761241b1b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class ResBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Residual block with LayerNorm and GELU activation.\n",
    "    \n",
    "    Architecture: LayerNorm -> Linear -> GELU -> Dropout -> Linear -> Add residual\n",
    "    \n",
    "    This allows the network to learn identity mappings easily while enabling\n",
    "    deeper architectures without vanishing gradients.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, dim: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.ln = nn.LayerNorm(dim)\n",
    "        self.fc1 = nn.Linear(dim, dim)\n",
    "        self.fc2 = nn.Linear(dim, dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        h = F.gelu(self.fc1(self.ln(x)))\n",
    "        h = self.dropout(h)\n",
    "        h = self.fc2(h)\n",
    "        return x + h  # Residual connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9b517d9-e564-4c73-9766-e770beb92100",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class Time2Vec(nn.Module):\n",
    "    \"\"\"\n",
    "    Time2Vec: Learnable time encoding layer.\n",
    "    \n",
    "    Learns both linear and periodic representations of time, allowing the model\n",
    "    to capture trends and cyclical patterns (daily, weekly, seasonal).\n",
    "    \n",
    "    Output: [linear_component, sin(learned_frequencies * t)]\n",
    "    \n",
    "    Reference: Kazemi et al., \"Time2Vec: Learning a Vector Representation of Time\" (2019)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, k: int):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            k: Number of periodic components (output dim = k + 1)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.wb = nn.Linear(1, 1)  # Linear trend\n",
    "        self.ws = nn.Linear(1, k)  # Periodic components\n",
    "    \n",
    "    def forward(self, t: torch.Tensor) -> torch.Tensor:\n",
    "        linear = self.wb(t)\n",
    "        periodic = torch.sin(self.ws(t))\n",
    "        return torch.cat([linear, periodic], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd170515-06f2-4a2c-b78e-8c108558b9a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Apply to train/val/test\n",
    "# -----------------------------\n",
    "# Build mapping dicts from training data only\n",
    "cat_maps = build_category_maps(train_pd, categorical_cols)\n",
    "\n",
    "# Apply safely to all datasets\n",
    "train_pd = apply_category_maps(train_pd, cat_maps, categorical_cols)\n",
    "val_pd   = apply_category_maps(val_pd, cat_maps, categorical_cols)\n",
    "test_pd  = apply_category_maps(test_pd, cat_maps, categorical_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0680a083-0e97-4fb4-9743-d72641610b40",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OP_UNIQUE_CARRIER: 18 categories -> embedding dim 2\n",
      "ORIGIN_AIRPORT_SEQ_ID: 373 categories -> embedding dim 5\n",
      "DEST_AIRPORT_SEQ_ID: 388 categories -> embedding dim 5\n",
      "route: 6252 categories -> embedding dim 13\n",
      "AIRPORT_HUB_CLASS: 7 categories -> embedding dim 1\n",
      "AIRLINE_CATEGORY: 4 categories -> embedding dim 1\n"
     ]
    }
   ],
   "source": [
    "class ResFiLMMLP(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Tower Neural Network with Feature-wise Linear Modulation (FiLM).\n",
    "    \n",
    "    Architecture:\n",
    "    ┌─────────────────┐     ┌─────────────────┐     ┌─────────────────┐\n",
    "    │  Embedding      │     │   Numeric       │     │   Time2Vec      │\n",
    "    │  Tower          │     │   Tower         │     │   Encoder       │\n",
    "    │  (Categorical)  │     │   (ResBlocks)   │     │   (Departure)   │\n",
    "    └────────┬────────┘     └────────┬────────┘     └────────┬────────┘\n",
    "             │                       │                       │\n",
    "             │    ┌──────────────────┘                       │\n",
    "             │    │  FiLM Modulation                         │\n",
    "             │    │  γ * emb + β                             │\n",
    "             ▼    ▼                                          │\n",
    "    ┌─────────────────┐                                      │\n",
    "    │  Modulated      │◄─────────────────────────────────────┘\n",
    "    │  Embeddings     │\n",
    "    └────────┬────────┘\n",
    "             │\n",
    "             ▼\n",
    "    ┌─────────────────────────────────────────────────────────┐\n",
    "    │              Concatenated Features                       │\n",
    "    └────────────────────────┬────────────────────────────────┘\n",
    "                             │\n",
    "             ┌───────────────┴───────────────┐\n",
    "             ▼                               ▼\n",
    "    ┌─────────────────┐             ┌─────────────────┐\n",
    "    │  Regression     │             │  Classification │\n",
    "    │  Head (MAE)     │             │  Head (BCE)     │\n",
    "    │  → Delay mins   │             │  → P(delayed)   │\n",
    "    └─────────────────┘             └─────────────────┘\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        cat_dims: List[int],\n",
    "        emb_dims: List[int],\n",
    "        num_numerical: int,\n",
    "        time_dim: int = 8,\n",
    "        hidden_dim: int = 256,\n",
    "        num_res_blocks: int = 4,\n",
    "        emb_dropout: float = 0.05,\n",
    "        num_dropout: float = 0.1,\n",
    "        film_dropout: float = 0.1,\n",
    "        final_dropout: float = 0.2\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            cat_dims: Number of categories for each categorical feature\n",
    "            emb_dims: Embedding dimension for each categorical feature\n",
    "            num_numerical: Number of numerical features\n",
    "            time_dim: Dimension of Time2Vec periodic components\n",
    "            hidden_dim: Hidden dimension for numeric tower\n",
    "            num_res_blocks: Number of residual blocks in numeric tower\n",
    "            emb_dropout: Dropout rate for embeddings\n",
    "            num_dropout: Dropout rate in residual blocks\n",
    "            film_dropout: Dropout rate for FiLM parameters\n",
    "            final_dropout: Dropout rate in prediction heads\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # === Embedding Tower ===\n",
    "        self.embeddings = nn.ModuleList([\n",
    "            nn.Embedding(cat_dim, emb_dim)\n",
    "            for cat_dim, emb_dim in zip(cat_dims, emb_dims)\n",
    "        ])\n",
    "        self.emb_total = sum(emb_dims)\n",
    "        self.emb_dropout = nn.Dropout(emb_dropout)\n",
    "        \n",
    "        # === Numeric Tower ===\n",
    "        self.fc_num = nn.Linear(num_numerical, hidden_dim)\n",
    "        self.res_blocks = nn.ModuleList([\n",
    "            ResBlock(hidden_dim, dropout=num_dropout)\n",
    "            for _ in range(num_res_blocks)\n",
    "        ])\n",
    "        \n",
    "        # === FiLM Layer ===\n",
    "        # Learns scale (γ) and shift (β) for embedding modulation\n",
    "        self.film = nn.Linear(hidden_dim, 2 * self.emb_total)\n",
    "        self.film_dropout = nn.Dropout(film_dropout)\n",
    "        \n",
    "        # === Time2Vec ===\n",
    "        self.t2v = Time2Vec(time_dim)\n",
    "        \n",
    "        # === Fusion Dimension ===\n",
    "        # modulated_emb + numeric_hidden + time2vec + raw_time\n",
    "        fused_dim = self.emb_total + hidden_dim + (time_dim + 1) + 1\n",
    "        \n",
    "        # === Regression Head (delay minutes) ===\n",
    "        self.reg_head = nn.Sequential(\n",
    "            nn.Linear(fused_dim, 256),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(final_dropout),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(final_dropout),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "        \n",
    "        # === Classification Head (delayed yes/no) ===\n",
    "        self.clf_head = nn.Sequential(\n",
    "            nn.Linear(fused_dim, 256),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(final_dropout),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(final_dropout),\n",
    "            nn.Linear(128, 1)  # Raw logit (apply sigmoid for probability)\n",
    "        )\n",
    "    \n",
    "    def forward(\n",
    "        self, \n",
    "        x_cat: torch.Tensor, \n",
    "        x_num: torch.Tensor, \n",
    "        x_time: torch.Tensor\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "        \n",
    "        Args:\n",
    "            x_cat: Categorical features [batch, num_cat_features]\n",
    "            x_num: Numerical features [batch, num_numerical]\n",
    "            x_time: Time feature [batch, 1]\n",
    "        \n",
    "        Returns:\n",
    "            Tuple of (regression_output, classification_logit)\n",
    "        \"\"\"\n",
    "        # Embedding tower\n",
    "        emb = [emb_layer(x_cat[:, i]) for i, emb_layer in enumerate(self.embeddings)]\n",
    "        emb = torch.cat(emb, dim=-1)\n",
    "        emb = self.emb_dropout(emb)\n",
    "        \n",
    "        # Numeric tower\n",
    "        h = F.gelu(self.fc_num(x_num))\n",
    "        for block in self.res_blocks:\n",
    "            h = block(h)\n",
    "        \n",
    "        # FiLM modulation: γ * emb + β\n",
    "        gamma, beta = torch.chunk(self.film(h), 2, dim=-1)\n",
    "        gamma = self.film_dropout(gamma)\n",
    "        beta = self.film_dropout(beta)\n",
    "        emb_mod = gamma * emb + beta\n",
    "        \n",
    "        # Time2Vec encoding\n",
    "        t_feat = self.t2v(x_time)\n",
    "        \n",
    "        # Concatenate all features\n",
    "        z = torch.cat([emb_mod, h, t_feat, x_time], dim=-1)\n",
    "        \n",
    "        # Dual prediction heads\n",
    "        reg_out = self.reg_head(z)\n",
    "        clf_out = self.clf_head(z)\n",
    "        \n",
    "        return reg_out, clf_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6eea0405-cb07-4b90-9f7a-2b8dda4cd0a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Normalize numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "550a1ca0-757a-4d02-ad82-3e8630330b67",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(\n",
    "    model: nn.Module,\n",
    "    dataloader: DataLoader,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    criterion_reg: nn.Module,\n",
    "    criterion_clf: nn.Module,\n",
    "    device: torch.device,\n",
    "    alpha: float = 0.5,\n",
    "    delay_threshold: float = 15.0\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Train model for one epoch.\n",
    "    \n",
    "    Args:\n",
    "        model: The neural network model\n",
    "        dataloader: Training data loader\n",
    "        optimizer: Optimizer instance\n",
    "        criterion_reg: Regression loss function (e.g., L1Loss)\n",
    "        criterion_clf: Classification loss function (e.g., BCEWithLogitsLoss)\n",
    "        device: Device to train on\n",
    "        alpha: Weight for regression loss (1-alpha for classification)\n",
    "        delay_threshold: Threshold in minutes for binary classification\n",
    "    \n",
    "    Returns:\n",
    "        Average loss for the epoch\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    for cat, num, time, y in dataloader:\n",
    "        cat, num, time, y = cat.to(device), num.to(device), time.to(device), y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        reg_out, clf_out = model(cat, num, time)\n",
    "        \n",
    "        # Binary target: delayed if >= threshold\n",
    "        y_bin = (y >= delay_threshold).float()\n",
    "        \n",
    "        # Combined loss\n",
    "        loss_reg = criterion_reg(reg_out, y)\n",
    "        loss_clf = criterion_clf(clf_out, y_bin)\n",
    "        loss = alpha * loss_reg + (1 - alpha) * loss_clf\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e2ba31f3-4799-4ede-9bc5-07cc522dcc55",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Load Torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33b9b3d0-7033-4223-beea-9b86a39f3b53",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_model(\n",
    "    model: nn.Module,\n",
    "    dataloader: DataLoader,\n",
    "    device: torch.device,\n",
    "    delay_threshold: float = 15.0\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Evaluate model on multiple metrics.\n",
    "    \n",
    "    Args:\n",
    "        model: The neural network model\n",
    "        dataloader: Evaluation data loader\n",
    "        device: Device to evaluate on\n",
    "        delay_threshold: Threshold in minutes for binary classification\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with MAE, RMSE, AUC, and F2 score\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    preds_reg, preds_clf = [], []\n",
    "    targets_reg, targets_clf = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for cat, num, time, y in dataloader:\n",
    "            cat, num, time, y = cat.to(device), num.to(device), time.to(device), y.to(device)\n",
    "            reg_out, clf_out = model(cat, num, time)\n",
    "            \n",
    "            preds_reg.append(reg_out.cpu())\n",
    "            preds_clf.append(torch.sigmoid(clf_out).cpu())\n",
    "            targets_reg.append(y.cpu())\n",
    "            targets_clf.append((y >= delay_threshold).float().cpu())\n",
    "    \n",
    "    # Concatenate all batches\n",
    "    y_pred_reg = torch.cat(preds_reg).numpy()\n",
    "    y_pred_clf = torch.cat(preds_clf).numpy()\n",
    "    y_true_reg = torch.cat(targets_reg).numpy()\n",
    "    y_true_clf = torch.cat(targets_clf).numpy()\n",
    "    \n",
    "    # Compute metrics\n",
    "    mae = mean_absolute_error(y_true_reg, y_pred_reg)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true_reg, y_pred_reg))\n",
    "    \n",
    "    try:\n",
    "        auc = roc_auc_score(y_true_clf, y_pred_clf)\n",
    "    except ValueError:\n",
    "        auc = 0.5  # Handle edge case with single class\n",
    "    \n",
    "    # F2 score at threshold 0.5\n",
    "    y_pred_bin = (y_pred_clf > 0.5).astype(int)\n",
    "    f2 = fbeta_score(y_true_clf, y_pred_bin, beta=2)\n",
    "    \n",
    "    return {\"mae\": mae, \"rmse\": rmse, \"auc\": auc, \"f2_score\": f2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e5f3921-7aa5-4743-94e4-bd224dac29c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def prepare_fold_data(\n",
    "    fold_df,\n",
    "    categorical_cols: List[str],\n",
    "    numerical_cols: List[str],\n",
    "    time_col: str,\n",
    "    target_col: str\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame, List[int], List[int]]:\n",
    "    \"\"\"\n",
    "    Prepare data for a single cross-validation fold.\n",
    "    \n",
    "    Applies feature engineering, encoding, and scaling to train/validation splits.\n",
    "    \n",
    "    Args:\n",
    "        fold_df: Spark DataFrame containing the fold data with 'split_type' column\n",
    "        categorical_cols: List of categorical column names\n",
    "        numerical_cols: List of numerical column names\n",
    "        time_col: Name of time column\n",
    "        target_col: Name of target column\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (train_pd, val_pd, cat_dims, emb_dims)\n",
    "    \"\"\"\n",
    "    # Split by split_type (excludes 'gap' data)\n",
    "    train_spark = fold_df.filter(sf.col(\"split_type\") == \"train\")\n",
    "    val_spark = fold_df.filter(sf.col(\"split_type\") == \"validation\")\n",
    "    \n",
    "    # Apply feature engineering\n",
    "    train_fe = apply_feature_engineering(train_spark)\n",
    "    val_fe = apply_feature_engineering(val_spark)\n",
    "    \n",
    "    # Convert to Pandas and preprocess\n",
    "    all_cols = categorical_cols + numerical_cols + [time_col, target_col]\n",
    "    train_pd = train_fe.select(all_cols).toPandas()\n",
    "    val_pd = val_fe.select(all_cols).toPandas()\n",
    "    \n",
    "    # Encode categoricals (fit on train)\n",
    "    cat_maps = build_category_maps(train_pd, categorical_cols)\n",
    "    train_pd = apply_category_maps(train_pd, cat_maps, categorical_cols)\n",
    "    val_pd = apply_category_maps(val_pd, cat_maps, categorical_cols)\n",
    "    \n",
    "    # Scale numericals (fit on train)\n",
    "    scaler = StandardScaler()\n",
    "    train_pd[numerical_cols] = scaler.fit_transform(train_pd[numerical_cols])\n",
    "    val_pd[numerical_cols] = scaler.transform(val_pd[numerical_cols])\n",
    "    \n",
    "    # Compute embedding dimensions\n",
    "    cat_dims, emb_dims = compute_embedding_dims(cat_maps, categorical_cols)\n",
    "    \n",
    "    return train_pd, val_pd, cat_dims, emb_dims\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4554b9cd-652e-4cb9-95ee-da064b2e30ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Model Definition (ResFiLM-MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc4f85fa-958c-40af-8c58-08ff25274141",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def run_cross_validation(\n",
    "    cv_df,\n",
    "    folds: List[int],\n",
    "    categorical_cols: List[str],\n",
    "    numerical_cols: List[str],\n",
    "    time_col: str,\n",
    "    target_col: str,\n",
    "    config: Dict\n",
    ") -> Dict[str, List[float]]:\n",
    "    \"\"\"\n",
    "    Run k-fold cross-validation with MLflow tracking.\n",
    "    \n",
    "    Args:\n",
    "        cv_df: Spark DataFrame with all CV folds\n",
    "        folds: List of fold IDs to process\n",
    "        categorical_cols: Categorical feature columns\n",
    "        numerical_cols: Numerical feature columns\n",
    "        time_col: Time column name\n",
    "        target_col: Target column name\n",
    "        config: Training configuration dict\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with CV summary metrics\n",
    "    \"\"\"\n",
    "    cv_summary = {\"best_val_f2\": [], \"best_val_mae\": []}\n",
    "    \n",
    "    mlflow.end_run()  # End any existing run\n",
    "    \n",
    "    with mlflow.start_run(run_name=\"CV_Orchestrator_ResFiLM_MLP\") as parent_run:\n",
    "        mlflow.log_params({\n",
    "            \"num_epochs\": config[\"num_epochs\"],\n",
    "            \"batch_size\": config[\"batch_size\"],\n",
    "            \"learning_rate\": config[\"learning_rate\"],\n",
    "            \"patience\": config[\"patience\"],\n",
    "            \"loss_alpha\": config[\"alpha\"],\n",
    "            \"delay_threshold\": config[\"delay_threshold\"]\n",
    "        })\n",
    "        \n",
    "        for fold in folds:\n",
    "            print(f\"\\n{'='*50}\")\n",
    "            print(f\"Starting Fold {fold}\")\n",
    "            print(f\"{'='*50}\")\n",
    "            \n",
    "            with mlflow.start_run(run_name=f\"Fold_{fold}\", nested=True):\n",
    "                # Prepare fold data\n",
    "                fold_data = cv_df.filter(sf.col(\"fold_id\") == fold)\n",
    "                train_pd, val_pd, cat_dims, emb_dims = prepare_fold_data(\n",
    "                    fold_data, categorical_cols, numerical_cols, time_col, target_col\n",
    "                )\n",
    "                \n",
    "                # Create data loaders\n",
    "                train_ds = FlightDataset(train_pd, categorical_cols, numerical_cols, time_col, target_col)\n",
    "                val_ds = FlightDataset(val_pd, categorical_cols, numerical_cols, time_col, target_col)\n",
    "                train_dl = DataLoader(train_ds, batch_size=config[\"batch_size\"], shuffle=True)\n",
    "                val_dl = DataLoader(val_ds, batch_size=config[\"batch_size\"])\n",
    "                \n",
    "                # Initialize model\n",
    "                model = ResFiLMMLP(\n",
    "                    cat_dims=cat_dims,\n",
    "                    emb_dims=emb_dims,\n",
    "                    num_numerical=len(numerical_cols),\n",
    "                    time_dim=8\n",
    "                ).to(config[\"device\"])\n",
    "                \n",
    "                optimizer = torch.optim.AdamW(model.parameters(), lr=config[\"learning_rate\"])\n",
    "                criterion_reg = nn.L1Loss()\n",
    "                criterion_clf = nn.BCEWithLogitsLoss(\n",
    "                    pos_weight=torch.tensor([config[\"pos_weight\"]]).to(config[\"device\"])\n",
    "                )\n",
    "                \n",
    "                # Training loop with early stopping\n",
    "                best_f2 = -1.0\n",
    "                best_mae = float('inf')\n",
    "                best_model_state = None\n",
    "                early_stop_counter = 0\n",
    "                \n",
    "                for epoch in range(config[\"num_epochs\"]):\n",
    "                    # Train\n",
    "                    train_loss = train_one_epoch(\n",
    "                        model, train_dl, optimizer, criterion_reg, criterion_clf,\n",
    "                        config[\"device\"], config[\"alpha\"], config[\"delay_threshold\"]\n",
    "                    )\n",
    "                    \n",
    "                    # Evaluate\n",
    "                    train_metrics = evaluate_model(model, train_dl, config[\"device\"], config[\"delay_threshold\"])\n",
    "                    val_metrics = evaluate_model(model, val_dl, config[\"device\"], config[\"delay_threshold\"])\n",
    "                    \n",
    "                    print(f\"  Epoch {epoch:2d} | \"\n",
    "                          f\"Train F2: {train_metrics['f2_score']:.3f} | \"\n",
    "                          f\"Val F2: {val_metrics['f2_score']:.3f} | \"\n",
    "                          f\"Val MAE: {val_metrics['mae']:.2f}\")\n",
    "                    \n",
    "                    # Log metrics\n",
    "                    mlflow.log_metrics({\n",
    "                        \"train_loss\": train_loss,\n",
    "                        \"train_f2\": train_metrics['f2_score'],\n",
    "                        \"val_f2\": val_metrics['f2_score'],\n",
    "                        \"val_mae\": val_metrics['mae'],\n",
    "                        \"val_rmse\": val_metrics['rmse'],\n",
    "                        \"val_auc\": val_metrics['auc']\n",
    "                    }, step=epoch)\n",
    "                    \n",
    "                    # Track best F2 model\n",
    "                    if val_metrics['f2_score'] > best_f2:\n",
    "                        best_f2 = val_metrics['f2_score']\n",
    "                        best_model_state = copy.deepcopy(model.state_dict())\n",
    "                        early_stop_counter = 0\n",
    "                    else:\n",
    "                        early_stop_counter += 1\n",
    "                    \n",
    "                    # Track best MAE\n",
    "                    if val_metrics['mae'] < best_mae:\n",
    "                        best_mae = val_metrics['mae']\n",
    "                    \n",
    "                    # Early stopping\n",
    "                    if early_stop_counter >= config[\"patience\"]:\n",
    "                        print(f\"  Early stopping at epoch {epoch}\")\n",
    "                        break\n",
    "                \n",
    "                # Log best model\n",
    "                if best_model_state:\n",
    "                    model.load_state_dict(best_model_state)\n",
    "                    mlflow.pytorch.log_model(model, f\"model_fold_{fold}_best_f2\")\n",
    "                \n",
    "                print(f\"  >> Fold {fold} Best Val F2: {best_f2:.4f}, Best MAE: {best_mae:.2f}\")\n",
    "                \n",
    "                cv_summary[\"best_val_f2\"].append(best_f2)\n",
    "                cv_summary[\"best_val_mae\"].append(best_mae)\n",
    "        \n",
    "        # Log CV summary\n",
    "        avg_f2 = np.mean(cv_summary['best_val_f2'])\n",
    "        avg_mae = np.mean(cv_summary['best_val_mae'])\n",
    "        mlflow.log_metrics({\"cv_avg_best_f2\": avg_f2, \"cv_avg_best_mae\": avg_mae})\n",
    "        \n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Cross-Validation Complete\")\n",
    "        print(f\"{'='*50}\")\n",
    "        print(f\"Average Best F2: {avg_f2:.4f}\")\n",
    "        print(f\"Average Best MAE: {avg_mae:.2f}\")\n",
    "    \n",
    "    return cv_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c0e3a2d0-9f40-4219-8bac-27a7a22c2869",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Prepare fold logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90074526-84f2-4b5c-a092-9340cb831ddc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Setup MLflow experiment\n",
    "spark.conf.set(\"spark.databricks.mlflow.trackMLlib.enabled\", \"true\")\n",
    "\n",
    "try:\n",
    "    experiment = mlflow.get_experiment_by_name(EXPERIMENT_NAME)\n",
    "    if experiment is None:\n",
    "        experiment_id = mlflow.create_experiment(EXPERIMENT_NAME)\n",
    "        print(f\"Created new experiment with ID: {experiment_id}\")\n",
    "    else:\n",
    "        print(f\"Using existing experiment: {experiment.name}\")\n",
    "    mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "except Exception as e:\n",
    "    print(f\"Error with experiment setup: {e}\")\n",
    "\n",
    "# Training configuration\n",
    "training_config = {\n",
    "    \"num_epochs\": NUM_EPOCHS,\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"learning_rate\": LEARNING_RATE,\n",
    "    \"patience\": PATIENCE,\n",
    "    \"alpha\": LOSS_ALPHA,\n",
    "    \"pos_weight\": POS_WEIGHT,\n",
    "    \"delay_threshold\": DELAY_THRESHOLD,\n",
    "    \"device\": DEVICE\n",
    "}\n",
    "\n",
    "print(\"Training configuration:\")\n",
    "for k, v in training_config.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6240eaba-64f5-4ea6-bad2-3c62d3acfc8f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Run cross-validation\n",
    "cv_results = run_cross_validation(\n",
    "    cv_df=cv_df,\n",
    "    folds=folds,\n",
    "    categorical_cols=CATEGORICAL_COLS,\n",
    "    numerical_cols=NUMERICAL_COLS,\n",
    "    time_col=TIME_COL,\n",
    "    target_col=TARGET_COL,\n",
    "    config=training_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ccd0a4b4-9bec-4d64-a24e-5ddd70168d1a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "<a name=\"results\"></a>\n",
    "## 8. Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a2e74416-e103-422f-9e96-2dd8632640e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Display CV results\n",
    "results_df = pd.DataFrame({\n",
    "    \"Fold\": list(range(1, len(cv_results[\"best_val_f2\"]) + 1)),\n",
    "    \"Best F2 Score\": cv_results[\"best_val_f2\"],\n",
    "    \"Best MAE (min)\": cv_results[\"best_val_mae\"]\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Cross-Validation Results by Fold\")\n",
    "print(\"=\"*60)\n",
    "print(results_df.to_string(index=False))\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(f\"Average F2 Score: {np.mean(cv_results['best_val_f2']):.4f} ± {np.std(cv_results['best_val_f2']):.4f}\")\n",
    "print(f\"Average MAE:      {np.mean(cv_results['best_val_mae']):.2f} ± {np.std(cv_results['best_val_mae']):.2f} minutes\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "562709cb-38e3-45e0-8351-2cf97950073c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numerical columns count after deduplication: 34\n",
      "Starting Cross-Validation on Folds: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "\n",
      "--- Starting Fold 1 ---\n",
      "Fold 1 | Ep 0 | Train F2: 0.712 | Val F2: 0.722 | Train MAE: 11.385 | Val MAE: 12.040\n",
      "Fold 1 | Ep 1 | Train F2: 0.723 | Val F2: 0.699 | Train MAE: 11.307 | Val MAE: 12.026\n",
      "Fold 1 | Ep 2 | Train F2: 0.734 | Val F2: 0.745 | Train MAE: 11.254 | Val MAE: 11.970\n",
      "Fold 1 | Ep 3 | Train F2: 0.724 | Val F2: 0.711 | Train MAE: 11.227 | Val MAE: 11.955\n",
      "Fold 1 | Ep 4 | Train F2: 0.728 | Val F2: 0.724 | Train MAE: 11.134 | Val MAE: 11.910\n",
      "Fold 1 | Ep 5 | Train F2: 0.729 | Val F2: 0.734 | Train MAE: 11.085 | Val MAE: 11.937\n",
      "Fold 1 | Ep 6 | Train F2: 0.730 | Val F2: 0.728 | Train MAE: 11.083 | Val MAE: 11.910\n",
      "Fold 1 | Ep 7 | Train F2: 0.730 | Val F2: 0.742 | Train MAE: 11.046 | Val MAE: 11.928\n",
      "  Early stopping triggered.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31m2025/12/10 02:48:20 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27faf6f1498e469b9cb8a5379a5210cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  >> Fold 1 Best Val F2: 0.7450\n",
      "\n",
      "--- Starting Fold 2 ---\n",
      "Fold 2 | Ep 0 | Train F2: 0.716 | Val F2: 0.726 | Train MAE: 11.799 | Val MAE: 13.205\n",
      "Fold 2 | Ep 1 | Train F2: 0.731 | Val F2: 0.745 | Train MAE: 11.625 | Val MAE: 12.925\n",
      "Fold 2 | Ep 2 | Train F2: 0.729 | Val F2: 0.743 | Train MAE: 11.684 | Val MAE: 12.980\n",
      "Fold 2 | Ep 3 | Train F2: 0.729 | Val F2: 0.735 | Train MAE: 11.557 | Val MAE: 13.000\n",
      "Fold 2 | Ep 4 | Train F2: 0.735 | Val F2: 0.747 | Train MAE: 11.462 | Val MAE: 12.906\n",
      "Fold 2 | Ep 5 | Train F2: 0.730 | Val F2: 0.743 | Train MAE: 11.391 | Val MAE: 12.941\n",
      "Fold 2 | Ep 6 | Train F2: 0.739 | Val F2: 0.753 | Train MAE: 11.355 | Val MAE: 12.897\n",
      "Fold 2 | Ep 7 | Train F2: 0.735 | Val F2: 0.747 | Train MAE: 11.444 | Val MAE: 13.037\n",
      "Fold 2 | Ep 8 | Train F2: 0.739 | Val F2: 0.751 | Train MAE: 11.285 | Val MAE: 12.942\n",
      "Fold 2 | Ep 9 | Train F2: 0.741 | Val F2: 0.755 | Train MAE: 11.346 | Val MAE: 13.006\n",
      "Fold 2 | Ep 10 | Train F2: 0.733 | Val F2: 0.745 | Train MAE: 11.244 | Val MAE: 12.938\n",
      "Fold 2 | Ep 11 | Train F2: 0.737 | Val F2: 0.752 | Train MAE: 11.228 | Val MAE: 12.944\n",
      "Fold 2 | Ep 12 | Train F2: 0.740 | Val F2: 0.752 | Train MAE: 11.207 | Val MAE: 12.947\n",
      "Fold 2 | Ep 13 | Train F2: 0.740 | Val F2: 0.753 | Train MAE: 11.140 | Val MAE: 12.971\n",
      "Fold 2 | Ep 14 | Train F2: 0.739 | Val F2: 0.751 | Train MAE: 11.100 | Val MAE: 12.991\n",
      "  Early stopping triggered.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31m2025/12/10 03:26:08 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "023f8f4903394d5d82d7a5651a7a6ddb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  >> Fold 2 Best Val F2: 0.7547\n",
      "\n",
      "--- Starting Fold 3 ---\n",
      "Fold 3 | Ep 0 | Train F2: 0.737 | Val F2: 0.777 | Train MAE: 11.715 | Val MAE: 14.357\n",
      "Fold 3 | Ep 1 | Train F2: 0.735 | Val F2: 0.777 | Train MAE: 11.685 | Val MAE: 14.309\n",
      "Fold 3 | Ep 2 | Train F2: 0.731 | Val F2: 0.755 | Train MAE: 11.608 | Val MAE: 14.448\n",
      "Fold 3 | Ep 3 | Train F2: 0.736 | Val F2: 0.752 | Train MAE: 11.519 | Val MAE: 14.477\n",
      "Fold 3 | Ep 4 | Train F2: 0.738 | Val F2: 0.769 | Train MAE: 11.443 | Val MAE: 14.282\n",
      "Fold 3 | Ep 5 | Train F2: 0.738 | Val F2: 0.755 | Train MAE: 11.405 | Val MAE: 14.182\n",
      "  Early stopping triggered.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31m2025/12/10 03:42:03 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0df8279aaefa49539055ba1d464ee584",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  >> Fold 3 Best Val F2: 0.7774\n",
      "\n",
      "--- Starting Fold 4 ---\n",
      "Fold 4 | Ep 0 | Train F2: 0.718 | Val F2: 0.771 | Train MAE: 12.035 | Val MAE: 15.477\n",
      "Fold 4 | Ep 1 | Train F2: 0.712 | Val F2: 0.769 | Train MAE: 11.832 | Val MAE: 15.049\n",
      "Fold 4 | Ep 2 | Train F2: 0.712 | Val F2: 0.757 | Train MAE: 11.984 | Val MAE: 15.446\n",
      "Fold 4 | Ep 3 | Train F2: 0.735 | Val F2: 0.777 | Train MAE: 12.124 | Val MAE: 15.572\n",
      "Fold 4 | Ep 4 | Train F2: 0.735 | Val F2: 0.779 | Train MAE: 11.562 | Val MAE: 14.980\n",
      "Fold 4 | Ep 5 | Train F2: 0.725 | Val F2: 0.775 | Train MAE: 11.616 | Val MAE: 14.973\n",
      "Fold 4 | Ep 6 | Train F2: 0.740 | Val F2: 0.777 | Train MAE: 11.932 | Val MAE: 15.307\n",
      "Fold 4 | Ep 7 | Train F2: 0.737 | Val F2: 0.775 | Train MAE: 11.701 | Val MAE: 15.158\n",
      "Fold 4 | Ep 8 | Train F2: 0.740 | Val F2: 0.778 | Train MAE: 11.423 | Val MAE: 14.905\n",
      "Fold 4 | Ep 9 | Train F2: 0.737 | Val F2: 0.779 | Train MAE: 11.458 | Val MAE: 14.947\n",
      "  Early stopping triggered.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31m2025/12/10 04:07:31 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a058bb3161504619885460f99b4d4fc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  >> Fold 4 Best Val F2: 0.7791\n",
      "\n",
      "--- Starting Fold 5 ---\n",
      "Fold 5 | Ep 0 | Train F2: 0.740 | Val F2: 0.756 | Train MAE: 12.019 | Val MAE: 14.078\n",
      "Fold 5 | Ep 1 | Train F2: 0.735 | Val F2: 0.757 | Train MAE: 11.947 | Val MAE: 14.134\n",
      "Fold 5 | Ep 2 | Train F2: 0.728 | Val F2: 0.753 | Train MAE: 11.910 | Val MAE: 14.053\n",
      "Fold 5 | Ep 3 | Train F2: 0.740 | Val F2: 0.758 | Train MAE: 11.856 | Val MAE: 13.997\n",
      "Fold 5 | Ep 4 | Train F2: 0.745 | Val F2: 0.758 | Train MAE: 11.789 | Val MAE: 13.971\n",
      "Fold 5 | Ep 5 | Train F2: 0.743 | Val F2: 0.758 | Train MAE: 11.729 | Val MAE: 13.967\n",
      "Fold 5 | Ep 6 | Train F2: 0.742 | Val F2: 0.758 | Train MAE: 11.745 | Val MAE: 14.103\n",
      "Fold 5 | Ep 7 | Train F2: 0.739 | Val F2: 0.754 | Train MAE: 11.662 | Val MAE: 13.889\n",
      "Fold 5 | Ep 8 | Train F2: 0.738 | Val F2: 0.758 | Train MAE: 11.654 | Val MAE: 13.892\n",
      "Fold 5 | Ep 9 | Train F2: 0.748 | Val F2: 0.758 | Train MAE: 11.604 | Val MAE: 13.942\n",
      "Fold 5 | Ep 10 | Train F2: 0.741 | Val F2: 0.757 | Train MAE: 11.607 | Val MAE: 13.885\n",
      "Fold 5 | Ep 11 | Train F2: 0.747 | Val F2: 0.759 | Train MAE: 11.545 | Val MAE: 14.068\n",
      "Fold 5 | Ep 12 | Train F2: 0.748 | Val F2: 0.758 | Train MAE: 11.488 | Val MAE: 13.885\n",
      "Fold 5 | Ep 13 | Train F2: 0.750 | Val F2: 0.758 | Train MAE: 11.483 | Val MAE: 13.840\n",
      "Fold 5 | Ep 14 | Train F2: 0.740 | Val F2: 0.757 | Train MAE: 11.494 | Val MAE: 13.911\n",
      "Fold 5 | Ep 15 | Train F2: 0.751 | Val F2: 0.758 | Train MAE: 11.411 | Val MAE: 14.069\n",
      "Fold 5 | Ep 16 | Train F2: 0.743 | Val F2: 0.757 | Train MAE: 11.431 | Val MAE: 13.974\n",
      "  Early stopping triggered.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31m2025/12/10 04:48:49 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86ebf65b60d0414db60134a8972ef437",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  >> Fold 5 Best Val F2: 0.7586\n",
      "\n",
      "--- Starting Fold 6 ---\n",
      "Fold 6 | Ep 0 | Train F2: 0.732 | Val F2: 0.733 | Train MAE: 12.137 | Val MAE: 13.982\n",
      "Fold 6 | Ep 1 | Train F2: 0.735 | Val F2: 0.756 | Train MAE: 12.083 | Val MAE: 13.915\n",
      "Fold 6 | Ep 2 | Train F2: 0.729 | Val F2: 0.745 | Train MAE: 12.117 | Val MAE: 14.104\n",
      "Fold 6 | Ep 3 | Train F2: 0.742 | Val F2: 0.755 | Train MAE: 11.984 | Val MAE: 13.886\n",
      "Fold 6 | Ep 4 | Train F2: 0.746 | Val F2: 0.758 | Train MAE: 11.903 | Val MAE: 13.992\n",
      "Fold 6 | Ep 5 | Train F2: 0.740 | Val F2: 0.749 | Train MAE: 11.885 | Val MAE: 14.067\n",
      "Fold 6 | Ep 6 | Train F2: 0.747 | Val F2: 0.759 | Train MAE: 11.852 | Val MAE: 13.967\n",
      "Fold 6 | Ep 7 | Train F2: 0.748 | Val F2: 0.760 | Train MAE: 11.783 | Val MAE: 13.927\n",
      "Fold 6 | Ep 8 | Train F2: 0.747 | Val F2: 0.758 | Train MAE: 11.827 | Val MAE: 14.057\n",
      "Fold 6 | Ep 9 | Train F2: 0.748 | Val F2: 0.757 | Train MAE: 11.734 | Val MAE: 13.979\n",
      "Fold 6 | Ep 10 | Train F2: 0.742 | Val F2: 0.753 | Train MAE: 11.674 | Val MAE: 13.904\n",
      "Fold 6 | Ep 11 | Train F2: 0.746 | Val F2: 0.755 | Train MAE: 11.691 | Val MAE: 14.041\n",
      "Fold 6 | Ep 12 | Train F2: 0.749 | Val F2: 0.759 | Train MAE: 11.629 | Val MAE: 13.936\n",
      "  Early stopping triggered.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31m2025/12/10 05:20:51 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c63684735e5449ab6dc3065afb6eab9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  >> Fold 6 Best Val F2: 0.7599\n",
      "\n",
      "--- Starting Fold 7 ---\n",
      "Fold 7 | Ep 0 | Train F2: 0.737 | Val F2: 0.736 | Train MAE: 12.902 | Val MAE: 13.798\n",
      "Fold 7 | Ep 1 | Train F2: 0.741 | Val F2: 0.729 | Train MAE: 12.819 | Val MAE: 13.736\n",
      "Fold 7 | Ep 2 | Train F2: 0.744 | Val F2: 0.737 | Train MAE: 12.756 | Val MAE: 13.714\n",
      "Fold 7 | Ep 3 | Train F2: 0.754 | Val F2: 0.751 | Train MAE: 12.705 | Val MAE: 13.682\n",
      "Fold 7 | Ep 4 | Train F2: 0.752 | Val F2: 0.750 | Train MAE: 12.629 | Val MAE: 13.626\n",
      "Fold 7 | Ep 5 | Train F2: 0.738 | Val F2: 0.733 | Train MAE: 12.648 | Val MAE: 13.669\n",
      "Fold 7 | Ep 6 | Train F2: 0.748 | Val F2: 0.740 | Train MAE: 12.591 | Val MAE: 13.654\n",
      "Fold 7 | Ep 7 | Train F2: 0.756 | Val F2: 0.751 | Train MAE: 12.517 | Val MAE: 13.616\n",
      "Fold 7 | Ep 8 | Train F2: 0.751 | Val F2: 0.744 | Train MAE: 12.544 | Val MAE: 13.644\n",
      "Fold 7 | Ep 9 | Train F2: 0.755 | Val F2: 0.750 | Train MAE: 12.465 | Val MAE: 13.597\n",
      "Fold 7 | Ep 10 | Train F2: 0.745 | Val F2: 0.731 | Train MAE: 12.578 | Val MAE: 13.792\n",
      "Fold 7 | Ep 11 | Train F2: 0.757 | Val F2: 0.754 | Train MAE: 12.425 | Val MAE: 13.681\n",
      "Fold 7 | Ep 12 | Train F2: 0.754 | Val F2: 0.745 | Train MAE: 12.382 | Val MAE: 13.680\n",
      "Fold 7 | Ep 13 | Train F2: 0.756 | Val F2: 0.754 | Train MAE: 12.339 | Val MAE: 13.566\n",
      "Fold 7 | Ep 14 | Train F2: 0.750 | Val F2: 0.732 | Train MAE: 12.269 | Val MAE: 13.579\n",
      "Fold 7 | Ep 15 | Train F2: 0.752 | Val F2: 0.740 | Train MAE: 12.299 | Val MAE: 13.640\n",
      "Fold 7 | Ep 16 | Train F2: 0.753 | Val F2: 0.741 | Train MAE: 12.222 | Val MAE: 13.640\n",
      "  Early stopping triggered.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31m2025/12/10 06:05:10 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f124d28d77bf4e02b68a20f263459af4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  >> Fold 7 Best Val F2: 0.7537\n",
      "\n",
      "--- Starting Fold 8 ---\n",
      "Fold 8 | Ep 0 | Train F2: 0.752 | Val F2: 0.696 | Train MAE: 13.112 | Val MAE: 11.134\n",
      "Fold 8 | Ep 1 | Train F2: 0.750 | Val F2: 0.690 | Train MAE: 12.960 | Val MAE: 11.082\n",
      "Fold 8 | Ep 2 | Train F2: 0.758 | Val F2: 0.703 | Train MAE: 12.902 | Val MAE: 11.054\n",
      "Fold 8 | Ep 3 | Train F2: 0.752 | Val F2: 0.692 | Train MAE: 12.868 | Val MAE: 11.031\n",
      "Fold 8 | Ep 4 | Train F2: 0.755 | Val F2: 0.700 | Train MAE: 12.797 | Val MAE: 11.099\n",
      "Fold 8 | Ep 5 | Train F2: 0.756 | Val F2: 0.701 | Train MAE: 12.767 | Val MAE: 11.073\n",
      "Fold 8 | Ep 6 | Train F2: 0.757 | Val F2: 0.701 | Train MAE: 12.706 | Val MAE: 11.038\n",
      "Fold 8 | Ep 7 | Train F2: 0.757 | Val F2: 0.703 | Train MAE: 12.724 | Val MAE: 11.208\n",
      "Fold 8 | Ep 8 | Train F2: 0.757 | Val F2: 0.700 | Train MAE: 12.642 | Val MAE: 11.123\n",
      "Fold 8 | Ep 9 | Train F2: 0.756 | Val F2: 0.700 | Train MAE: 12.590 | Val MAE: 11.059\n",
      "Fold 8 | Ep 10 | Train F2: 0.755 | Val F2: 0.695 | Train MAE: 12.589 | Val MAE: 11.116\n",
      "Fold 8 | Ep 11 | Train F2: 0.759 | Val F2: 0.704 | Train MAE: 12.657 | Val MAE: 11.266\n",
      "Fold 8 | Ep 12 | Train F2: 0.758 | Val F2: 0.704 | Train MAE: 12.522 | Val MAE: 11.124\n",
      "Fold 8 | Ep 13 | Train F2: 0.752 | Val F2: 0.694 | Train MAE: 12.530 | Val MAE: 11.202\n",
      "Fold 8 | Ep 14 | Train F2: 0.760 | Val F2: 0.704 | Train MAE: 12.499 | Val MAE: 11.209\n",
      "Fold 8 | Ep 15 | Train F2: 0.759 | Val F2: 0.703 | Train MAE: 12.462 | Val MAE: 11.190\n",
      "Fold 8 | Ep 16 | Train F2: 0.759 | Val F2: 0.702 | Train MAE: 12.429 | Val MAE: 11.191\n",
      "  Early stopping triggered.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31m2025/12/10 06:48:46 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7e053ed533d4e2fb3f5537cecc4dfd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  >> Fold 8 Best Val F2: 0.7039\n",
      "\n",
      "--- Starting Fold 9 ---\n",
      "Fold 9 | Ep 0 | Train F2: 0.754 | Val F2: 0.644 | Train MAE: 13.414 | Val MAE: 8.722\n",
      "Fold 9 | Ep 1 | Train F2: 0.758 | Val F2: 0.662 | Train MAE: 13.489 | Val MAE: 8.848\n",
      "Fold 9 | Ep 2 | Train F2: 0.755 | Val F2: 0.637 | Train MAE: 13.218 | Val MAE: 8.670\n",
      "Fold 9 | Ep 3 | Train F2: 0.757 | Val F2: 0.639 | Train MAE: 13.179 | Val MAE: 8.755\n",
      "Fold 9 | Ep 4 | Train F2: 0.758 | Val F2: 0.638 | Train MAE: 13.159 | Val MAE: 8.802\n",
      "Fold 9 | Ep 5 | Train F2: 0.761 | Val F2: 0.662 | Train MAE: 13.348 | Val MAE: 8.708\n",
      "Fold 9 | Ep 6 | Train F2: 0.752 | Val F2: 0.620 | Train MAE: 13.086 | Val MAE: 8.719\n",
      "Fold 9 | Ep 7 | Train F2: 0.758 | Val F2: 0.630 | Train MAE: 12.949 | Val MAE: 8.716\n",
      "Fold 9 | Ep 8 | Train F2: 0.757 | Val F2: 0.619 | Train MAE: 12.936 | Val MAE: 8.636\n",
      "Fold 9 | Ep 9 | Train F2: 0.760 | Val F2: 0.640 | Train MAE: 12.887 | Val MAE: 8.711\n",
      "Fold 9 | Ep 10 | Train F2: 0.762 | Val F2: 0.662 | Train MAE: 12.855 | Val MAE: 8.646\n",
      "  Early stopping triggered.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31m2025/12/10 07:18:53 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f45491ae3a242d887e613226a471cb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  >> Fold 9 Best Val F2: 0.6624\n",
      "\n",
      "--- Starting Fold 10 ---\n",
      "Fold 10 | Ep 0 | Train F2: 0.743 | Val F2: 0.652 | Train MAE: 13.126 | Val MAE: 8.689\n",
      "Fold 10 | Ep 1 | Train F2: 0.751 | Val F2: 0.657 | Train MAE: 13.017 | Val MAE: 8.782\n",
      "Fold 10 | Ep 2 | Train F2: 0.748 | Val F2: 0.617 | Train MAE: 12.916 | Val MAE: 8.591\n",
      "Fold 10 | Ep 3 | Train F2: 0.748 | Val F2: 0.624 | Train MAE: 12.857 | Val MAE: 8.661\n",
      "Fold 10 | Ep 4 | Train F2: 0.750 | Val F2: 0.635 | Train MAE: 12.849 | Val MAE: 8.708\n",
      "Fold 10 | Ep 5 | Train F2: 0.752 | Val F2: 0.655 | Train MAE: 12.761 | Val MAE: 8.569\n",
      "Fold 10 | Ep 6 | Train F2: 0.745 | Val F2: 0.641 | Train MAE: 12.818 | Val MAE: 8.712\n",
      "  Early stopping triggered.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31m2025/12/10 07:36:49 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85da39e357c94ec78c0fe3de20073b70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  >> Fold 10 Best Val F2: 0.6572\n",
      "\n",
      "=== CV Complete ===\n",
      "Avg Best F2: 0.7352\n"
     ]
    }
   ],
   "source": [
    "### Key Findings\n",
    "\n",
    "\"\"\"\n",
    "The ResFiLM-MLP model demonstrates strong performance on flight delay prediction:\n",
    "\n",
    "1. **Classification (F2 Score)**\n",
    "   - Average F2: 0.735 across 10 folds\n",
    "   - Best fold: 0.779 (Fold 4)\n",
    "   - The F2 metric prioritizes recall, appropriate for delay prediction where\n",
    "     missing a delay (false negative) is more costly than a false alarm.\n",
    "\n",
    "2. **Regression (MAE)**\n",
    "   - Average MAE: ~12-14 minutes across most folds\n",
    "   - Notable variation between folds due to seasonal patterns in delay data\n",
    "   - Folds 9-10 show lower MAE but also lower F2, suggesting less delay variation\n",
    "\n",
    "3. **Architecture Benefits**\n",
    "   - FiLM modulation allows categorical embeddings to be conditioned on \n",
    "     numerical features (weather, congestion)\n",
    "   - Time2Vec captures both linear trends and periodic patterns in departure times\n",
    "   - Multi-task learning improves feature representations for both tasks\n",
    "\n",
    "4. **Training Observations**\n",
    "   - Early stopping typically triggers at epochs 6-15\n",
    "   - Classification loss converges faster than regression loss\n",
    "   - Positive class weighting (4.0) effectively handles class imbalance\n",
    "\"\"\"\n",
    "\n",
    "print(\"Notebook execution complete. Models logged to MLflow.\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "NN_MLP_Pipeline_MK2",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Table of Contents\n",
        "\n",
        "1. [Configuration & Setup](#1-configuration--setup)\n",
        "2. [Feature Definitions](#2-feature-definitions)\n",
        "3. [Data Preparation](#3-data-preparation)\n",
        "4. [Model Architecture](#4-model-architecture)\n",
        "5. [Training Utilities](#5-training-utilities)\n",
        "6. [Hyperparameter Tuning](#6-hyperparameter-tuning)\n",
        "7. [Final Training](#7-final-training)\n",
        "8. [Threshold Optimization](#8-threshold-optimization)\n",
        "9. [Inference & Results](#9-inference--results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 1. Configuration & Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# IMPORTS\n",
        "# =============================================================================\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import gc\n",
        "import time\n",
        "import copy\n",
        "import uuid\n",
        "from typing import List, Dict, Tuple, Optional\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# PySpark\n",
        "import pyspark.sql.functions as sf\n",
        "from pyspark.sql import Window\n",
        "from pyspark.sql.types import FloatType, IntegerType\n",
        "from pyspark.ml.feature import StringIndexer\n",
        "\n",
        "# PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "# Scikit-learn\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import fbeta_score, mean_absolute_error\n",
        "\n",
        "# MLflow & Optuna\n",
        "import mlflow\n",
        "from mlflow.models import infer_signature\n",
        "import optuna\n",
        "from optuna.integration.mlflow import MLflowCallback\n",
        "\n",
        "# Enable Arrow for faster Spark-to-Pandas conversion\n",
        "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"MLflow version: {mlflow.__version__}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# CONFIGURATION\n",
        "# =============================================================================\n",
        "\n",
        "# Paths\n",
        "BASE_PATH = \"dbfs:/student-groups/Group_2_2/5_year_custom_joined\"\n",
        "DATA_PATH = f\"{BASE_PATH}/fe_graph_and_holiday_nnfeat/training_splits\"\n",
        "CV_DATA_PATH = f\"{BASE_PATH}/fe_graph_and_holiday_nnfeat/cv_splits\"\n",
        "PREDICTIONS_PATH = f\"{BASE_PATH}/nn_predictions_final\"\n",
        "\n",
        "# MLflow\n",
        "EXPERIMENT_NAME = \"/Shared/team_2_2/mlflow-nn-tower-final\"\n",
        "mlflow.set_experiment(EXPERIMENT_NAME)\n",
        "\n",
        "# Device Configuration\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {DEVICE}\")\n",
        "\n",
        "# Best Hyperparameters (from Optuna tuning)\n",
        "BEST_PARAMS = {\n",
        "    \"lr\": 0.0001556,\n",
        "    \"batch_size\": 4096,\n",
        "    \"alpha\": 0.342,           # Loss weighting: α*MAE + (1-α)*BCE\n",
        "    \"time_dim\": 16,           # Time2Vec dimensions\n",
        "    \"emb_drop\": 0.046,        # Embedding dropout\n",
        "    \"num_drop\": 0.324,        # Numerical tower dropout\n",
        "    \"final_drop\": 0.100       # Final layer dropout\n",
        "}\n",
        "\n",
        "# Training Configuration\n",
        "NUM_EPOCHS = 10\n",
        "PATIENCE = 4                  # Early stopping patience\n",
        "DELAY_THRESHOLD = 15.0        # Minutes - defines \"delayed\" flight\n",
        "OPTIMAL_THRESHOLD = 0.36      # Classification threshold (from optimization)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 2. Feature Definitions\n",
        "\n",
        "The model uses three types of features:\n",
        "- **Categorical**: Encoded as learnable embeddings\n",
        "- **Numerical**: Standardized and processed through residual blocks\n",
        "- **Temporal**: Departure time encoded via Time2Vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# FEATURE DEFINITIONS\n",
        "# =============================================================================\n",
        "\n",
        "CATEGORICAL_COLS = [\n",
        "    \"OP_UNIQUE_CARRIER\",       # Airline carrier code\n",
        "    \"ORIGIN_AIRPORT_SEQ_ID\",   # Origin airport identifier\n",
        "    \"DEST_AIRPORT_SEQ_ID\",     # Destination airport identifier\n",
        "    \"route\",                   # Origin-Destination pair\n",
        "    \"AIRPORT_HUB_CLASS\",       # Hub classification\n",
        "    \"AIRLINE_CATEGORY\"         # Airline category\n",
        "]\n",
        "\n",
        "NUMERICAL_COLS = [\n",
        "    # Flight characteristics\n",
        "    \"DISTANCE\", \"CRS_ELAPSED_TIME\",\n",
        "    \n",
        "    # Historical delay features\n",
        "    \"prev_flight_delay_in_minutes\", \"origin_delays_4h\",\n",
        "    \"delay_origin_7d\", \"delay_origin_carrier_7d\", \"delay_route_7d\",\n",
        "    \"flight_count_24h\", \"AVG_TAXI_OUT_ORIGIN\", \"AVG_ARR_DELAY_ORIGIN\",\n",
        "    \n",
        "    # Graph-based features\n",
        "    \"in_degree\", \"out_degree\", \"weighted_in_degree\", \"weighted_out_degree\",\n",
        "    \"betweenness\", \"closeness\",\n",
        "    \n",
        "    # Airport features\n",
        "    \"N_RUNWAYS\",\n",
        "    \n",
        "    # Weather features\n",
        "    \"HourlyVisibility\", \"HourlyStationPressure\", \"HourlyWindSpeed\",\n",
        "    \"HourlyDryBulbTemperature\", \"HourlyDewPointTemperature\",\n",
        "    \"HourlyRelativeHumidity\", \"HourlyAltimeterSetting\",\n",
        "    \"HourlyWetBulbTemperature\", \"HourlyPrecipitation\",\n",
        "    \"HourlyCloudCoverage\", \"HourlyCloudElevation\",\n",
        "    \n",
        "    # Traffic features\n",
        "    \"ground_flights_last_hour\", \"arrivals_last_hour\",\n",
        "    \n",
        "    # Cyclical time encodings\n",
        "    \"dow_sin\", \"dow_cos\",  # Day of week\n",
        "    \"doy_sin\", \"doy_cos\"   # Day of year\n",
        "]\n",
        "\n",
        "# Ensure no duplicates\n",
        "NUMERICAL_COLS = list(dict.fromkeys(NUMERICAL_COLS))\n",
        "\n",
        "TIME_COL = \"CRS_DEP_MINUTES\"\n",
        "TARGET_COL = \"DEP_DELAY_NEW\"\n",
        "\n",
        "print(f\"Categorical features: {len(CATEGORICAL_COLS)}\")\n",
        "print(f\"Numerical features: {len(NUMERICAL_COLS)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 3. Data Preparation\n",
        "\n",
        "Functions for loading Spark DataFrames, applying string indexing, scaling numerical features, and converting to PyTorch datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# PYTORCH DATASET\n",
        "# =============================================================================\n",
        "\n",
        "class FlightDataset(Dataset):\n",
        "    \"\"\"\n",
        "    PyTorch Dataset for flight delay prediction.\n",
        "    \n",
        "    Separates features into categorical (for embeddings), numerical (for MLP),\n",
        "    and temporal (for Time2Vec) components.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(\n",
        "        self, \n",
        "        df: pd.DataFrame,\n",
        "        cat_cols: List[str] = CATEGORICAL_COLS,\n",
        "        num_cols: List[str] = NUMERICAL_COLS,\n",
        "        time_col: str = TIME_COL,\n",
        "        target_col: str = TARGET_COL,\n",
        "        id_col: str = \"flight_uid\"\n",
        "    ):\n",
        "        self.cat = torch.tensor(df[cat_cols].values, dtype=torch.long)\n",
        "        self.num = torch.tensor(df[num_cols].values, dtype=torch.float32)\n",
        "        self.time = torch.tensor(df[time_col].values, dtype=torch.float32).unsqueeze(1)\n",
        "        self.y = torch.tensor(df[target_col].values, dtype=torch.float32).unsqueeze(1)\n",
        "        self.ids = df[id_col].values if id_col in df.columns else np.arange(len(df))\n",
        "    \n",
        "    def __len__(self) -> int:\n",
        "        return len(self.y)\n",
        "    \n",
        "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, ...]:\n",
        "        return self.cat[idx], self.num[idx], self.time[idx], self.y[idx], self.ids[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# DATA PREPARATION FUNCTIONS\n",
        "# =============================================================================\n",
        "\n",
        "def spark_to_pandas(\n",
        "    df, \n",
        "    indexers: List[StringIndexer],\n",
        "    cat_cols: List[str] = CATEGORICAL_COLS,\n",
        "    num_cols: List[str] = NUMERICAL_COLS,\n",
        "    time_col: str = TIME_COL,\n",
        "    target_col: str = TARGET_COL\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Transform Spark DataFrame to Pandas with proper type casting.\n",
        "    \n",
        "    Args:\n",
        "        df: Spark DataFrame\n",
        "        indexers: Fitted StringIndexer models for categorical columns\n",
        "        cat_cols: Categorical column names\n",
        "        num_cols: Numerical column names\n",
        "        time_col: Time feature column name\n",
        "        target_col: Target column name\n",
        "    \n",
        "    Returns:\n",
        "        Pandas DataFrame with indexed categoricals and float numericals\n",
        "    \"\"\"\n",
        "    # Apply string indexers\n",
        "    for indexer in indexers:\n",
        "        df = indexer.transform(df)\n",
        "    \n",
        "    # Build select expression with proper types\n",
        "    select_expr = (\n",
        "        [sf.col(f\"{c}_idx\").cast(IntegerType()).alias(c) for c in cat_cols] +\n",
        "        [sf.col(c).cast(FloatType()) for c in num_cols] +\n",
        "        [sf.col(time_col).cast(FloatType()), \n",
        "         sf.col(target_col).cast(FloatType()),\n",
        "         sf.col(\"flight_uid\")]\n",
        "    )\n",
        "    \n",
        "    return df.select(*select_expr).toPandas()\n",
        "\n",
        "\n",
        "def prepare_data_splits(\n",
        "    train_path: str,\n",
        "    val_path: str,\n",
        "    test_path: str = None\n",
        ") -> Tuple[pd.DataFrame, pd.DataFrame, Optional[pd.DataFrame], List, StandardScaler]:\n",
        "    \"\"\"\n",
        "    Load and prepare train/val/test splits with consistent preprocessing.\n",
        "    \n",
        "    Returns:\n",
        "        train_pd, val_pd, test_pd (optional), indexers, scaler\n",
        "    \"\"\"\n",
        "    print(\"Loading data...\")\n",
        "    train_spark = spark.read.parquet(train_path)\n",
        "    val_spark = spark.read.parquet(val_path)\n",
        "    test_spark = spark.read.parquet(test_path) if test_path else None\n",
        "    \n",
        "    # Fit string indexers on training data\n",
        "    print(\"Fitting string indexers...\")\n",
        "    indexers = [\n",
        "        StringIndexer(inputCol=c, outputCol=f\"{c}_idx\", handleInvalid=\"keep\")\n",
        "        .fit(train_spark) \n",
        "        for c in CATEGORICAL_COLS\n",
        "    ]\n",
        "    \n",
        "    # Convert to Pandas\n",
        "    print(\"Converting to Pandas...\")\n",
        "    train_pd = spark_to_pandas(train_spark, indexers)\n",
        "    val_pd = spark_to_pandas(val_spark, indexers)\n",
        "    test_pd = spark_to_pandas(test_spark, indexers) if test_spark else None\n",
        "    \n",
        "    # Fit scaler on training data\n",
        "    print(\"Fitting scaler...\")\n",
        "    scaler = StandardScaler()\n",
        "    train_pd[NUMERICAL_COLS] = scaler.fit_transform(train_pd[NUMERICAL_COLS])\n",
        "    val_pd[NUMERICAL_COLS] = scaler.transform(val_pd[NUMERICAL_COLS])\n",
        "    if test_pd is not None:\n",
        "        test_pd[NUMERICAL_COLS] = scaler.transform(test_pd[NUMERICAL_COLS])\n",
        "    \n",
        "    # Calculate embedding dimensions\n",
        "    cat_dims = [int(train_pd[c].max() + 2) for c in CATEGORICAL_COLS]\n",
        "    emb_dims = [min(64, int(n**0.3)) for n in cat_dims]\n",
        "    \n",
        "    print(f\"Train: {len(train_pd):,} | Val: {len(val_pd):,}\", end=\"\")\n",
        "    if test_pd is not None:\n",
        "        print(f\" | Test: {len(test_pd):,}\")\n",
        "    else:\n",
        "        print()\n",
        "    \n",
        "    return train_pd, val_pd, test_pd, cat_dims, emb_dims, indexers, scaler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 4. Model Architecture\n",
        "\n",
        "The **ResFiLM-MLP** architecture consists of:\n",
        "\n",
        "1. **Embedding Tower**: Learns dense representations for categorical features\n",
        "2. **Numerical Tower**: 4 residual blocks with LayerNorm and GELU activations\n",
        "3. **FiLM Layer**: Feature-wise Linear Modulation - uses numerical features to generate γ (scale) and β (shift) parameters that modulate the embeddings\n",
        "4. **Time2Vec**: Learnable periodic encoding for departure time\n",
        "5. **Dual Prediction Heads**: Separate heads for regression (delay minutes) and classification (delayed/not delayed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# TRAINING UTILITIES\n",
        "# =============================================================================\n",
        "\n",
        "def train_one_epoch(\n",
        "    model: nn.Module,\n",
        "    loader: DataLoader,\n",
        "    optimizer: optim.Optimizer,\n",
        "    criterion_reg: nn.Module,\n",
        "    criterion_clf: nn.Module,\n",
        "    alpha: float,\n",
        "    device: torch.device = DEVICE\n",
        ") -> float:\n",
        "    \"\"\"\n",
        "    Train model for one epoch.\n",
        "    \n",
        "    Loss = α * MAE_loss + (1-α) * BCE_loss\n",
        "    \n",
        "    Args:\n",
        "        model: Neural network model\n",
        "        loader: Training data loader\n",
        "        optimizer: Optimizer\n",
        "        criterion_reg: Regression loss (L1/MAE)\n",
        "        criterion_clf: Classification loss (BCE)\n",
        "        alpha: Weight for regression loss\n",
        "        device: Compute device\n",
        "    \n",
        "    Returns:\n",
        "        Average training loss\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    \n",
        "    for cat, num, time_feat, y, _ in loader:\n",
        "        cat = cat.to(device)\n",
        "        num = num.to(device)\n",
        "        time_feat = time_feat.to(device)\n",
        "        y = y.to(device)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        reg_out, clf_out = model(cat, num, time_feat)\n",
        "        y_class = (y >= DELAY_THRESHOLD).float()\n",
        "        \n",
        "        loss = alpha * criterion_reg(reg_out, y) + (1 - alpha) * criterion_clf(clf_out, y_class)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        total_loss += loss.item()\n",
        "    \n",
        "    return total_loss / len(loader)\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate_model(\n",
        "    model: nn.Module,\n",
        "    loader: DataLoader,\n",
        "    threshold: float = 0.5,\n",
        "    device: torch.device = DEVICE\n",
        ") -> Tuple[float, float]:\n",
        "    \"\"\"\n",
        "    Evaluate model on a dataset.\n",
        "    \n",
        "    Args:\n",
        "        model: Neural network model\n",
        "        loader: Data loader\n",
        "        threshold: Classification threshold\n",
        "        device: Compute device\n",
        "    \n",
        "    Returns:\n",
        "        f2_score, mae\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    all_y_true, all_y_reg, all_y_prob = [], [], []\n",
        "    \n",
        "    for cat, num, time_feat, y, _ in loader:\n",
        "        reg_out, clf_out = model(\n",
        "            cat.to(device), num.to(device), time_feat.to(device)\n",
        "        )\n",
        "        \n",
        "        all_y_true.append(y.cpu())\n",
        "        all_y_reg.append(reg_out.cpu())\n",
        "        all_y_prob.append(torch.sigmoid(clf_out).cpu())\n",
        "    \n",
        "    y_true = torch.cat(all_y_true).numpy().flatten()\n",
        "    y_reg = torch.cat(all_y_reg).numpy().flatten()\n",
        "    y_prob = torch.cat(all_y_prob).numpy().flatten()\n",
        "    \n",
        "    # Metrics\n",
        "    mae = mean_absolute_error(y_true, y_reg)\n",
        "    y_true_class = (y_true >= DELAY_THRESHOLD).astype(int)\n",
        "    y_pred_class = (y_prob >= threshold).astype(int)\n",
        "    f2 = fbeta_score(y_true_class, y_pred_class, beta=2, zero_division=0)\n",
        "    \n",
        "    return f2, mae"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# PREDICTION & SAVING UTILITIES\n",
        "# =============================================================================\n",
        "\n",
        "@torch.no_grad()\n",
        "def generate_predictions(\n",
        "    model: nn.Module,\n",
        "    loader: DataLoader,\n",
        "    threshold: float = OPTIMAL_THRESHOLD,\n",
        "    device: torch.device = DEVICE\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Generate predictions for all samples in a data loader.\n",
        "    \n",
        "    Args:\n",
        "        model: Trained model\n",
        "        loader: Data loader\n",
        "        threshold: Classification threshold\n",
        "        device: Compute device\n",
        "    \n",
        "    Returns:\n",
        "        DataFrame with flight_uid, targets, and predictions\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    results = {\n",
        "        \"flight_uid\": [],\n",
        "        \"target_delay\": [],\n",
        "        \"pred_delay\": [],\n",
        "        \"target_class\": [],\n",
        "        \"pred_prob\": [],\n",
        "        \"pred_class\": []\n",
        "    }\n",
        "    \n",
        "    for cat, num, time_feat, y, ids in loader:\n",
        "        reg_out, clf_out = model(\n",
        "            cat.to(device), num.to(device), time_feat.to(device)\n",
        "        )\n",
        "        \n",
        "        y_prob = torch.sigmoid(clf_out).cpu().numpy().flatten()\n",
        "        \n",
        "        results[\"flight_uid\"].extend(ids)\n",
        "        results[\"target_delay\"].extend(y.numpy().flatten())\n",
        "        results[\"pred_delay\"].extend(reg_out.cpu().numpy().flatten())\n",
        "        results[\"target_class\"].extend((y >= DELAY_THRESHOLD).float().numpy().flatten())\n",
        "        results[\"pred_prob\"].extend(y_prob)\n",
        "        results[\"pred_class\"].extend((y_prob >= threshold).astype(int))\n",
        "    \n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "\n",
        "def save_predictions(\n",
        "    predictions_df: pd.DataFrame,\n",
        "    split_name: str,\n",
        "    save_path: str = PREDICTIONS_PATH\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Save predictions to DBFS as Parquet.\n",
        "    \n",
        "    Args:\n",
        "        predictions_df: DataFrame with predictions\n",
        "        split_name: Name of the split (e.g., \"test\", \"validation\")\n",
        "        save_path: Base path for saving\n",
        "    \n",
        "    Returns:\n",
        "        Full path where predictions were saved\n",
        "    \"\"\"\n",
        "    unique_id = str(uuid.uuid4())[:8]\n",
        "    full_path = f\"{save_path}/{split_name}_{unique_id}\"\n",
        "    \n",
        "    spark.createDataFrame(predictions_df).write.mode(\"overwrite\").parquet(full_path)\n",
        "    print(f\"✓ Saved predictions to: {full_path}\")\n",
        "    \n",
        "    return full_path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 6. Hyperparameter Tuning\n",
        "\n",
        "Optuna-based hyperparameter optimization using 3-fold cross-validation on a subset of folds for efficiency."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# HYPERPARAMETER TUNING (Optuna)\n",
        "# =============================================================================\n",
        "\n",
        "def prepare_fold_data(fold_df, fold_id: int):\n",
        "    \"\"\"Prepare data for a specific CV fold.\"\"\"\n",
        "    train_fe = fold_df.filter(sf.col(\"split_type\") == \"train\")\n",
        "    val_fe = fold_df.filter(sf.col(\"split_type\") == \"validation\")\n",
        "    \n",
        "    # Cast types\n",
        "    for c in NUMERICAL_COLS + [TIME_COL, TARGET_COL]:\n",
        "        train_fe = train_fe.withColumn(c, sf.col(c).cast(FloatType()))\n",
        "        val_fe = val_fe.withColumn(c, sf.col(c).cast(FloatType()))\n",
        "    \n",
        "    # String indexing\n",
        "    indexers = []\n",
        "    for c in CATEGORICAL_COLS:\n",
        "        indexer = StringIndexer(\n",
        "            inputCol=c, outputCol=f\"{c}_idx\",\n",
        "            stringOrderType=\"alphabetAsc\", handleInvalid=\"keep\"\n",
        "        )\n",
        "        model = indexer.fit(train_fe)\n",
        "        train_fe = model.transform(train_fe)\n",
        "        val_fe = model.transform(val_fe)\n",
        "        indexers.append(model)\n",
        "    \n",
        "    # Select and cast\n",
        "    final_cols = [f\"{c}_idx\" for c in CATEGORICAL_COLS] + NUMERICAL_COLS + [TIME_COL, TARGET_COL]\n",
        "    for c in CATEGORICAL_COLS:\n",
        "        train_fe = train_fe.withColumn(f\"{c}_idx\", sf.col(f\"{c}_idx\").cast(IntegerType()))\n",
        "        val_fe = val_fe.withColumn(f\"{c}_idx\", sf.col(f\"{c}_idx\").cast(IntegerType()))\n",
        "    \n",
        "    # Convert to Pandas\n",
        "    train_pd = train_fe.select(final_cols).toPandas()\n",
        "    val_pd = val_fe.select(final_cols).toPandas()\n",
        "    \n",
        "    # Rename columns\n",
        "    rename_map = {f\"{c}_idx\": c for c in CATEGORICAL_COLS}\n",
        "    train_pd = train_pd.rename(columns=rename_map)\n",
        "    val_pd = val_pd.rename(columns=rename_map)\n",
        "    \n",
        "    # Scale\n",
        "    scaler = StandardScaler()\n",
        "    train_pd[NUMERICAL_COLS] = scaler.fit_transform(train_pd[NUMERICAL_COLS])\n",
        "    val_pd[NUMERICAL_COLS] = scaler.transform(val_pd[NUMERICAL_COLS])\n",
        "    \n",
        "    # Embedding dimensions\n",
        "    cat_dims = [int(train_pd[c].max() + 2) for c in CATEGORICAL_COLS]\n",
        "    emb_dims = [min(64, int(n**0.3)) for n in cat_dims]\n",
        "    \n",
        "    return train_pd, val_pd, cat_dims, emb_dims\n",
        "\n",
        "\n",
        "def optuna_objective(trial, cv_full_df, folds):\n",
        "    \"\"\"Optuna objective function for hyperparameter optimization.\"\"\"\n",
        "    params = {\n",
        "        \"lr\": trial.suggest_float(\"lr\", 1e-4, 5e-3, log=True),\n",
        "        \"batch_size\": trial.suggest_categorical(\"batch_size\", [1024, 2048, 4096]),\n",
        "        \"alpha\": trial.suggest_float(\"alpha\", 0.3, 0.7),\n",
        "        \"time_dim\": trial.suggest_categorical(\"time_dim\", [4, 8, 16]),\n",
        "        \"emb_drop\": trial.suggest_float(\"emb_drop\", 0.0, 0.4),\n",
        "        \"num_drop\": trial.suggest_float(\"num_drop\", 0.0, 0.4),\n",
        "        \"final_drop\": trial.suggest_float(\"final_drop\", 0.0, 0.4)\n",
        "    }\n",
        "    \n",
        "    # Use 3 folds for efficiency: first, middle, last\n",
        "    tuning_folds = [folds[0], folds[len(folds)//2], folds[-1]]\n",
        "    val_f2_scores = []\n",
        "    \n",
        "    print(f\"\\n[Trial {trial.number}] Params: batch={params['batch_size']}, lr={params['lr']:.5f}\")\n",
        "    \n",
        "    for i, fold_id in enumerate(tuning_folds):\n",
        "        fold_df = cv_full_df.filter(sf.col(\"fold_id\") == fold_id)\n",
        "        train_pd, val_pd, cat_dims, emb_dims = prepare_fold_data(fold_df, fold_id)\n",
        "        \n",
        "        # Create datasets and loaders\n",
        "        train_ds = FlightDataset(train_pd, id_col=None)\n",
        "        val_ds = FlightDataset(val_pd, id_col=None)\n",
        "        train_dl = DataLoader(train_ds, batch_size=params[\"batch_size\"], shuffle=True, num_workers=0)\n",
        "        val_dl = DataLoader(val_ds, batch_size=params[\"batch_size\"], num_workers=0)\n",
        "        \n",
        "        # Create model\n",
        "        model = ResFiLMMLP(\n",
        "            cat_dims, emb_dims, len(NUMERICAL_COLS),\n",
        "            time_dim=params[\"time_dim\"],\n",
        "            emb_dropout=params[\"emb_drop\"],\n",
        "            num_dropout=params[\"num_drop\"],\n",
        "            final_dropout=params[\"final_drop\"]\n",
        "        ).to(DEVICE)\n",
        "        \n",
        "        optimizer = optim.AdamW(model.parameters(), lr=params[\"lr\"])\n",
        "        criterion_reg = nn.L1Loss()\n",
        "        criterion_clf = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([4.0]).to(DEVICE))\n",
        "        \n",
        "        # Train for 3 epochs\n",
        "        for _ in range(3):\n",
        "            train_one_epoch(model, train_dl, optimizer, criterion_reg, criterion_clf, params[\"alpha\"])\n",
        "        \n",
        "        # Evaluate\n",
        "        val_f2, val_mae = evaluate_model(model, val_dl)\n",
        "        val_f2_scores.append(val_f2)\n",
        "        print(f\"  Fold {fold_id}: Val F2={val_f2:.3f}\")\n",
        "        \n",
        "        # Pruning\n",
        "        trial.report(np.mean(val_f2_scores), i)\n",
        "        if trial.should_prune():\n",
        "            raise optuna.exceptions.TrialPruned()\n",
        "    \n",
        "    mean_f2 = np.mean(val_f2_scores)\n",
        "    \n",
        "    # Log to MLflow\n",
        "    with mlflow.start_run(nested=True, run_name=f\"Trial_{trial.number}\"):\n",
        "        mlflow.log_params(params)\n",
        "        mlflow.log_metric(\"val_f2\", mean_f2)\n",
        "    \n",
        "    return mean_f2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# RUN HYPERPARAMETER TUNING\n",
        "# =============================================================================\n",
        "\n",
        "# Note: Set RUN_TUNING = True to execute hyperparameter search\n",
        "# This takes ~8 hours on Databricks with CPU\n",
        "\n",
        "RUN_TUNING = False\n",
        "\n",
        "if RUN_TUNING:\n",
        "    # Load CV data\n",
        "    cv_full_df = spark.read.parquet(CV_DATA_PATH)\n",
        "    folds = sorted([row['fold_id'] for row in cv_full_df.select(\"fold_id\").distinct().collect()])\n",
        "    \n",
        "    print(f\"Starting Optuna Tuning (8 Trials)\")\n",
        "    print(f\"Folds available: {folds}\")\n",
        "    \n",
        "    mlflow.set_experiment(\"/Shared/team_2_2/mlflow-nn-tower-tuned\")\n",
        "    \n",
        "    with mlflow.start_run(run_name=\"Hyperparameter_Tuning\"):\n",
        "        study = optuna.create_study(direction=\"maximize\")\n",
        "        study.optimize(\n",
        "            lambda trial: optuna_objective(trial, cv_full_df, folds),\n",
        "            n_trials=8\n",
        "        )\n",
        "    \n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"BEST PARAMETERS: {study.best_params}\")\n",
        "    print(f\"BEST F2 SCORE: {study.best_value:.4f}\")\n",
        "    print(f\"{'='*60}\")\n",
        "else:\n",
        "    print(\"Hyperparameter tuning skipped. Using pre-tuned parameters:\")\n",
        "    print(f\"  {BEST_PARAMS}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# FINAL TRAINING FUNCTION\n",
        "# =============================================================================\n",
        "\n",
        "def train_final_model(\n",
        "    train_dl: DataLoader,\n",
        "    val_dl: DataLoader,\n",
        "    cat_dims: List[int],\n",
        "    emb_dims: List[int],\n",
        "    params: Dict = BEST_PARAMS,\n",
        "    num_epochs: int = NUM_EPOCHS,\n",
        "    patience: int = PATIENCE,\n",
        "    device: torch.device = DEVICE\n",
        ") -> Tuple[nn.Module, Dict]:\n",
        "    \"\"\"\n",
        "    Train final model with early stopping.\n",
        "    \n",
        "    Args:\n",
        "        train_dl: Training data loader\n",
        "        val_dl: Validation data loader\n",
        "        cat_dims: Categorical feature dimensions\n",
        "        emb_dims: Embedding dimensions\n",
        "        params: Hyperparameters\n",
        "        num_epochs: Maximum training epochs\n",
        "        patience: Early stopping patience\n",
        "        device: Compute device\n",
        "    \n",
        "    Returns:\n",
        "        Trained model and training history\n",
        "    \"\"\"\n",
        "    # Initialize model\n",
        "    model = ResFiLMMLP(\n",
        "        cat_dims, emb_dims, len(NUMERICAL_COLS),\n",
        "        time_dim=params[\"time_dim\"],\n",
        "        emb_dropout=params[\"emb_drop\"],\n",
        "        num_dropout=params[\"num_drop\"],\n",
        "        final_dropout=params[\"final_drop\"]\n",
        "    ).to(device)\n",
        "    \n",
        "    optimizer = optim.AdamW(model.parameters(), lr=params[\"lr\"])\n",
        "    criterion_reg = nn.L1Loss()\n",
        "    criterion_clf = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([4.0]).to(device))\n",
        "    \n",
        "    # Training state\n",
        "    best_f2 = -1.0\n",
        "    best_state = None\n",
        "    patience_counter = 0\n",
        "    history = {\"train_f2\": [], \"train_mae\": [], \"val_f2\": [], \"val_mae\": []}\n",
        "    \n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"TRAINING: {num_epochs} epochs, patience={patience}\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    for epoch in range(num_epochs):\n",
        "        # Training\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        train_tp, train_fp, train_fn = 0, 0, 0\n",
        "        train_ae_sum = 0.0\n",
        "        total_samples = 0\n",
        "        \n",
        "        for i, (cat, num, time_feat, y, _) in enumerate(train_dl):\n",
        "            cat, num, time_feat, y = cat.to(device), num.to(device), time_feat.to(device), y.to(device)\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            reg_out, clf_out = model(cat, num, time_feat)\n",
        "            y_class = (y >= DELAY_THRESHOLD).float()\n",
        "            \n",
        "            loss = params[\"alpha\"] * criterion_reg(reg_out, y) + (1 - params[\"alpha\"]) * criterion_clf(clf_out, y_class)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            # Track metrics\n",
        "            with torch.no_grad():\n",
        "                train_loss += loss.item()\n",
        "                train_ae_sum += torch.sum(torch.abs(reg_out - y)).item()\n",
        "                y_pred = (torch.sigmoid(clf_out) > 0.5).long()\n",
        "                y_true = y_class.long()\n",
        "                train_tp += ((y_true == 1) & (y_pred == 1)).sum().item()\n",
        "                train_fp += ((y_true == 0) & (y_pred == 1)).sum().item()\n",
        "                train_fn += ((y_true == 1) & (y_pred == 0)).sum().item()\n",
        "                total_samples += y.size(0)\n",
        "            \n",
        "            if (i + 1) % 500 == 0:\n",
        "                print(f\"  Epoch {epoch} - Batch {i+1}/{len(train_dl)}\", end=\"\\r\")\n",
        "        \n",
        "        # Calculate training metrics\n",
        "        train_mae = train_ae_sum / total_samples\n",
        "        precision = train_tp / (train_tp + train_fp + 1e-8)\n",
        "        recall = train_tp / (train_tp + train_fn + 1e-8)\n",
        "        train_f2 = (1 + 4) * (precision * recall) / (4 * precision + recall + 1e-8)\n",
        "        \n",
        "        # Validation\n",
        "        val_f2, val_mae = evaluate_model(model, val_dl)\n",
        "        \n",
        "        # Store history\n",
        "        history[\"train_f2\"].append(train_f2)\n",
        "        history[\"train_mae\"].append(train_mae)\n",
        "        history[\"val_f2\"].append(val_f2)\n",
        "        history[\"val_mae\"].append(val_mae)\n",
        "        \n",
        "        print(f\"  Epoch {epoch}: Val F2={val_f2:.4f} MAE={val_mae:.2f} | Train F2={train_f2:.4f} MAE={train_mae:.2f}\")\n",
        "        \n",
        "        # Early stopping check\n",
        "        if val_f2 > best_f2:\n",
        "            best_f2 = val_f2\n",
        "            best_state = copy.deepcopy(model.state_dict())\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            print(f\"    >> No improvement. Patience: {patience_counter}/{patience}\")\n",
        "            if patience_counter >= patience:\n",
        "                print(\"    >> Early stopping triggered!\")\n",
        "                break\n",
        "    \n",
        "    # Load best model\n",
        "    model.load_state_dict(best_state)\n",
        "    print(f\"\\n✓ Training complete. Best Val F2: {best_f2:.4f}\")\n",
        "    \n",
        "    return model, history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# RUN FINAL TRAINING\n",
        "# =============================================================================\n",
        "\n",
        "# Note: Set RUN_TRAINING = True to execute training\n",
        "# This takes ~3-4 hours on Databricks with CPU\n",
        "\n",
        "RUN_TRAINING = False\n",
        "\n",
        "if RUN_TRAINING:\n",
        "    # Prepare data\n",
        "    train_pd, val_pd, _, cat_dims, emb_dims, indexers, scaler = prepare_data_splits(\n",
        "        f\"{DATA_PATH}/train.parquet\",\n",
        "        f\"{DATA_PATH}/val.parquet\"\n",
        "    )\n",
        "    \n",
        "    # Create datasets\n",
        "    train_ds = FlightDataset(train_pd)\n",
        "    val_ds = FlightDataset(val_pd)\n",
        "    \n",
        "    train_dl = DataLoader(train_ds, batch_size=BEST_PARAMS[\"batch_size\"], shuffle=True, num_workers=0)\n",
        "    val_dl = DataLoader(val_ds, batch_size=BEST_PARAMS[\"batch_size\"], num_workers=0)\n",
        "    \n",
        "    # Clean up\n",
        "    del train_pd, val_pd\n",
        "    gc.collect()\n",
        "    \n",
        "    # Train with MLflow logging\n",
        "    with mlflow.start_run(run_name=\"Final_Production_Training\"):\n",
        "        mlflow.log_params(BEST_PARAMS)\n",
        "        \n",
        "        model, history = train_final_model(train_dl, val_dl, cat_dims, emb_dims)\n",
        "        \n",
        "        # Log final metrics\n",
        "        mlflow.log_metric(\"best_val_f2\", max(history[\"val_f2\"]))\n",
        "        mlflow.log_metric(\"best_val_mae\", min(history[\"val_mae\"]))\n",
        "        \n",
        "        # Save model\n",
        "        mlflow.pytorch.log_model(model, \"model_final\")\n",
        "        \n",
        "    print(\"✓ Model saved to MLflow\")\n",
        "else:\n",
        "    print(\"Training skipped. Load pre-trained model from MLflow for inference.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 8. Threshold Optimization\n",
        "\n",
        "Find the optimal classification threshold by sweeping thresholds and maximizing F2 score on the validation set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# THRESHOLD OPTIMIZATION\n",
        "# =============================================================================\n",
        "\n",
        "@torch.no_grad()\n",
        "def find_optimal_threshold(\n",
        "    model: nn.Module,\n",
        "    loader: DataLoader,\n",
        "    threshold_range: np.ndarray = np.arange(0.05, 0.95, 0.01),\n",
        "    device: torch.device = DEVICE\n",
        ") -> Tuple[float, float, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Find optimal classification threshold by maximizing F2 score.\n",
        "    \n",
        "    Args:\n",
        "        model: Trained model\n",
        "        loader: Validation data loader\n",
        "        threshold_range: Thresholds to evaluate\n",
        "        device: Compute device\n",
        "    \n",
        "    Returns:\n",
        "        optimal_threshold, best_f2, threshold_curve_df\n",
        "    \"\"\"\n",
        "    print(\"Generating validation probabilities...\")\n",
        "    model.eval()\n",
        "    \n",
        "    all_y_true, all_y_prob = [], []\n",
        "    for cat, num, time_feat, y, _ in loader:\n",
        "        _, clf_out = model(cat.to(device), num.to(device), time_feat.to(device))\n",
        "        all_y_true.append((y >= DELAY_THRESHOLD).long().cpu().numpy().flatten())\n",
        "        all_y_prob.append(torch.sigmoid(clf_out).cpu().numpy().flatten())\n",
        "    \n",
        "    y_true = np.concatenate(all_y_true)\n",
        "    y_prob = np.concatenate(all_y_prob)\n",
        "    \n",
        "    print(\"Sweeping thresholds...\")\n",
        "    best_f2, optimal_threshold = -1.0, 0.5\n",
        "    f2_scores = []\n",
        "    \n",
        "    for threshold in threshold_range:\n",
        "        y_pred = (y_prob >= threshold).astype(int)\n",
        "        f2 = fbeta_score(y_true, y_pred, beta=2, zero_division=0)\n",
        "        f2_scores.append(f2)\n",
        "        \n",
        "        if f2 > best_f2:\n",
        "            best_f2 = f2\n",
        "            optimal_threshold = threshold\n",
        "    \n",
        "    curve_df = pd.DataFrame({\"threshold\": threshold_range, \"f2_score\": f2_scores})\n",
        "    \n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"OPTIMAL THRESHOLD: {optimal_threshold:.3f}\")\n",
        "    print(f\"BEST F2 SCORE: {best_f2:.5f}\")\n",
        "    print(f\"{'='*50}\")\n",
        "    \n",
        "    return optimal_threshold, best_f2, curve_df\n",
        "\n",
        "\n",
        "# Example usage (requires trained model and data):\n",
        "# optimal_threshold, best_f2, curve_df = find_optimal_threshold(model, val_dl)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 9. Inference & Results\n",
        "\n",
        "Load the trained model and generate predictions on test data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# INFERENCE ON TEST SET\n",
        "# =============================================================================\n",
        "\n",
        "# MLflow Run ID for the trained model\n",
        "MLFLOW_RUN_ID = \"8706b956e0bd4ed681234979ad86206b\"\n",
        "\n",
        "RUN_INFERENCE = False\n",
        "\n",
        "if RUN_INFERENCE:\n",
        "    print(\"Loading trained model from MLflow...\")\n",
        "    model = mlflow.pytorch.load_model(f\"runs:/{MLFLOW_RUN_ID}/model_final\")\n",
        "    model.eval()\n",
        "    \n",
        "    # Prepare test data\n",
        "    print(\"Preparing test data...\")\n",
        "    train_spark = spark.read.parquet(f\"{DATA_PATH}/train.parquet\")\n",
        "    test_spark = spark.read.parquet(f\"{DATA_PATH}/test.parquet\")\n",
        "    \n",
        "    # Fit indexers on train (must match training)\n",
        "    indexers = [\n",
        "        StringIndexer(inputCol=c, outputCol=f\"{c}_idx\", handleInvalid=\"keep\")\n",
        "        .fit(train_spark) \n",
        "        for c in CATEGORICAL_COLS\n",
        "    ]\n",
        "    \n",
        "    # Convert test to Pandas\n",
        "    test_pd = spark_to_pandas(test_spark, indexers)\n",
        "    \n",
        "    # Fit scaler on train, transform test\n",
        "    train_pd_for_scaler = spark_to_pandas(train_spark, indexers)\n",
        "    scaler = StandardScaler()\n",
        "    scaler.fit(train_pd_for_scaler[NUMERICAL_COLS])\n",
        "    test_pd[NUMERICAL_COLS] = scaler.transform(test_pd[NUMERICAL_COLS])\n",
        "    \n",
        "    del train_spark, test_spark, train_pd_for_scaler\n",
        "    gc.collect()\n",
        "    \n",
        "    # Create test loader\n",
        "    test_ds = FlightDataset(test_pd)\n",
        "    test_dl = DataLoader(test_ds, batch_size=BEST_PARAMS[\"batch_size\"], num_workers=0)\n",
        "    \n",
        "    # Generate predictions\n",
        "    print(\"Generating predictions...\")\n",
        "    start_time = time.time()\n",
        "    predictions_df = generate_predictions(model, test_dl, threshold=OPTIMAL_THRESHOLD)\n",
        "    inference_time = time.time() - start_time\n",
        "    \n",
        "    # Calculate metrics\n",
        "    test_f2 = fbeta_score(\n",
        "        predictions_df[\"target_class\"],\n",
        "        predictions_df[\"pred_class\"],\n",
        "        beta=2\n",
        "    )\n",
        "    test_mae = mean_absolute_error(\n",
        "        predictions_df[\"target_delay\"],\n",
        "        predictions_df[\"pred_delay\"]\n",
        "    )\n",
        "    \n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"TEST SET RESULTS (Threshold: {OPTIMAL_THRESHOLD})\")\n",
        "    print(f\"{'='*60}\")\n",
        "    print(f\"  F2 Score: {test_f2:.5f}\")\n",
        "    print(f\"  MAE: {test_mae:.4f} minutes\")\n",
        "    print(f\"  Inference Time: {inference_time:.2f} seconds\")\n",
        "    print(f\"  Samples: {len(predictions_df):,}\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    # Save predictions\n",
        "    save_predictions(predictions_df, \"FINAL_TEST_OPTIMIZED\")\n",
        "else:\n",
        "    print(\"Inference skipped. Set RUN_INFERENCE = True to generate predictions.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 10. Summary & Conclusions\n",
        "\n",
        "### Model Performance\n",
        "\n",
        "| Dataset | F2 Score | MAE (minutes) |\n",
        "|---------|----------|---------------|\n",
        "| Train | 0.621 | 9.53 |\n",
        "| Validation | 0.626 | 10.84 |\n",
        "| **Test** | **0.619** | **11.34** |\n",
        "\n",
        "### Key Findings\n",
        "\n",
        "1. **Multi-Task Learning**: The dual-head architecture with combined regression and classification loss effectively handles both delay prediction and delay classification tasks.\n",
        "\n",
        "2. **FiLM Modulation**: Feature-wise Linear Modulation allows the numerical features (weather, historical delays) to contextualize the categorical embeddings (airports, carriers), improving feature interaction modeling.\n",
        "\n",
        "3. **Class Imbalance**: The 4x positive weighting in BCE loss significantly improved recall for delayed flights, which is critical given the F2 metric's emphasis on recall.\n",
        "\n",
        "4. **Threshold Optimization**: Moving from the default 0.5 threshold to 0.36 improved F2 from ~0.60 to ~0.63, demonstrating the importance of threshold tuning for imbalanced classification.\n",
        "\n",
        "### Hyperparameter Insights\n",
        "\n",
        "| Parameter | Optimal Value | Impact |\n",
        "|-----------|---------------|--------|\n",
        "| Learning Rate | 1.56e-4 | Lower LR with AdamW prevented overfitting |\n",
        "| Batch Size | 4096 | Larger batches stabilized training |\n",
        "| α (Loss Weight) | 0.342 | Slight emphasis on classification |\n",
        "| Time Dim | 16 | Rich temporal encoding |\n",
        "| Dropout | 0.05-0.32 | Moderate regularization |\n",
        "\n",
        "### Future Improvements\n",
        "\n",
        "1. **GPU Training**: Current implementation runs on CPU; GPU would enable larger models and more epochs\n",
        "2. **Attention Mechanisms**: Self-attention over temporal sequences could capture longer-range dependencies\n",
        "3. **Ensemble**: Combining with XGBoost predictions could improve robustness"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}

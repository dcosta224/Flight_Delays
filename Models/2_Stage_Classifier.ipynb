{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25d6a5ac-8bbf-47d4-822e-27b3f1b35460",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Two-Stage Flight Delay Classification Pipeline\n",
    "\n",
    "**Author:** Daniel Costa  \n",
    "**Project:** Flight Delay Prediction (W261 Final Project)  \n",
    "**Dataset:** US Domestic Flights (2015-2019)\n",
    "\n",
    "---\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook implements a **two-stage classification pipeline** for predicting flight delays. The approach combines quantile regression with a classifier to handle the inherent uncertainty in delay prediction.\n",
    "\n",
    "### Model Architecture\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────────────┐\n",
    "│                     TWO-STAGE INTERVAL CLASSIFIER                       │\n",
    "├─────────────────────────────────────────────────────────────────────────┤\n",
    "│                                                                         │\n",
    "│   STAGE 1: Quantile Regressors                                          │\n",
    "│   ┌─────────────────────┐    ┌─────────────────────┐                    │\n",
    "│   │  XGBoost Regressor  │    │  XGBoost Regressor  │                    │\n",
    "│   │  (Low Quantile)     │    │  (High Quantile)    │                    │\n",
    "│   │  α = 0.50           │    │  α = 0.98           │                    │\n",
    "│   └─────────┬───────────┘    └─────────┬───────────┘                    │\n",
    "│             │                          │                                │\n",
    "│             ▼                          ▼                                │\n",
    "│         qLow pred                  qHigh pred                           │\n",
    "│             │                          │                                │\n",
    "│             └────────────┬─────────────┘                                │\n",
    "│                          │                                              │\n",
    "│                          ▼                                              │\n",
    "│   ┌─────────────────────────────────────────────────────────────────┐   │\n",
    "│   │                    DECISION LOGIC                                │   │\n",
    "│   │                                                                  │   │\n",
    "│   │  if threshold < qLow:   → DELAYED (confident)                   │   │\n",
    "│   │  if threshold > qHigh:  → ON-TIME (confident)                   │   │\n",
    "│   │  else:                  → AMBIGUOUS (use Stage 2)               │   │\n",
    "│   └─────────────────────────────────────────────────────────────────┘   │\n",
    "│                          │                                              │\n",
    "│                          ▼ (ambiguous cases only)                       │\n",
    "│   STAGE 2: Binary Classifier                                            │\n",
    "│   ┌─────────────────────────────────────────────────────────────────┐   │\n",
    "│   │  XGBoost Classifier                                              │   │\n",
    "│   │  Features: [original features, qLow, qHigh, qHigh - qLow]       │   │\n",
    "│   │  Trained on undersampled ambiguous cases                        │   │\n",
    "│   └─────────────────────────────────────────────────────────────────┘   │\n",
    "│                                                                         │\n",
    "└─────────────────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "### Key Results\n",
    "\n",
    "| Metric | Training | Validation |\n",
    "|--------|----------|------------|\n",
    "| **F2 Score** | 0.617 | 0.621 |\n",
    "| Precision | - | - |\n",
    "| Recall | - | - |\n",
    "| PR-AUC | - | - |\n",
    "\n",
    "### Why Two Stages?\n",
    "\n",
    "1. **Confidence-Based Routing**: Quantile regressors identify cases where the model is confident vs. uncertain\n",
    "2. **Focused Classifier**: The second-stage classifier specializes on ambiguous cases\n",
    "3. **Class Imbalance Handling**: Undersampling in the ambiguous region improves classifier performance\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Setup & Configuration](#1-setup--configuration)\n",
    "2. [Data Loading & Preprocessing](#2-data-loading--preprocessing)\n",
    "3. [Feature Engineering](#3-feature-engineering)\n",
    "4. [Model Architecture](#4-model-architecture)\n",
    "   - [4.1 IntervalClassifier](#41-intervalclassifier)\n",
    "   - [4.2 IntervalClassifierModel](#42-intervalclassifiermodel)\n",
    "5. [Training Pipeline](#5-training-pipeline)\n",
    "6. [Evaluation](#6-evaluation)\n",
    "7. [Results Analysis](#7-results-analysis)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Imports\n",
    "# -----------------------------------------------------------------------------\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import col, lit, when, isnan, count as f_count\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import NumericType, StringType\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "from pyspark.ml import Estimator, Model, Pipeline\n",
    "from pyspark.ml.util import DefaultParamsReadable, DefaultParamsWritable\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n",
    "\n",
    "from xgboost.spark import SparkXGBRegressor, SparkXGBClassifier\n",
    "\n",
    "from pyspark import StorageLevel\n",
    "from mlflow.models.signature import infer_signature\n",
    "import mlflow\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Configuration\n",
    "# -----------------------------------------------------------------------------\n",
    "# Data paths\n",
    "BASE_PATH = \"dbfs:/student-groups/Group_2_2\"\n",
    "DATASET_NAME = \"5_year\"\n",
    "INPUT_DIR = f\"{BASE_PATH}/{DATASET_NAME}_custom_joined/fe_graph_and_holiday/training_splits\"\n",
    "OUTPUT_DIR = f\"{BASE_PATH}/2_stage_5y_preds\"\n",
    "\n",
    "# Model hyperparameters\n",
    "DELAY_THRESHOLD = 15.0  # minutes - flights delayed >= 15 min are considered delayed\n",
    "LOW_QUANTILE = 0.50     # Lower bound quantile\n",
    "HIGH_QUANTILE = 0.98    # Upper bound quantile\n",
    "MAX_DEPTH = 6\n",
    "N_ESTIMATORS = 100\n",
    "LEARNING_RATE = 0.05\n",
    "NUM_ROUND = 200\n",
    "NUM_WORKERS = 7\n",
    "CLASSIFIER_MIN_CHILD_WEIGHT = 20\n",
    "\n",
    "# MLflow configuration\n",
    "EXPERIMENT_NAME = \"/Shared/team_2_2/mlflow-2-stage-fe_graph_holiday-1\"\n",
    "MODEL_NAME = \"XGB_5y_2_STAGE_fe_graph_holiday\"\n",
    "\n",
    "# Random seed for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Enable MLflow autologging\n",
    "mlflow.spark.autolog()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Feature Definitions\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# Base features from flight data\n",
    "BASE_FEATURES = [\n",
    "    \"QUARTER\", \"MONTH\", \"YEAR\", \"DAY_OF_MONTH\", \"DAY_OF_WEEK\",\n",
    "    \"OP_CARRIER\", \"ORIGIN_AIRPORT_SEQ_ID\", \"DEST_AIRPORT_SEQ_ID\",\n",
    "    \"CRS_ELAPSED_TIME\", \"DISTANCE\", \"DEP_DELAY_NEW\", \"utc_timestamp\"\n",
    "]\n",
    "\n",
    "# Engineered features from Phase 2\n",
    "ENGINEERED_FEATURES = [\n",
    "    \"CRS_DEP_MINUTES\", \"prev_flight_delay_in_minutes\", \"prev_flight_delay\",\n",
    "    \"origin_delays_4h\", \"delay_origin_7d\", \"delay_origin_carrier_7d\",\n",
    "    \"delay_route_7d\", \"flight_count_24h\", \"LANDING_TIME_DIFF_MINUTES\",\n",
    "    \"AVG_ARR_DELAY_ORIGIN\", \"AVG_TAXI_OUT_ORIGIN\"\n",
    "]\n",
    "\n",
    "# Weather features\n",
    "WEATHER_FEATURES = [\n",
    "    \"HourlyDryBulbTemperature\", \"HourlyDewPointTemperature\",\n",
    "    \"HourlyRelativeHumidity\", \"HourlyAltimeterSetting\", \"HourlyVisibility\",\n",
    "    \"HourlyStationPressure\", \"HourlyWetBulbTemperature\", \"HourlyPrecipitation\",\n",
    "    \"HourlyCloudCoverage\", \"HourlyCloudElevation\", \"HourlyWindSpeed\"\n",
    "]\n",
    "\n",
    "# Graph-based features\n",
    "GRAPH_FEATURES = [\n",
    "    \"out_degree\", \"in_degree\", \"weighted_out_degree\", \"weighted_in_degree\",\n",
    "    \"N_RUNWAYS\", \"betweenness_unweighted\", \"closeness\", \"betweenness\",\n",
    "    \"avg_origin_dep_delay\", \"avg_dest_arr_delay\", \"avg_daily_route_flights\",\n",
    "    \"avg_route_delay\", \"avg_hourly_flights\"\n",
    "]\n",
    "\n",
    "# Additional categorical/derived features\n",
    "OTHER_FEATURES = [\n",
    "    \"IS_HOLIDAY\", \"IS_HOLIDAY_WINDOW\", \"AIRPORT_HUB_CLASS\",\n",
    "    \"RATING\", \"AIRLINE_CATEGORY\"\n",
    "]\n",
    "\n",
    "# Combine all features for model input\n",
    "MODEL_COLUMNS = (BASE_FEATURES + ENGINEERED_FEATURES + \n",
    "                 WEATHER_FEATURES + GRAPH_FEATURES + OTHER_FEATURES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deduplicate_columns(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Remove duplicate column names from DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        df: Input DataFrame potentially with duplicate columns\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with unique column names only\n",
    "    \"\"\"\n",
    "    unique_cols = []\n",
    "    seen = set()\n",
    "    for col_name in df.columns:\n",
    "        if col_name not in seen:\n",
    "            unique_cols.append(col_name)\n",
    "            seen.add(col_name)\n",
    "    return df.select(unique_cols)\n",
    "\n",
    "\n",
    "def clean_features(df: DataFrame, feature_cols: list) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Remove rows with null, NaN, or infinite values in feature columns.\n",
    "    \n",
    "    Args:\n",
    "        df: Input DataFrame\n",
    "        feature_cols: List of feature column names to check\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with clean feature values\n",
    "    \"\"\"\n",
    "    for f in feature_cols:\n",
    "        dtype = dict(df.dtypes)[f]\n",
    "        \n",
    "        if df.schema[f].dataType.__class__.__bases__[0] is NumericType:\n",
    "            df = df.filter(\n",
    "                col(f).isNotNull() &\n",
    "                (~isnan(col(f))) &\n",
    "                (col(f) != float(\"inf\")) &\n",
    "                (col(f) != float(\"-inf\"))\n",
    "            )\n",
    "        elif isinstance(df.schema[f].dataType, StringType):\n",
    "            df = df.filter(col(f).isNotNull() & (col(f) != \"\"))\n",
    "        else:\n",
    "            df = df.filter(col(f).isNotNull())\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def load_data(input_dir: str) -> tuple:\n",
    "    \"\"\"\n",
    "    Load train and test datasets from parquet files.\n",
    "    \n",
    "    Args:\n",
    "        input_dir: Path to directory containing train/test parquet files\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (train_df, test_df)\n",
    "    \"\"\"\n",
    "    train_df = spark.read.parquet(f\"{input_dir}/train.parquet\")\n",
    "    test_df = spark.read.parquet(f\"{input_dir}/test.parquet\")\n",
    "    return train_df, test_df\n",
    "\n",
    "\n",
    "def prepare_data(train_df: DataFrame, test_df: DataFrame, \n",
    "                 model_cols: list) -> tuple:\n",
    "    \"\"\"\n",
    "    Prepare data for training by filtering nulls and selecting features.\n",
    "    \n",
    "    Args:\n",
    "        train_df: Raw training DataFrame\n",
    "        test_df: Raw test DataFrame\n",
    "        model_cols: List of columns to include\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (cleaned_train_df, cleaned_test_df)\n",
    "    \"\"\"\n",
    "    # Filter out null labels and select model columns\n",
    "    train_df = train_df.filter(F.col(\"DEP_DELAY_NEW\").isNotNull()).select(model_cols)\n",
    "    test_df = test_df.filter(F.col(\"DEP_DELAY_NEW\").isNotNull()).select(model_cols)\n",
    "    \n",
    "    # Remove duplicate columns\n",
    "    train_df = deduplicate_columns(train_df)\n",
    "    test_df = deduplicate_columns(test_df)\n",
    "    \n",
    "    # Clean feature values\n",
    "    train_df = clean_features(train_df, model_cols)\n",
    "    test_df = clean_features(test_df, model_cols)\n",
    "    \n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare data\n",
    "print(\"Loading data...\")\n",
    "train_df_raw, test_df_raw = load_data(INPUT_DIR)\n",
    "\n",
    "print(\"Preparing data...\")\n",
    "train_df, test_df = prepare_data(train_df_raw, test_df_raw, MODEL_COLUMNS)\n",
    "\n",
    "# Cache for performance\n",
    "train_df = train_df.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "test_df = test_df.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "\n",
    "print(f\"Training samples: {train_df.count():,}\")\n",
    "print(f\"Test samples:     {test_df.count():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Architecture\n",
    "\n",
    "### 3.1 IntervalClassifier\n",
    "\n",
    "The `IntervalClassifier` implements a two-stage approach:\n",
    "1. **Stage 1**: Two quantile regressors predict confidence bounds\n",
    "2. **Stage 2**: A classifier handles ambiguous cases where the threshold falls between bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IntervalClassifier(Estimator, DefaultParamsReadable, DefaultParamsWritable):\n",
    "    \"\"\"\n",
    "    Two-stage classifier using quantile regression for confidence intervals.\n",
    "    \n",
    "    Stage 1: Fits lower and upper quantile regressors to predict delay bounds.\n",
    "    Stage 2: Trains a classifier on ambiguous cases (where threshold falls \n",
    "             between predicted bounds).\n",
    "    \n",
    "    At inference time:\n",
    "    - If threshold < lower_bound: predict DELAYED (confident)\n",
    "    - If threshold > upper_bound: predict ON-TIME (confident)  \n",
    "    - Otherwise: use classifier prediction (ambiguous)\n",
    "    \n",
    "    Args:\n",
    "        lowerEstimator: Regressor for lower quantile (e.g., 0.5)\n",
    "        upperEstimator: Regressor for upper quantile (e.g., 0.98)\n",
    "        baseClassifier: Binary classifier for ambiguous cases\n",
    "        labelCol: Name of label column\n",
    "        featuresCol: Name of features column\n",
    "        predictionCol: Name of output prediction column\n",
    "        threshold: Delay threshold in minutes (default: 15.0)\n",
    "        quantile_gap: Gap between quantiles (default: 0.1)\n",
    "        undersample_majority: Whether to balance classes in ambiguous region\n",
    "        undersample_seed: Random seed for undersampling\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 lowerEstimator,\n",
    "                 upperEstimator,\n",
    "                 baseClassifier,\n",
    "                 labelCol=\"delay_minutes\",\n",
    "                 featuresCol=\"features\",\n",
    "                 predictionCol=\"final_prediction\",\n",
    "                 threshold=15.0,\n",
    "                 quantile_gap=0.1,\n",
    "                 qLowCol=\"low_pred\",\n",
    "                 qHighCol=\"high_pred\",\n",
    "                 clfPredictionCol=\"clf_prediction\",\n",
    "                 undersample_majority=True,\n",
    "                 undersample_seed=42):\n",
    "        super().__init__()\n",
    "        self.lowerEstimator = lowerEstimator\n",
    "        self.upperEstimator = upperEstimator\n",
    "        self.baseClassifier = baseClassifier\n",
    "        self.labelCol = labelCol\n",
    "        self.featuresCol = featuresCol\n",
    "        self.predictionCol = predictionCol\n",
    "        self.threshold = float(threshold)\n",
    "        self.quantile_gap = float(quantile_gap)\n",
    "        self.qLowCol = qLowCol\n",
    "        self.qHighCol = qHighCol\n",
    "        self.clfPredictionCol = clfPredictionCol\n",
    "        self.undersample_majority = undersample_majority\n",
    "        self.undersample_seed = undersample_seed\n",
    "        self.qDiffCol = f\"{qHighCol}_minus_{qLowCol}\"\n",
    "\n",
    "    def _fit(self, dataset: DataFrame) -> Model:\n",
    "        \"\"\"Fit the two-stage model.\"\"\"\n",
    "        # Stage 1: Fit quantile regressors\n",
    "        print(\"Training Lower Quantile Estimator...\")\n",
    "        lowerModel = self.lowerEstimator.fit(dataset)\n",
    "        \n",
    "        print(\"Training Upper Quantile Estimator...\")\n",
    "        upperModel = self.upperEstimator.fit(dataset)\n",
    "\n",
    "        # Add quantile predictions\n",
    "        df_q = lowerModel.transform(dataset).withColumnRenamed(\"prediction\", self.qLowCol)\n",
    "        df_q = upperModel.transform(df_q).withColumnRenamed(\"prediction\", self.qHighCol)\n",
    "\n",
    "        # Filter to ambiguous cases (threshold between bounds)\n",
    "        print(\"Filtering ambiguous cases...\")\n",
    "        thr = lit(self.threshold)\n",
    "        df_ambig = df_q.filter(\n",
    "            (col(self.qLowCol) <= thr) & (thr <= col(self.qHighCol))\n",
    "        )\n",
    "\n",
    "        # Create binary label\n",
    "        df_ambig = df_ambig.withColumn(\n",
    "            \"bin_label\",\n",
    "            (col(self.labelCol) >= thr).cast(\"double\")\n",
    "        )\n",
    "\n",
    "        # Undersample majority class\n",
    "        if self.undersample_majority:\n",
    "            df_ambig = self._undersample(df_ambig)\n",
    "\n",
    "        # Augment features with quantile predictions\n",
    "        df_ambig = self._augment_features(df_ambig)\n",
    "\n",
    "        # Stage 2: Fit classifier on ambiguous cases\n",
    "        print(\"Training Stage 2 Classifier...\")\n",
    "        clf = self.baseClassifier\n",
    "        clf.setParams(\n",
    "            label_col=\"bin_label\",\n",
    "            features_col=self.featuresCol,\n",
    "            prediction_col=self.clfPredictionCol\n",
    "        )\n",
    "        clfModel = clf.fit(df_ambig)\n",
    "\n",
    "        return IntervalClassifierModel(\n",
    "            lowerModel=lowerModel,\n",
    "            upperModel=upperModel,\n",
    "            clfModel=clfModel,\n",
    "            labelCol=self.labelCol,\n",
    "            featuresCol=self.featuresCol,\n",
    "            predictionCol=self.predictionCol,\n",
    "            threshold=self.threshold,\n",
    "            qLowCol=self.qLowCol,\n",
    "            qHighCol=self.qHighCol,\n",
    "            qDiffCol=self.qDiffCol,\n",
    "            clfPredictionCol=self.clfPredictionCol\n",
    "        )\n",
    "\n",
    "    def _undersample(self, df_ambig: DataFrame) -> DataFrame:\n",
    "        \"\"\"Undersample majority class to balance ambiguous cases.\"\"\"\n",
    "        print(\"Undersampling majority class...\")\n",
    "        class_counts = (\n",
    "            df_ambig.groupBy(\"bin_label\")\n",
    "            .agg(f_count(\"*\").alias(\"cnt\"))\n",
    "            .collect()\n",
    "        )\n",
    "\n",
    "        if len(class_counts) != 2:\n",
    "            raise ValueError(\"Ambiguous cases must contain both classes.\")\n",
    "\n",
    "        (label0, cnt0), (label1, cnt1) = [\n",
    "            (row[\"bin_label\"], row[\"cnt\"]) for row in class_counts\n",
    "        ]\n",
    "\n",
    "        if cnt0 <= cnt1:\n",
    "            minority_label, minority_cnt = label0, cnt0\n",
    "            majority_label, majority_cnt = label1, cnt1\n",
    "        else:\n",
    "            minority_label, minority_cnt = label1, cnt1\n",
    "            majority_label, majority_cnt = label0, cnt0\n",
    "\n",
    "        print(f\"  Majority class {majority_label}: {majority_cnt:,} → {minority_cnt:,}\")\n",
    "\n",
    "        if majority_cnt > 0 and minority_cnt > 0:\n",
    "            frac_majority = float(minority_cnt) / float(majority_cnt)\n",
    "            fractions = {\n",
    "                float(minority_label): 1.0,\n",
    "                float(majority_label): frac_majority\n",
    "            }\n",
    "            df_ambig = df_ambig.sampleBy(\n",
    "                \"bin_label\", fractions=fractions, seed=self.undersample_seed\n",
    "            )\n",
    "\n",
    "        return df_ambig\n",
    "\n",
    "    def _augment_features(self, df: DataFrame) -> DataFrame:\n",
    "        \"\"\"Add quantile predictions as additional features.\"\"\"\n",
    "        df = df.withColumn(self.qDiffCol, col(self.qHighCol) - col(self.qLowCol))\n",
    "        \n",
    "        augFeaturesCol = self.featuresCol + \"_aug\"\n",
    "        assembler = VectorAssembler(\n",
    "            inputCols=[self.featuresCol, self.qLowCol, self.qHighCol, self.qDiffCol],\n",
    "            outputCol=augFeaturesCol\n",
    "        )\n",
    "        df = assembler.transform(df)\n",
    "        df = df.drop(self.featuresCol).withColumnRenamed(augFeaturesCol, self.featuresCol)\n",
    "        \n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IntervalClassifierModel(Model, DefaultParamsReadable, DefaultParamsWritable):\n",
    "    \"\"\"\n",
    "    Fitted two-stage interval classifier model.\n",
    "    \n",
    "    Applies the decision logic:\n",
    "    - threshold < qLow: predict 1 (delayed)\n",
    "    - threshold > qHigh: predict 0 (on-time)\n",
    "    - otherwise: use classifier prediction\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, lowerModel, upperModel, clfModel, labelCol, featuresCol,\n",
    "                 predictionCol, threshold, qLowCol, qHighCol, qDiffCol, clfPredictionCol):\n",
    "        super().__init__()\n",
    "        self.lowerModel = lowerModel\n",
    "        self.upperModel = upperModel\n",
    "        self.clfModel = clfModel\n",
    "        self.labelCol = labelCol\n",
    "        self.featuresCol = featuresCol\n",
    "        self.predictionCol = predictionCol\n",
    "        self.threshold = float(threshold)\n",
    "        self.qLowCol = qLowCol\n",
    "        self.qHighCol = qHighCol\n",
    "        self.qDiffCol = qDiffCol\n",
    "        self.clfPredictionCol = clfPredictionCol\n",
    "\n",
    "    def _transform(self, dataset: DataFrame) -> DataFrame:\n",
    "        \"\"\"Apply two-stage prediction logic.\"\"\"\n",
    "        # Get quantile predictions\n",
    "        df_q = self.lowerModel.transform(dataset).withColumnRenamed(\"prediction\", self.qLowCol)\n",
    "        df_q = self.upperModel.transform(df_q).withColumnRenamed(\"prediction\", self.qHighCol)\n",
    "\n",
    "        # Augment features\n",
    "        df_q = df_q.withColumn(self.qDiffCol, col(self.qHighCol) - col(self.qLowCol))\n",
    "        augFeaturesCol = self.featuresCol + \"_aug\"\n",
    "        assembler = VectorAssembler(\n",
    "            inputCols=[self.featuresCol, self.qLowCol, self.qHighCol, self.qDiffCol],\n",
    "            outputCol=augFeaturesCol\n",
    "        )\n",
    "        df_q = assembler.transform(df_q)\n",
    "        df_q = df_q.drop(self.featuresCol).withColumnRenamed(augFeaturesCol, self.featuresCol)\n",
    "\n",
    "        # Get classifier predictions\n",
    "        df_clf = self.clfModel.transform(df_q)\n",
    "\n",
    "        # Apply decision logic\n",
    "        thr = lit(self.threshold)\n",
    "        df_final = df_clf.withColumn(\n",
    "            self.predictionCol,\n",
    "            when(thr < col(self.qLowCol), lit(1.0))       # Confident: delayed\n",
    "            .when(thr > col(self.qHighCol), lit(0.0))     # Confident: on-time\n",
    "            .otherwise(col(self.clfPredictionCol))        # Ambiguous: use classifier\n",
    "        )\n",
    "\n",
    "        # Add decision source for analysis\n",
    "        df_final = df_final.withColumn(\n",
    "            \"decision_source\",\n",
    "            when(thr < col(self.qLowCol), lit(\"quantile_high\"))\n",
    "            .when(thr > col(self.qHighCol), lit(\"quantile_low\"))\n",
    "            .otherwise(lit(\"classifier\"))\n",
    "        )\n",
    "\n",
    "        return df_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Feature Encoding\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# Categorical encoding for carrier\n",
    "carrier_indexer = StringIndexer(\n",
    "    inputCol=\"OP_CARRIER\", \n",
    "    outputCol=\"carrier_idx\", \n",
    "    handleInvalid=\"keep\"\n",
    ")\n",
    "carrier_encoder = OneHotEncoder(\n",
    "    inputCol=\"carrier_idx\", \n",
    "    outputCol=\"carrier_vec\"\n",
    ")\n",
    "\n",
    "# Numerical features for model input\n",
    "NUMERICAL_FEATURES = [\n",
    "    \"QUARTER\", \"MONTH\", \"YEAR\", \"DAY_OF_MONTH\", \"DAY_OF_WEEK\",\n",
    "    \"carrier_vec\", \"CRS_ELAPSED_TIME\", \"DISTANCE\", \"CRS_DEP_MINUTES\",\n",
    "    \"prev_flight_delay_in_minutes\", \"prev_flight_delay\", \"origin_delays_4h\",\n",
    "    \"delay_origin_7d\", \"delay_origin_carrier_7d\", \"delay_route_7d\",\n",
    "    \"flight_count_24h\", \"LANDING_TIME_DIFF_MINUTES\", \"AVG_ARR_DELAY_ORIGIN\",\n",
    "    \"AVG_TAXI_OUT_ORIGIN\"\n",
    "] + WEATHER_FEATURES + GRAPH_FEATURES + [\n",
    "    \"IS_HOLIDAY\", \"IS_HOLIDAY_WINDOW\", \"AIRPORT_HUB_CLASS\", \"RATING\", \"AIRLINE_CATEGORY\"\n",
    "]\n",
    "\n",
    "# Vector assembler\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=NUMERICAL_FEATURES,\n",
    "    outputCol=\"features\",\n",
    "    handleInvalid=\"skip\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Model Definition\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# Stage 1: Quantile Regressors\n",
    "xgb_regressor_low = SparkXGBRegressor(\n",
    "    objective=\"reg:quantileerror\",\n",
    "    quantile_alpha=LOW_QUANTILE,\n",
    "    num_round=NUM_ROUND,\n",
    "    features_col=\"features\",\n",
    "    label_col=\"DEP_DELAY_NEW\",\n",
    "    num_workers=NUM_WORKERS,\n",
    "    max_depth=MAX_DEPTH,\n",
    "    n_estimators=N_ESTIMATORS,\n",
    "    learning_rate=LEARNING_RATE\n",
    ")\n",
    "\n",
    "xgb_regressor_high = SparkXGBRegressor(\n",
    "    objective=\"reg:quantileerror\",\n",
    "    quantile_alpha=HIGH_QUANTILE,\n",
    "    num_round=NUM_ROUND,\n",
    "    features_col=\"features\",\n",
    "    label_col=\"DEP_DELAY_NEW\",\n",
    "    num_workers=NUM_WORKERS,\n",
    "    max_depth=MAX_DEPTH,\n",
    "    n_estimators=N_ESTIMATORS,\n",
    "    learning_rate=LEARNING_RATE\n",
    ")\n",
    "\n",
    "# Stage 2: Binary Classifier\n",
    "classifier = SparkXGBClassifier(\n",
    "    num_round=NUM_ROUND,\n",
    "    features_col=\"features\",\n",
    "    label_col=\"bin_label\",\n",
    "    prediction_col=\"clf_prediction\",\n",
    "    max_depth=MAX_DEPTH,\n",
    "    n_estimators=N_ESTIMATORS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    min_child_weight=CLASSIFIER_MIN_CHILD_WEIGHT\n",
    ")\n",
    "\n",
    "# Two-Stage Interval Classifier\n",
    "interval_clf = IntervalClassifier(\n",
    "    lowerEstimator=xgb_regressor_low,\n",
    "    upperEstimator=xgb_regressor_high,\n",
    "    baseClassifier=classifier,\n",
    "    labelCol=\"DEP_DELAY_NEW\",\n",
    "    featuresCol=\"features\",\n",
    "    threshold=DELAY_THRESHOLD,\n",
    "    predictionCol=\"final_prediction\"\n",
    ")\n",
    "\n",
    "# Full Pipeline\n",
    "pipeline = Pipeline(stages=[\n",
    "    carrier_indexer,\n",
    "    carrier_encoder,\n",
    "    assembler,\n",
    "    interval_clf\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Training\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "run_name = f\"XGB-2_stage_5y_q{LOW_QUANTILE}_{HIGH_QUANTILE}\"\n",
    "\n",
    "with mlflow.start_run(run_name=run_name):\n",
    "    # Log hyperparameters\n",
    "    mlflow.log_params({\n",
    "        \"low_quantile\": LOW_QUANTILE,\n",
    "        \"high_quantile\": HIGH_QUANTILE,\n",
    "        \"threshold\": DELAY_THRESHOLD,\n",
    "        \"max_depth\": MAX_DEPTH,\n",
    "        \"n_estimators\": N_ESTIMATORS,\n",
    "        \"learning_rate\": LEARNING_RATE,\n",
    "        \"num_round\": NUM_ROUND,\n",
    "        \"classifier_min_child_weight\": CLASSIFIER_MIN_CHILD_WEIGHT\n",
    "    })\n",
    "    \n",
    "    print(\"Training pipeline...\")\n",
    "    model = pipeline.fit(train_df)\n",
    "    \n",
    "    print(\"Generating predictions...\")\n",
    "    train_preds = model.transform(train_df)\n",
    "    test_preds = model.transform(test_df)\n",
    "    \n",
    "    # Add binary labels for evaluation\n",
    "    train_preds = train_preds.withColumn(\n",
    "        \"label_bin\", \n",
    "        (col(\"DEP_DELAY_NEW\") >= lit(DELAY_THRESHOLD)).cast(\"double\")\n",
    "    )\n",
    "    test_preds = test_preds.withColumn(\n",
    "        \"label_bin\",\n",
    "        (col(\"DEP_DELAY_NEW\") >= lit(DELAY_THRESHOLD)).cast(\"double\")\n",
    "    )\n",
    "    \n",
    "    print(\"✓ Training complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_f2(df: DataFrame, label_col: str = \"label_bin\", \n",
    "               pred_col: str = \"final_prediction\", beta: float = 2.0) -> float:\n",
    "    \"\"\"\n",
    "    Compute F2 score (emphasizes recall over precision).\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with predictions and labels\n",
    "        label_col: Name of true label column\n",
    "        pred_col: Name of prediction column\n",
    "        beta: F-beta parameter (2.0 for F2)\n",
    "        \n",
    "    Returns:\n",
    "        F2 score\n",
    "    \"\"\"\n",
    "    tp = df.filter((col(label_col) == 1) & (col(pred_col) == 1)).count()\n",
    "    fp = df.filter((col(label_col) == 0) & (col(pred_col) == 1)).count()\n",
    "    fn = df.filter((col(label_col) == 1) & (col(pred_col) == 0)).count()\n",
    "    \n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else float(\"nan\")\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else float(\"nan\")\n",
    "    \n",
    "    denom = (beta**2) * precision + recall\n",
    "    f2 = ((1 + beta**2) * precision * recall / denom) if denom > 0 else float(\"nan\")\n",
    "    \n",
    "    return f2, precision, recall\n",
    "\n",
    "\n",
    "def evaluate_model(train_preds: DataFrame, test_preds: DataFrame):\n",
    "    \"\"\"Compute and display evaluation metrics.\"\"\"\n",
    "    # Compute metrics\n",
    "    train_f2, train_prec, train_rec = compute_f2(train_preds)\n",
    "    test_f2, test_prec, test_rec = compute_f2(test_preds)\n",
    "    \n",
    "    # Decision source breakdown\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"EVALUATION RESULTS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(f\"\\n{'Metric':<20} {'Training':>12} {'Test':>12}\")\n",
    "    print(\"-\"*44)\n",
    "    print(f\"{'F2 Score':<20} {train_f2:>12.4f} {test_f2:>12.4f}\")\n",
    "    print(f\"{'Precision':<20} {train_prec:>12.4f} {test_prec:>12.4f}\")\n",
    "    print(f\"{'Recall':<20} {train_rec:>12.4f} {test_rec:>12.4f}\")\n",
    "    \n",
    "    # Decision source distribution\n",
    "    print(\"\\n\" + \"-\"*44)\n",
    "    print(\"Decision Source Distribution (Test Set):\")\n",
    "    test_preds.groupBy(\"decision_source\").count().show()\n",
    "    \n",
    "    return {\n",
    "        \"train_f2\": train_f2, \"test_f2\": test_f2,\n",
    "        \"train_precision\": train_prec, \"test_precision\": test_prec,\n",
    "        \"train_recall\": train_rec, \"test_recall\": test_rec\n",
    "    }\n",
    "\n",
    "\n",
    "# Run evaluation\n",
    "metrics = evaluate_model(train_preds, test_preds)\n",
    "\n",
    "# Log metrics to MLflow\n",
    "with mlflow.start_run(run_name=run_name, nested=True):\n",
    "    for name, value in metrics.items():\n",
    "        mlflow.log_metric(name, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Summary\n",
    "\n",
    "This notebook implements a two-stage classification pipeline that achieves **F2 = 0.621** on the test set.\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "| Decision Source | Count | Description |\n",
    "|-----------------|-------|-------------|\n",
    "| `quantile_high` | ~X% | Confidently predicted as DELAYED |\n",
    "| `quantile_low` | ~X% | Confidently predicted as ON-TIME |\n",
    "| `classifier` | ~X% | Ambiguous cases resolved by Stage 2 |\n",
    "\n",
    "### Design Decisions\n",
    "\n",
    "1. **Quantile Selection**: α_low = 0.50, α_high = 0.98 provides good coverage\n",
    "2. **Undersampling**: Balances classes in ambiguous region for better classifier training\n",
    "3. **Feature Augmentation**: Adding quantile predictions as classifier features improves performance\n",
    "4. **F2 Metric**: Emphasizes recall (catching delays) over precision\n",
    "\n",
    "### Future Improvements\n",
    "\n",
    "- Tune quantile thresholds more granularly\n",
    "- Experiment with different classifiers for Stage 2\n",
    "- Add confidence calibration for probability outputs"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 3915201981341392,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "2_Stage_Dev",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
